[
  {
    "title": "A Brief History of AI",
    "url": "https://aitopics.org/misc/brief-history",
    "content": "Artificial intelligence's conceptual roots trace back to Greek mythology's depiction of intelligent machines.  The development of modern AI followed World War II with the advent of computers, enabling the creation of programs capable of complex intellectual tasks.  Key milestones include Aristotle's development of syllogistic logic (4th century BC), Ramon Lull's combinatorics machines (13th century), Al-Jazari's programmable humanoid robot (1206 AD), and Pascal's mechanical calculator (1642).  The 17th century saw further advancements with Hobbes' mechanistic theory of thinking and the development of arithmetical machines.  The invention of the printing press and clocks also contributed to the foundational technologies underpinning AI's later development.\n\n\nIntroduction \n The intellectual roots of AI, and the concept of intelligent machines, may be found in Greek mythology. Intelligent artifacts appear in literature since then, with real (and fraudulent) mechanical devices actually demonstrated to behave with some degree of intelligence. Some of these conceptual achievements are listed below under \"Ancient History.\" \n After modern computers became available, following World War II, it has become possible to create programs that perform difficult intellectual tasks. From these programs, general tools are constructed which have applications in a wide variety of everday problems. Some of these computational milestones are listed below under \"Modern History.\" \n Ancient History \n Greek myths of Hephaestus, the blacksmith who manufactured mechanical servants, and the bronze man Talos incorporate the idea of intelligent robots. Many other myths in antiquity involve human-like artifacts. Many mechanical toys and models were actually constructed, e.g., by Archytas of Tarentum , Hero, Daedalus and other real persons. \n 4th century B.C. \n \n Aristotle invented syllogistic logic, the first formal deductive reasoning system. \n \n 13th century \n \n Talking heads were said to have been created, Roger Bacon and Albert the Great reputedly among the owners. \n Ramon Lull , Spanish theologian, invented machines for discovering nonmathematical truths through combinatorics. \n In 1206 A.D., Al-Jazari , an Arab inventor, designed what is believed to be the first programmable humanoid robot, a boat carrying four mechanical musicians powered by water flow. \n \n 15th century \n Invention of printing using moveable type. Gutenberg Bible printed (1456). \n \n 15th-16th century \n Clocks, the first modern measuring machines, were first produced using lathes. \n \n 16th century \n Clockmakers extended their craft to creating mechanical animals and other novelties. For example, see DaVinci's walking lion (1515) . \n Rabbi Loew of Prague is said to have invented the Golem , a clay man brought to life (1580). \n \n 17th century \n Early in the century, Descartes proposed that bodies of animals are nothing more than complex machines. Many other 17th century thinkers offered variations and elaborations of Cartesian mechanism. \n Pascal created the first mechanical digital calculating machine (1642). \n Thomas Hobbes published The Leviathan (1651), containing a mechanistic and combinatorial theory of thinking. \n Arithmetical machines devised by Sir Samuel Morland between 1662 and 1666 \n Leibniz improved Pascal's machine to do multiplication &amp; division with a machine called the Step Reckoner (1673) and envisioned a universal calculus of reasoning by which arguments could be decided mechanically. \n \n 18th century \n The 18th century saw a profusion of mechanical toys, including the celebrated mechanical duck of Vaucanson and von Kempelen's phony mechanical chess player, The Turk (1769). Edgar Allen Poe wrote (in the Southern Literary Messenger, April 1836) that the Turk could not be a machine because, if it were, it would not lose. \n \n 19th century \n \n Joseph-Marie Jacquard invented the Jacquard loom , the first programmable machine, with instructions on punched cards (1801). \n Luddites (by Marjie Bloy, PhD. Victorian Web) (led by Ned Ludd) destroyed machinery in England (1811-1816). See also What the Luddites Really Fought Against . By Richard Conniff, Smithsonian magazine (March 2011). \n Mary Shelley published the story of Frankenstein's monster (1818). The book Frankenstein, or the Modern Prometheus available from Project Gutenberg. \n Charles Babbage &amp; Ada Byron (Lady Lovelace) designed a programmable mechanical calculating machines, the Analytical Engine (1832). A working model was built in 2002; a short video shows it working. \n George Boole developed a binary algebra representing (some) \"laws of thought,\" published in The Laws of Thought (1854). \n Modern propositional logic developed by Gottlob Frege in his 1879 work Begriffsschrift and later clarified and expanded by Russell , Tarski , Godel , Church and others. \n \n 20th century - First Half \n \n Bertrand Russell and Alfred North Whitehead published Principia Mathematica , which revolutionaized formal logic. Russell, Ludwig Wittgenstein, and Rudolf Carnap lead philosophy into logical analysis of knowledge. \n Torres y Quevedo built his chess machine 'Ajedrecista', using electromagnets under the board to play the endgame rook and king against the lone king, possibly the first computer game (1912). \n Karel Capek's play \"R.U.R.\" (Rossum's Universal Robots) produced in 1921 (London opening, 1923). - First use of the word 'robot' in English. \n Alan Turing proposed the universal Turing machine (1936-37) \n Electro, a mechanical man, introduced by Westinghouse Electricat the World's Fair in New York (1939), along with Sparko, a mechanical dog. \n Warren McCulloch &amp; Walter Pitts publish \"A Logical Calculus of the Ideas Immanent in Nervous Activity\" (1943), laying foundations for neural networks. \n Arturo Rosenblueth, Norbert Wiener &amp; Julian Bigelow coin the term \"cybernetics\" in a 1943 paper. Wiener's popular book by that name published in 1948. \n Emil Post proves that production systems are a general computational mechanism (1943). See Ch.2 of Rule Based Expert Systems for the uses of production systems in AI. Post also did important work on completeness, inconsistency, and proof theory. \n George Polya published his best-selling book on thinking heuristically, How to Solve It in 1945. This book introduced the term 'heuristic' into modern thinking and has influenced many AI scientists. \n Vannevar Bush published As We May Think (Atlantic Monthly, July 1945) a prescient vision of the future in which computers assist humans in many activities. \n Grey Walter experimented with autonomous robots, turtles named Elsie and Elmer, at Bristol (1948-49) based on the premise that a small number of brain cells could give rise to complex behaviors. \n A.M. Turing published \"Computing Machinery and Intelligence\" (1950) . - Introduction of Turing Test as a way of operationalizing a test of intelligent behavior. See The Turing Institute for more on Turing. \n Claude Shannon published detailed analysis of chess playing as search in \"Programming a computer to play chess\" (1950). \n Isaac Asimov published his three laws of robotics (1950). \n \n Modern History \n The modern history of AI begins with the development of stored-program electronic computers. For a short summary, see Genius and Tragedy at Dawn of Computer Age By ALICE RAWSTHORN, NY Times (March 25, 2012 ), a review of technology historian George Dyson's book “Turing’s Cathedral: The Origins of the Digital Universe.” \n 1956 John McCarthy coined the term \"artificial intelligence\" as the topic of the Dartmouth Conference , the first conference devoted to the subject. \n Demonstration of the first running AI program, the Logic Theorist (LT) written by Allen Newell, J.C. Shaw and Herbert Simon (Carnegie Institute of Technology, now Carnegie Mellon University). See Over the holidays 50 years ago, two scientists hatched artificial intelligence . \n 1957 The General Problem Solver (GPS) demonstrated by Newell, Shaw &amp; Simon. \n 1952-62 Arthur Samuel (IBM) wrote the first game-playing program, for checkers, to achieve sufficient skill to challenge a world champion. Samuel's machine learning programs were responsible for the high performance of the checkers player. \n 1958 John McCarthy (MIT) invented the Lisp language. \n Herb Gelernter &amp; Nathan Rochester (IBM) described a theorem prover in geometry that exploits a semantic model of the domain in the form of diagrams of \"typical\" cases. \n Teddington Conference on the Mechanization of Thought Processes was held in the UK and among the papers presented were John McCarthy's Programs with Common Sense , \" Oliver Selfridge's \"Pandemonium,\" and Marvin Minsky's \"Some Methods of Heuristic Programming and Artificial Intelligence.\" \n Late 50's &amp; Early 60's Margaret Masterman &amp; colleagues at Cambridge design semantic nets for machine translation . See Themes in the work of Margaret Masterman by Yorick Wilks (1988). \n 1961 James Slagle (PhD dissertation, MIT) wrote (in Lisp) the first symbolic integration program, SAINT, which solved calculus problems at the college freshman level. \n 1962 First industrial robot company, Unimation, founded. \n 1963 Thomas Evans' program, ANALOGY, written as part of his PhD work at MIT, demonstrated that computers can solve the same analogy problems as are given on IQ tests. \n Ivan Sutherland's MIT dissertation on Sketchpad introduced the idea of interactive graphics into computing. \n Edward A. Feigenbaum &amp; Julian Feldman published Computers and Thought , the first collection of articles about artificial intelligence. \n 1964 Danny Bobrow's dissertation at MIT (tech.report #1 from MIT's AI group, Project MAC), shows that computers can understand natural language well enough to solve algebra word problems correctly. \n Bert Raphael's MIT dissertation on the SIR program demonstrates the power of a logical representation of knowledge for question-answering systems \n 1965 J. Alan Robinson invented a mechanical proof procedure, the Resolution Method, which allowed programs to work efficiently with formal logic as a representation language. (See Carl Hewitt's downloadable PDF file Middle History of Logic Programming ). \n Joseph Weizenbaum (MIT) built ELIZA, an interactive program that carries on a dialogue in English on any topic. It was a popular toy at AI centers on the ARPA-net when a version that \"simulated\" the dialogue of a psychotherapist was programmed. \n 1966 Ross Quillian (PhD dissertation, Carnegie Inst. of Technology; now CMU) demonstrated semantic nets. \n First Machine Intelligence workshop at Edinburgh - the first of an influential annual series organized by Donald Michie and others. \n Negative report on machine translation kills much work in Natural Language Processing (NLP) for many years. \n 1967 Dendral program (Edward Feigenbaum, Joshua Lederberg, Bruce Buchanan, Georgia Sutherland at Stanford) demonstrated to interpret mass spectra on organic chemical compounds. First successful knowledge-based program for scientific reasoning. \n Joel Moses (PhD work at MIT) demonstrated the power of symbolic reasoning for integration problems in the Macsyma (PDF file) program. First successful knowledge-based program in mathematics. \n Richard Greenblatt at MIT built a knowledge-based chess-playing program, MacHack, that was good enough to achieve a class-C rating in tournament play. \n Late 60s Doug Engelbart invented the mouse at SRI. \n 1968 Marvin Minsky &amp; Seymour Papert publish Perceptrons, demonstrating limits of simple neural nets. \n 1969 SRI robot, Shakey, demonstrated combining locomotion, perception and problem solving. \n Roger Schank (Stanford) defined conceptual dependency model for natural language understanding. Later developed (in PhD dissertations at Yale) for use in story understanding by Robert Wilensky and Wendy Lehnert, and for use in understanding memory by Janet Kolodner. \n First International Joint Conference on Artificial Intelligence (IJCAI) held in Washington, D.C. \n 1970 Jaime Carbonell (Sr.) developed SCHOLAR, an interactive program for computer-aided instruction based on semantic nets as the representation of knowledge. \n Bill Woods described Augmented Transition Networks (ATN's) as a representation for natural language understanding. \n Patrick Winston's PhD program, ARCH, at MIT learned concepts from examples in the world of children's blocks. \n Early 70's Jane Robinson &amp; Don Walker established influential Natural Language Processing group at SRI. \n 1971 Terry Winograd's PhD thesis (MIT) demonstrated the ability of computers to understand English sentences in a restricted world of children's blocks, in a coupling of his language understanding program, SHRDLU , with a robot arm that carried out instructions typed in English. \n 1972 Prolog developed by Alain Colmerauer. \n 1973 The Assembly Robotics group at Edinburgh University builds Freddy, the Famous Scottish Robot, capable of using vision to locate and assemble models. \n 1974 Ted Shortliffe's PhD dissertation on MYCIN (Stanford) demonstrated the power of rule-based systems for knowledge representation and inference in the domain of medical diagnosis and therapy. Sometimes called the first expert system. \n Earl Sacerdoti developed one of the first planning programs, ABSTRIPS, and developed techniques of hierarchical planning. \n 1975 Marvin Minsky published his widely-read and influential article on Frames as a representation of knowledge, in which many ideas about schemas and semantic links are brought together. \n The Meta-Dendral learning program produced new results in chemistry (some rules of mass spectrometry) the first scientific discoveries by a computer to be published in a refereed journal. \n Mid 70's Barbara Grosz (SRI) established limits to traditional AI approaches to discourse modeling. Subsequent work by Grosz, Bonnie Webber and Candace Sidner developed the notion of \"centering\", used in establishing focus of discourse and anaphoric references in NLP. \n Alan Kay and Adele Goldberg (Xerox PARC) developed the Smalltalk language, establishing the power of object-oriented programming and of icon-oriented interfaces. \n David Marr and MIT colleagues describe the \"primal sketch\" and its role in visual perception. \n 1976 Doug Lenat's AM program (Stanford PhD dissertation) demonstrated the discovery model (loosely-guided search for interesting conjectures). \n Randall Davis demonstrated the power of meta-level reasoning in his PhD dissertation at Stanford. \n Late 70's Stanford's SUMEX-AIM resource, headed by Ed Feigenbaum and Joshua Lederberg, demonstrates the power of the ARPAnet for scientific collaboration . \n 1978 Tom Mitchell, at Stanford, invented the concept of Version Spaces for describing the search space of a concept formation program. \n Herb Simon wins the Nobel Prize in Economics for his theory of bounded rationality , one of the cornerstones of AI known as \"satisficing\". \n The MOLGEN program, written at Stanford by Mark Stefik and Peter Friedland, demonstrated that an object-oriented representation of knowledge can be used to plan gene-cloning experiments.\n \n 1979 Mycin program, initially written as Ted Shortliffe's Ph.D. dissertation at Stanford, was demonstrated to perform at the level of experts. Bill VanMelle's PhD dissertation at Stanford demonstrated the generality of MYCIN's representation of knowledge and style of reasoning in his EMYCIN program, the model for many commercial expert system \"shells\". \n Jack Myers and Harry Pople at University of Pittsburgh developed INTERNIST, a knowledge-based medical diagnosis program based on Dr. Myers' clinical knowledge. \n Cordell Green, David Barstow, Elaine Kant and others at Stanford demonstrated the CHI system for automatic programming. \n The Stanford Cart , built by Hans Moravec, becomes the first computer-controlled, autonomous vehicle when it successfully traverses a chair-filled room and circumnavigates the Stanford AI Lab. \n Drew McDermott &amp; Jon Doyle at MIT, and John McCarthy at Stanford begin publishing work on non-monotonic logics and formal aspects of truth maintenance. \n 1980's Lisp Machines developed and marketed. \n First expert system shells and commercial applications. \n 1980 Lee Erman, Rick Hayes-Roth, Victor Lesser and Raj Reddy published the first description of the blackboard model, as the framework for the HEARSAY-II speech understanding system. \n First National Conference of the American Association of Artificial Intelligence (AAAI) held at Stanford. \n 1981 Danny Hillis designs the connection machine, a massively parallel architecture that brings new power to AI, and to computation in general. (Later founds Thinking Machines, Inc.) \n 1983 John Laird &amp; Paul Rosenbloom, working with Allen Newell, complete CMU dissertations on SOAR. \n James Allen invents the Interval Calculus, the first widely used formalization of temporal events. \n Mid 80's Neural Networks become widely used with the Backpropagation algorithm (first described by Werbos in 1974). \n 1985 The autonomous drawing program, Aaron , created by Harold Cohen, is demonstrated at the AAAI National Conference (based on more than a decade of work, and with subsequent work showing major developments). \n 1987 Marvin Minsky publishes The Society of Mind , a theoretical description of the mind as a collection of cooperating agents. \n 1989 Dean Pomerleau at CMU creates ALVINN (An Autonomous Land Vehicle in a Neural Network), which grew into the system that drove a car coast-to-coast under computer control for all but about 50 of the 2850 miles. \n 1990's Major advances in all areas of AI, with significant demonstrations in machine learning, intelligent tutoring, case-based reasoning, multi-agent planning, scheduling, uncertain reasoning, data mining, natural language understanding and translation, vision, virtual reality, games, and other topics. \n Rod Brooks' COG Project at MIT, with numerous collaborators, makes significant progress in building a humanoid robot \n TD-Gammon, a backgammon program written by Gerry Tesauro, demonstrates that reinforcement learning is powerful enough to create a championship-level game-playing program by competing favorably with world-class players. \n EQP theorem prover at Argonne National Labs proves the Robbins Conjecture in mathematics (October-November, 1996). \n The Deep Blue chess program beats the current world chess champion, Garry Kasparov, in a widely followed match and rematch (See Deep Blue Wins ). (May 11th, 1997). \n NASA’s pathfinder mission made a successful landing and the first autonomous robotics system, Sojourner, was deployed on the surface of Mars. (July 4, 1997) \n First official Robo-Cup soccer match (1997) featuring table-top matches with 40 teams of interacting robots and over 5000 spectators. \n Web crawlers and other AI-based information extraction programs become essential in widespread use of the world-wide-web. \n Demonstration of an Intelligent Room and Emotional Agents at MIT's AI Lab. Initiation of work on the Oxygen Architecture, which connects mobile and stationary computers in an adaptive network. \n 2000's Interactive robot pets (a.k.a. \"smart toys\") become commercially available, realizing the vision of the 18th cen. novelty toy makers. \n Cynthia Breazeal at MIT publishes her dissertation on Sociable Machines, describing KISMET, a robot with a face that expresses emotions. \n Stanford's autonomous vehicle, Stanley, wins DARPA Grand Challenge race. (October 2005). (See In a Grueling Desert Race, a Winner, but Not a Driver . \n The Nomad robot explores remote regions of Antarctica looking for meteorite samples. \n Today See AITopics Home Page for history in the making ! \n Selected References \n \n Buchanan, Bruce G. A (Very) Brief History of Artificial Intelligence . AI Magazine 26(4): Winter 2005, 53–60. \n Cohen, Jonathan. Human Robots in Myth and Science . NY: A.S.Barnes, 1967. \n Feigenbaum, E.A. &amp; Feldman, J. (eds.) Computers and Though t . NY: McGraw-Hill, 1963. \n Gardner, Martin. Logic Machines and Diagrams . NY: McGraw-Hill, 1958. \n McCorduck, Pamela. Machines Who Think . San Francisco: W.H. Freeman, 1979.",
    "raw_content": "Introduction \n The intellectual roots of AI, and the concept of intelligent machines, may be found in Greek mythology. Intelligent artifacts appear in literature since then, with real (and fraudulent) mechanical devices actually demonstrated to behave with some degree of intelligence. Some of these conceptual achievements are listed below under \"Ancient History.\" \n After modern computers became available, following World War II, it has become possible to create programs that perform difficult intellectual tasks. From these programs, general tools are constructed which have applications in a wide variety of everday problems. Some of these computational milestones are listed below under \"Modern History.\" \n Ancient History \n Greek myths of Hephaestus, the blacksmith who manufactured mechanical servants, and the bronze man Talos incorporate the idea of intelligent robots. Many other myths in antiquity involve human-like artifacts. Many mechanical toys and models were actually constructed, e.g., by Archytas of Tarentum , Hero, Daedalus and other real persons. \n 4th century B.C. \n \n Aristotle invented syllogistic logic, the first formal deductive reasoning system. \n \n 13th century \n \n Talking heads were said to have been created, Roger Bacon and Albert the Great reputedly among the owners. \n Ramon Lull , Spanish theologian, invented machines for discovering nonmathematical truths through combinatorics. \n In 1206 A.D., Al-Jazari , an Arab inventor, designed what is believed to be the first programmable humanoid robot, a boat carrying four mechanical musicians powered by water flow. \n \n 15th century \n Invention of printing using moveable type. Gutenberg Bible printed (1456). \n \n 15th-16th century \n Clocks, the first modern measuring machines, were first produced using lathes. \n \n 16th century \n Clockmakers extended their craft to creating mechanical animals and other novelties. For example, see DaVinci's walking lion (1515) . \n Rabbi Loew of Prague is said to have invented the Golem , a clay man brought to life (1580). \n \n 17th century \n Early in the century, Descartes proposed that bodies of animals are nothing more than complex machines. Many other 17th century thinkers offered variations and elaborations of Cartesian mechanism. \n Pascal created the first mechanical digital calculating machine (1642). \n Thomas Hobbes published The Leviathan (1651), containing a mechanistic and combinatorial theory of thinking. \n Arithmetical machines devised by Sir Samuel Morland between 1662 and 1666 \n Leibniz improved Pascal's machine to do multiplication &amp; division with a machine called the Step Reckoner (1673) and envisioned a universal calculus of reasoning by which arguments could be decided mechanically. \n \n 18th century \n The 18th century saw a profusion of mechanical toys, including the celebrated mechanical duck of Vaucanson and von Kempelen's phony mechanical chess player, The Turk (1769). Edgar Allen Poe wrote (in the Southern Literary Messenger, April 1836) that the Turk could not be a machine because, if it were, it would not lose. \n \n 19th century \n \n Joseph-Marie Jacquard invented the Jacquard loom , the first programmable machine, with instructions on punched cards (1801). \n Luddites (by Marjie Bloy, PhD. Victorian Web) (led by Ned Ludd) destroyed machinery in England (1811-1816). See also What the Luddites Really Fought Against . By Richard Conniff, Smithsonian magazine (March 2011). \n Mary Shelley published the story of Frankenstein's monster (1818). The book Frankenstein, or the Modern Prometheus available from Project Gutenberg. \n Charles Babbage &amp; Ada Byron (Lady Lovelace) designed a programmable mechanical calculating machines, the Analytical Engine (1832). A working model was built in 2002; a short video shows it working. \n George Boole developed a binary algebra representing (some) \"laws of thought,\" published in The Laws of Thought (1854). \n Modern propositional logic developed by Gottlob Frege in his 1879 work Begriffsschrift and later clarified and expanded by Russell , Tarski , Godel , Church and others. \n \n 20th century - First Half \n \n Bertrand Russell and Alfred North Whitehead published Principia Mathematica , which revolutionaized formal logic. Russell, Ludwig Wittgenstein, and Rudolf Carnap lead philosophy into logical analysis of knowledge. \n Torres y Quevedo built his chess machine 'Ajedrecista', using electromagnets under the board to play the endgame rook and king against the lone king, possibly the first computer game (1912). \n Karel Capek's play \"R.U.R.\" (Rossum's Universal Robots) produced in 1921 (London opening, 1923). - First use of the word 'robot' in English. \n Alan Turing proposed the universal Turing machine (1936-37) \n Electro, a mechanical man, introduced by Westinghouse Electricat the World's Fair in New York (1939), along with Sparko, a mechanical dog. \n Warren McCulloch &amp; Walter Pitts publish \"A Logical Calculus of the Ideas Immanent in Nervous Activity\" (1943), laying foundations for neural networks. \n Arturo Rosenblueth, Norbert Wiener &amp; Julian Bigelow coin the term \"cybernetics\" in a 1943 paper. Wiener's popular book by that name published in 1948. \n Emil Post proves that production systems are a general computational mechanism (1943). See Ch.2 of Rule Based Expert Systems for the uses of production systems in AI. Post also did important work on completeness, inconsistency, and proof theory. \n George Polya published his best-selling book on thinking heuristically, How to Solve It in 1945. This book introduced the term 'heuristic' into modern thinking and has influenced many AI scientists. \n Vannevar Bush published As We May Think (Atlantic Monthly, July 1945) a prescient vision of the future in which computers assist humans in many activities. \n Grey Walter experimented with autonomous robots, turtles named Elsie and Elmer, at Bristol (1948-49) based on the premise that a small number of brain cells could give rise to complex behaviors. \n A.M. Turing published \"Computing Machinery and Intelligence\" (1950) . - Introduction of Turing Test as a way of operationalizing a test of intelligent behavior. See The Turing Institute for more on Turing. \n Claude Shannon published detailed analysis of chess playing as search in \"Programming a computer to play chess\" (1950). \n Isaac Asimov published his three laws of robotics (1950). \n \n Modern History \n The modern history of AI begins with the development of stored-program electronic computers. For a short summary, see Genius and Tragedy at Dawn of Computer Age By ALICE RAWSTHORN, NY Times (March 25, 2012 ), a review of technology historian George Dyson's book “Turing’s Cathedral: The Origins of the Digital Universe.” \n 1956 John McCarthy coined the term \"artificial intelligence\" as the topic of the Dartmouth Conference , the first conference devoted to the subject. \n Demonstration of the first running AI program, the Logic Theorist (LT) written by Allen Newell, J.C. Shaw and Herbert Simon (Carnegie Institute of Technology, now Carnegie Mellon University). See Over the holidays 50 years ago, two scientists hatched artificial intelligence . \n 1957 The General Problem Solver (GPS) demonstrated by Newell, Shaw &amp; Simon. \n 1952-62 Arthur Samuel (IBM) wrote the first game-playing program, for checkers, to achieve sufficient skill to challenge a world champion. Samuel's machine learning programs were responsible for the high performance of the checkers player. \n 1958 John McCarthy (MIT) invented the Lisp language. \n Herb Gelernter &amp; Nathan Rochester (IBM) described a theorem prover in geometry that exploits a semantic model of the domain in the form of diagrams of \"typical\" cases. \n Teddington Conference on the Mechanization of Thought Processes was held in the UK and among the papers presented were John McCarthy's Programs with Common Sense , \" Oliver Selfridge's \"Pandemonium,\" and Marvin Minsky's \"Some Methods of Heuristic Programming and Artificial Intelligence.\" \n Late 50's &amp; Early 60's Margaret Masterman &amp; colleagues at Cambridge design semantic nets for machine translation . See Themes in the work of Margaret Masterman by Yorick Wilks (1988). \n 1961 James Slagle (PhD dissertation, MIT) wrote (in Lisp) the first symbolic integration program, SAINT, which solved calculus problems at the college freshman level. \n 1962 First industrial robot company, Unimation, founded. \n 1963 Thomas Evans' program, ANALOGY, written as part of his PhD work at MIT, demonstrated that computers can solve the same analogy problems as are given on IQ tests. \n Ivan Sutherland's MIT dissertation on Sketchpad introduced the idea of interactive graphics into computing. \n Edward A. Feigenbaum &amp; Julian Feldman published Computers and Thought , the first collection of articles about artificial intelligence. \n 1964 Danny Bobrow's dissertation at MIT (tech.report #1 from MIT's AI group, Project MAC), shows that computers can understand natural language well enough to solve algebra word problems correctly. \n Bert Raphael's MIT dissertation on the SIR program demonstrates the power of a logical representation of knowledge for question-answering systems \n 1965 J. Alan Robinson invented a mechanical proof procedure, the Resolution Method, which allowed programs to work efficiently with formal logic as a representation language. (See Carl Hewitt's downloadable PDF file Middle History of Logic Programming ). \n Joseph Weizenbaum (MIT) built ELIZA, an interactive program that carries on a dialogue in English on any topic. It was a popular toy at AI centers on the ARPA-net when a version that \"simulated\" the dialogue of a psychotherapist was programmed. \n 1966 Ross Quillian (PhD dissertation, Carnegie Inst. of Technology; now CMU) demonstrated semantic nets. \n First Machine Intelligence workshop at Edinburgh - the first of an influential annual series organized by Donald Michie and others. \n Negative report on machine translation kills much work in Natural Language Processing (NLP) for many years. \n 1967 Dendral program (Edward Feigenbaum, Joshua Lederberg, Bruce Buchanan, Georgia Sutherland at Stanford) demonstrated to interpret mass spectra on organic chemical compounds. First successful knowledge-based program for scientific reasoning. \n Joel Moses (PhD work at MIT) demonstrated the power of symbolic reasoning for integration problems in the Macsyma (PDF file) program. First successful knowledge-based program in mathematics. \n Richard Greenblatt at MIT built a knowledge-based chess-playing program, MacHack, that was good enough to achieve a class-C rating in tournament play. \n Late 60s Doug Engelbart invented the mouse at SRI. \n 1968 Marvin Minsky &amp; Seymour Papert publish Perceptrons, demonstrating limits of simple neural nets. \n 1969 SRI robot, Shakey, demonstrated combining locomotion, perception and problem solving. \n Roger Schank (Stanford) defined conceptual dependency model for natural language understanding. Later developed (in PhD dissertations at Yale) for use in story understanding by Robert Wilensky and Wendy Lehnert, and for use in understanding memory by Janet Kolodner. \n First International Joint Conference on Artificial Intelligence (IJCAI) held in Washington, D.C. \n 1970 Jaime Carbonell (Sr.) developed SCHOLAR, an interactive program for computer-aided instruction based on semantic nets as the representation of knowledge. \n Bill Woods described Augmented Transition Networks (ATN's) as a representation for natural language understanding. \n Patrick Winston's PhD program, ARCH, at MIT learned concepts from examples in the world of children's blocks. \n Early 70's Jane Robinson &amp; Don Walker established influential Natural Language Processing group at SRI. \n 1971 Terry Winograd's PhD thesis (MIT) demonstrated the ability of computers to understand English sentences in a restricted world of children's blocks, in a coupling of his language understanding program, SHRDLU , with a robot arm that carried out instructions typed in English. \n 1972 Prolog developed by Alain Colmerauer. \n 1973 The Assembly Robotics group at Edinburgh University builds Freddy, the Famous Scottish Robot, capable of using vision to locate and assemble models. \n 1974 Ted Shortliffe's PhD dissertation on MYCIN (Stanford) demonstrated the power of rule-based systems for knowledge representation and inference in the domain of medical diagnosis and therapy. Sometimes called the first expert system. \n Earl Sacerdoti developed one of the first planning programs, ABSTRIPS, and developed techniques of hierarchical planning. \n 1975 Marvin Minsky published his widely-read and influential article on Frames as a representation of knowledge, in which many ideas about schemas and semantic links are brought together. \n The Meta-Dendral learning program produced new results in chemistry (some rules of mass spectrometry) the first scientific discoveries by a computer to be published in a refereed journal. \n Mid 70's Barbara Grosz (SRI) established limits to traditional AI approaches to discourse modeling. Subsequent work by Grosz, Bonnie Webber and Candace Sidner developed the notion of \"centering\", used in establishing focus of discourse and anaphoric references in NLP. \n Alan Kay and Adele Goldberg (Xerox PARC) developed the Smalltalk language, establishing the power of object-oriented programming and of icon-oriented interfaces. \n David Marr and MIT colleagues describe the \"primal sketch\" and its role in visual perception. \n 1976 Doug Lenat's AM program (Stanford PhD dissertation) demonstrated the discovery model (loosely-guided search for interesting conjectures). \n Randall Davis demonstrated the power of meta-level reasoning in his PhD dissertation at Stanford. \n Late 70's Stanford's SUMEX-AIM resource, headed by Ed Feigenbaum and Joshua Lederberg, demonstrates the power of the ARPAnet for scientific collaboration . \n 1978 Tom Mitchell, at Stanford, invented the concept of Version Spaces for describing the search space of a concept formation program. \n Herb Simon wins the Nobel Prize in Economics for his theory of bounded rationality , one of the cornerstones of AI known as \"satisficing\". \n The MOLGEN program, written at Stanford by Mark Stefik and Peter Friedland, demonstrated that an object-oriented representation of knowledge can be used to plan gene-cloning experiments.\n \n 1979 Mycin program, initially written as Ted Shortliffe's Ph.D. dissertation at Stanford, was demonstrated to perform at the level of experts. Bill VanMelle's PhD dissertation at Stanford demonstrated the generality of MYCIN's representation of knowledge and style of reasoning in his EMYCIN program, the model for many commercial expert system \"shells\". \n Jack Myers and Harry Pople at University of Pittsburgh developed INTERNIST, a knowledge-based medical diagnosis program based on Dr. Myers' clinical knowledge. \n Cordell Green, David Barstow, Elaine Kant and others at Stanford demonstrated the CHI system for automatic programming. \n The Stanford Cart , built by Hans Moravec, becomes the first computer-controlled, autonomous vehicle when it successfully traverses a chair-filled room and circumnavigates the Stanford AI Lab. \n Drew McDermott &amp; Jon Doyle at MIT, and John McCarthy at Stanford begin publishing work on non-monotonic logics and formal aspects of truth maintenance. \n 1980's Lisp Machines developed and marketed. \n First expert system shells and commercial applications. \n 1980 Lee Erman, Rick Hayes-Roth, Victor Lesser and Raj Reddy published the first description of the blackboard model, as the framework for the HEARSAY-II speech understanding system. \n First National Conference of the American Association of Artificial Intelligence (AAAI) held at Stanford. \n 1981 Danny Hillis designs the connection machine, a massively parallel architecture that brings new power to AI, and to computation in general. (Later founds Thinking Machines, Inc.) \n 1983 John Laird &amp; Paul Rosenbloom, working with Allen Newell, complete CMU dissertations on SOAR. \n James Allen invents the Interval Calculus, the first widely used formalization of temporal events. \n Mid 80's Neural Networks become widely used with the Backpropagation algorithm (first described by Werbos in 1974). \n 1985 The autonomous drawing program, Aaron , created by Harold Cohen, is demonstrated at the AAAI National Conference (based on more than a decade of work, and with subsequent work showing major developments). \n 1987 Marvin Minsky publishes The Society of Mind , a theoretical description of the mind as a collection of cooperating agents. \n 1989 Dean Pomerleau at CMU creates ALVINN (An Autonomous Land Vehicle in a Neural Network), which grew into the system that drove a car coast-to-coast under computer control for all but about 50 of the 2850 miles. \n 1990's Major advances in all areas of AI, with significant demonstrations in machine learning, intelligent tutoring, case-based reasoning, multi-agent planning, scheduling, uncertain reasoning, data mining, natural language understanding and translation, vision, virtual reality, games, and other topics. \n Rod Brooks' COG Project at MIT, with numerous collaborators, makes significant progress in building a humanoid robot \n TD-Gammon, a backgammon program written by Gerry Tesauro, demonstrates that reinforcement learning is powerful enough to create a championship-level game-playing program by competing favorably with world-class players. \n EQP theorem prover at Argonne National Labs proves the Robbins Conjecture in mathematics (October-November, 1996). \n The Deep Blue chess program beats the current world chess champion, Garry Kasparov, in a widely followed match and rematch (See Deep Blue Wins ). (May 11th, 1997). \n NASA’s pathfinder mission made a successful landing and the first autonomous robotics system, Sojourner, was deployed on the surface of Mars. (July 4, 1997) \n First official Robo-Cup soccer match (1997) featuring table-top matches with 40 teams of interacting robots and over 5000 spectators. \n Web crawlers and other AI-based information extraction programs become essential in widespread use of the world-wide-web. \n Demonstration of an Intelligent Room and Emotional Agents at MIT's AI Lab. Initiation of work on the Oxygen Architecture, which connects mobile and stationary computers in an adaptive network. \n 2000's Interactive robot pets (a.k.a. \"smart toys\") become commercially available, realizing the vision of the 18th cen. novelty toy makers. \n Cynthia Breazeal at MIT publishes her dissertation on Sociable Machines, describing KISMET, a robot with a face that expresses emotions. \n Stanford's autonomous vehicle, Stanley, wins DARPA Grand Challenge race. (October 2005). (See In a Grueling Desert Race, a Winner, but Not a Driver . \n The Nomad robot explores remote regions of Antarctica looking for meteorite samples. \n Today See AITopics Home Page for history in the making ! \n Selected References \n \n Buchanan, Bruce G. A (Very) Brief History of Artificial Intelligence . AI Magazine 26(4): Winter 2005, 53–60. \n Cohen, Jonathan. Human Robots in Myth and Science . NY: A.S.Barnes, 1967. \n Feigenbaum, E.A. &amp; Feldman, J. (eds.) Computers and Though t . NY: McGraw-Hill, 1963. \n Gardner, Martin. Logic Machines and Diagrams . NY: McGraw-Hill, 1958. \n McCorduck, Pamela. Machines Who Think . San Francisco: W.H. Freeman, 1979.",
    "score": 0.472382128238678,
    "source_type": "exa",
    "metadata": {
      "summary": "Artificial intelligence's conceptual roots trace back to Greek mythology's depiction of intelligent machines.  The development of modern AI followed World War II with the advent of computers, enabling the creation of programs capable of complex intellectual tasks.  Key milestones include Aristotle's development of syllogistic logic (4th century BC), Ramon Lull's combinatorics machines (13th century), Al-Jazari's programmable humanoid robot (1206 AD), and Pascal's mechanical calculator (1642).  The 17th century saw further advancements with Hobbes' mechanistic theory of thinking and the development of arithmetical machines.  The invention of the printing press and clocks also contributed to the foundational technologies underpinning AI's later development.\n",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  },
  {
    "title": "Timeline - A History of Artificial Intelligence",
    "url": "https://ahistoryofai.com/",
    "content": "This timeline of artificial intelligence highlights key moments in its development.  2019 saw Google's AlphaStar defeat professional StarCraft II players and the opening of its first African AI lab in Ghana.  2018 brought the AI Now Report, exposing unsafe practices in AI systems, and Cedric Vilani's strategy for making France an AI leader.  2017 featured Daniel Dennett's warnings about the dangers of strong AI in his book *From Bacteria to Bach and Back*.  The site provides further details on the information age, golden years, modern history, renaissance, middle ages, and antiquity of AI development.\n\n\nBy S. Hussain Ather \n Jump to: \n Information Age \n Golden years \n Modern history \n Renaissance \n Middle Ages \n Antiquity \n \n \n \n \n \n 2019 \n Google’s AlphaStar defeated pro StarCraft II players . The artificial intelligence agent could process information about visible enemy characters and its own base while analyzing multiple parts of the map simultaneously. \n \n \n 2019 \n Google opened its first Africa Artificial Intelligence lab in Ghana . The Google AI Centre in Ghana would provide solutions to the problems the continent faces related to health care and agriculture. \n \n \n 2018 \n The AI Now Report 2018 by the AI Now Institute revealed unsafe and poor practices by IBM Watson, the U.S. Immigration and Customs Enforcement, the Xinjiang Autonomous Region, and Amazon’s Rekongnition tool. more… \n \n \n 2018 \n Mathematician and Member of the French Parliament Cedric Vilani released “For a Meaningful Artificial Intelligence: Towards a French and Euoprean Strategy,” his vision and strategy to make France a leader in artificial intelligence. He covered economic policy, agile research, impacts on employment, ecological concerns, ethical issues, and inclusive, diverse artificial intelligence. \n \n \n 2017 \n Philosopher and cognitive scientist Daniel Dennett warned of the dangers of artificial intelligence in From Bacteria to Bach and Back . It’s “possible in principle” to create human-like artificial intelligence, but any strong artificial intelligence would raise issues of comprehension. Superintelligence is not as significant as philosopher Nick Bostrom believes. \n \n \n \n 2015 \n The United States Special Operations Command put forward the idea of the TALOS (Tactical Asault Light Operator Suit) , a robotic exoskeleton to augment the soldier’s senses. \n \n \n 2015 \n 5- 0: Google DeepMind’s AlphaGo beat 3 time European Go champion 2 Fan Hui. Fan described the program as “very strong and stable, it seems like a wall. … I know AlphaGo is a computer, but if no one told me, maybe I would think the player was a little strange, but a very strong player, a real person” in Nature . \n \n \n \n \n 2014 \n Swedish philosopher Nick Bostrom argued that if machine brains surpass human brains in general intelligence, their Superintelligence could surpass humans as dominant life forms. More… \n \n \n \n 2014 \n The Talos Principle: Through the philosophical video game “The Talos Principle,” the player controls a robot and addresses metaphysical puzzles such as the difference between choices by humans and those by robots as well as whether reality exists or is merely a virtual simulation. \n \n \n 2011 \n IBM’s Watson defeated Jeopardy! champions Brad Rutter and Ken Jennings. Watson used over 100 different methods of reasoning including analyzing language and investigating hypotheses. In the years since, Watson gained the abilities to read and interpret information. \n \n \n 2009 \n On the shoulders of giant robots: “Eureka machine” by Cornell scientists generated Newton’s three laws of motion in a few hours. It took years for scientist Isaac Newton to do so in 17th-century England. \n \n \n 1999 \n Carnegie Mellon roboticist Hans Moravec predicted that robots exceed humans by 2050. In his book Robot: Mere Machine to Transcendent Mind , the relationship and differences between humans and robots become muddled with dangerous implications of robots replacing humans. \n \n \n 1989 \n Mathematical physicist Roger Penrose argued in his book The Emperor’s New Mind a computer can’t re-create consciousness. Quantum mechanics, interactions of subatomic particles, plays an essential role in human consciousness and brain function. \n \n \n 1988 \n Moravec’s paradox explained that computers can easily prove theorems and solve mathematical problems that are easy for computers, but struggle with recognizing a face or moving around safely. It’s why vision and robotics sensorimotor research struggled so much during the 1970s and 1980s. \n \n \n \n 1986 \n Computer scientist Natarajan Shankar used the Nqthm theorem-prover to prove Gödel’s first incompleteness theorem . The incompleteness theorems prove that, if a computer can track a list of rules that can develop the basic laws of mathematics, then the list can’t be both consistent and complete. These theorems are among the few that computers can verify. \n \n \n 1986 \n Psychologists David Rumelhart and James McClelland introduced parallel distributed processing to use artificial neural networks to explain psychology. They emphasized the parallel nature of neural processing and the distributed nature of neural representations \n \n \n 1986 \n Carnegie Mellon University engineers built Navlab, the first autonomous car. The vehicle used five racks of computer hardware, video hardware, a GPS receiver, and a Warp supercomputer. It reached a top speed of 20 mph, or about 32 km/h. \n \n \n \n 1982 \n Physicist John Hopfield and psychologists Geoffrey Hinton and David Rumelhartm made discoveries that revived connectionism , a method of explaining mental phenomena by simulating neural networks . Hopfield proved “Hopfield nets” could learn and process information in new ways while Hinton and Rumelhart found new methods to train neural networks. \n \n \n 1980 \n Research scientist Kunihiko Fukushima published his work on the neocognitron , a deep convolutional neural network . Convolutional networks recognize visual patterns, and the neocognitron self-organized to recognize images by geometrical similarity of their shapes regardless of position. \n \n \n 1980 \n Philosopher John Searle formulated the Chinese room argument to discredit the idea that a computer can be programmed with the appropriate functions to behave the same way a human mind would. more… \n \n \n \n 1980 \n The father of expert systems , computer scientist Edward Feigenbaum developed a computer that makes decisions as a human can. They use rules to reason through knowledge. more… \n \n \n \n \n \n \n \n 1972 \n “Domo arigato, Mr. Roboto”: Waseda University in Tokyo created the android WABOT-1, or WAseda roBOT, the first full-scale intelligent humanoid robot. It could walk, grip objects, speak Japanese, and listen. It could measure how far away it was from certain objects using its vision and auditory senses. \n \n \n \n 1970 \n Japanese robotics professor Masahiro Mori coined “Uncanny Valley” to describe the “strangeness” of the emotional response to human-like robots. more… \n \n \n 1966 \n PhD student at Carnegie Mellon University Ross Qullian showed semantic networks could use graphs to model the structure and storage of human knowledge. Quillian hoped to explore the meaning of English words through their relationships. more… \n \n \n 1965 \n Computer scientist Edward Feigenbaum created the first expert system, Dendral , a decade-long project to engineer software to deduce the molecular structure of organic compounds using scientific instrument data. more… \n \n \n 1965 \n Philosopher-mathematician Alan Robinson created the Resolution method that let programs work efficiently with formal logic as a representation language in solving mathematical proofs. \n \n \n 1965 \n MIT computer scientist Joseph Weizenbaum created the computer program ELIZA , named after Eliza Doolittle of George Bernard Shaw’s play Pygmalion . While Doolittle was taught to speak in an upper-class English accent, ELIZA communicated in English on everything and even simulated a psychotherapist dialogue. \n \n \n 1965 \n Philosopher-mathematician John Alan Robinson created the complete algorithm for logical reasoning . It let computers solve equations and test arguments with symbols such as + and →. \n \n \n 1964 \n Philosopher John Lucas argued that Gödel’s first incompleteness theorem entails no computer can reach human-like intelligence in his paper “Minds, Machines, and Gödel.” For any automaton, there’s a mathematical formula it can’t prove, but a human can. \n \n \n 1963 \n John McCarthy started Project MAC, which would later become the MIT Artificial Intelligence Lab . The research would contribute to cognition, computer vision, decision theory, distributed systems, machine learning, multi-agent systems, neural networks , probabilistic inference, and robotics. Later that year, McCarthy and Marvin Minskey launched SAIL: Stanford Artificial Intelligence Laboratory . The research institute would pave the way for operating systems, artificial intelligence, and the theory of computation. \n \n \n 1959 \n Economist Herbert Simon, programmer J. C. Shaw, and computer scientist Allen Newell create the General Problem Solver based on logic machine architecture . It solved basic problems, but suffered from similar issues as other research in the golden years. There was an astronomical number of combinations by which a computer could search space through heuristics. \n \n \n 1956 \n Computer scientist John McCarthy coined the term “artificial intelligence” at the Dartmouth College Summer Research Project on Artificial Intelligence, organized by McCarthy, cognitive scientist Marvin Minsky, computer scientist Nathan Rochester, and mathematician Claude Shannon. This ushered in the golden years of artificial intelligence. \n \n \n \n \n \n 1955 \n The Logic Theorist (LT) , the first running artificial intelligence program, demonstrated by Allen Newell, J.C. Shaw, and Hertbert Simon at Carnegie Institute of Technology. Newell and Simon began to create the Logic Theorist before the existence of the field of artificial intelligence itself. more… \n \n \n 1950 \n English philosopher-mathematician Alan Turing created the Turing Test to ask, “Can machines think?” in his paper “Computer Machinery and Intelligence.” Two years later, in a BBC Radio Broadcast, Turing considered a similar problem of whether a jury could ask questions to a computer such that the computer would respond to convince them it is really a person. more… \n \n \n 1950 \n Writer Isaac Asimov published I, Robot to share themes of human-robot interactions. In the stories, “robopsychologist” Dr. Calvin created robots and studied how they behaved in comparison to humans. \n The collection of stories also contained the three laws of robotics : \n \n A robot may not injure a human being or, through inaction, allow a human being to come to harm. \n A robot must obey orders given it by human beings except where such orders would conflict with the First Law. \n A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. \n \n \n \n \n 1945 \n How to Solve it by mathematician George Poly: this book introduced the term ‘heuristic’ into modern thinking. Computer scientists, philosophers, psychologists, and other scholars would describe heuristics as rough methods of reasoning – human or computer. \n \n \n 1944 \n Mathematician-physicist John von Neumann and economist Oskar Morgenstern introduced game theory , or how artificial intelligence makes rational decisions, in their book Theory of Games and Economic Behavior . Teaching computers to play chess and maximize values would lead to innovation in both computer science and economics. \n \n \n 1943 \n Scientists Warren McCulloch and Warren Pitts developed the first artificial neuron, a mathematical model of a biological neuron. It would later become fundamental to neural networks in the field of machine learning. \n \n \n 1940 \n Nuclear scientist Edward Uhler Condon created the Nimatron , a computer that played Nim and one of the earliest examples of artificial intelligence in video games. Over 50,000 people played it over six months while only a few beat it. Digital logic as a toy would become central to artificial intelligence in video games. \n \n \n 1939 \n Westinghouse Electric Corporation unveiled Elektro the robot that could walk, talk, and smoke cigarettes. \n \n \n \n 1936 \n English philosopher-mathematician created the Turing Machine , a device consisting of a tape, an infinite line of cells, and a head, an active element that moves along it. Turing Machines are abstract devices to simulate logic and test theoretical ideas about the potential of computers. more… \n \n \n 1920 \n Czech writer Karl Čapek wrote the science fiction play R.U.R., Rossumovi Univerzální Roboti (Rossum’s Universal Robots) , which introdcued “robot” into English. In the play, a factory makes robots, artificial people of flesh and blood, that rise up to destroy the human race. It reflected the Fordist assembly line and fears post-World War I. Though these were not mechanical beings, “robot” would replace “automaton” and “android” in many languages. \n \n \n 1913 \n Principia Mathematica by mathematicians Bertrand Russell and Alfred North Whitehead revolutionized formal logic. It laid the foundations for type checking and type inference algorithms later used in theorem-proving computers and formalized mathematics in computer programming lunges. \n \n \n \n 1912 \n Spanish mathematician Leonardo Torres y Quevedo built the first computer game, El Ajedrecista (The Chessplayer) , which used mechanical arms to move chess pieces. Though it didn’t use the minimum number of moves to win, it did aim for an endgame of three chess pieces. It would then move a white king and a rook to checkmate the black king moved by a human opponent. After spending years speculating about how automata could think like humans, he replaced the arms with electromagnets in 1920. \n \n \n 1900 \n Artificial emotion: American author L. Frank Baum wrote The Wonderful Wizard of Oz , in which the Tin Woodman desired a heart through his philosophical debates with the Scarecrow and the Cowardly Lion. Though he lacked a brain, Woodman cared much more about having no heart. Despite this shortcoming, he displayed tender emotion much more than Dorothy’s other companions. \n \n \n 1881 \n Italian authorist Carlo Collodi published “The Adventures of Pinnochio” through stories in the magazine Giornale per i bambini . He shared the story of a toy with emotions and judgements yearning to become human. \n \n \n 1863 \n English author Samuel Butler speculated machines would evolve like humans and overtake them as a superior race . Influenced by biologist Charles Darwin’s 1859 work On the Origin of Species , Butler’s letter to a New Zealand newspaper The Press outlined the themes of an explosive technological change. It would create a singularity in which computers overtake humans, similar to philosopher Nick Bostrom’s superintelligence . \n \n \n 1822 \n Mathematician-philosopher Charles Babbages invented the difference engine , a calculator that performed functions and calculate values automatically. Though it didn’t work as well as Babbage had envisioned, it laid the foundation for automated computation. \n \n \n 1818 \n Frankenstein by novelist Mary Shelley : with philosophy, literature, science, and history, Shelley speculated how humans would attempt to use scientific progress to tamper with nature. Frankenstein and his rejected monster remain central to debates about fetal tissue research, life extension, human cloning, and artificial intelligence. more… \n \n \n 1804 \n Jacquard loom , the first programmable machine, invented by merchant Joseph-Marie Jacquard using instructions on punched cards. These mechanical devices output a series of punched cards. Creating new patterns and cards were precursors to computer programming and data entry. Mathematician-philosopher Charles Babbage knew of Jacquard looms and planned to use cards to store programs in his Analytical Engine. \n \n \n 1796 \n Japanese craftsman Hisashige Tanaka created mechanical toys that served tea, fired arrows, and painted Japanese kanji characters. The dolls would even bow in Japanese tradition. People used them in their homes and during religious festivals. He outlined these automaton in Karakuri Zui (Illustrated Machinery) . The word “karakuri” means “trick or “mechanisms” in showing the awe it evoked, similar to the sebas (awe) the ancient Greeks described. \n \n \n \n 1770 \n Hungarian inventor Wolfgang von Kempelen created the Mechanical Turk , in which a human would hide inside a machine to make it appear as though an automata were playing chess and fool Empress Maria Theresa of Austria. \n \n \n 1750 \n French physician-philosopher Julien Offray de La Mettrie argued only the mechanical workings of the brain govern cognition in L’Homme Machine . This mechanistic realism countered Descartes’ duality of the mind and body . Complicated physical interactions in the brain governed thought. This meant humans behaved like machines, but also very much like other animals. \n \n \n 1673 \n French philosopher-mathematician René Descartes wrote, in Discourse on the Method , automata can’t respond to things the way a human can. This concept of what humans have the machines lack would serve the basis for the Turing test . As he studied physics and theology, he believed mind and body are separate. more… \n \n \n \n 1641 \n English philosopher Thomas Hobbes presented a materialistic explanation of the mind . Describing thinking and cognition as mechanical sums of physical processes, similar to the way robots, automata, androids, and other forms of artificial intelligence think. \n \n \n \n \n \n \n 1596 \n In his poem, “The Faerie Queen” , English writer Edmund Spenser described the invincible, incessant Iron Knight Talus robot going on a killing spree. The story posed questions such as whether morals can be interpreted through mechanized processes and whether machines can understand justice and compassion. Such an “artificial moral agent” (AMA) would continue to take place in debates among ethicists and scientists for centuries. \n \n \n 1533 \n In 1533, German mathematician-astronomer Johannes Müller von Königsberg, more commonly known as his Latin pseudonym Regiomontanus , built an iron automata eagle that could fly. Historical records are sparse, but, by one account, the eagle greeted and followed a visiting emperor. \n \n \n 1495 \n Italian Renaissance polymath Leonardo da Vinci showed his “mechanical knight.” Researchers at the University of California have speculated the significance of some of da Vinci’s markings by reading his technical drawings. Using pulleys and cables, the “knight” would move its arms and legs like a human. \n \n \n \n \n \n 1200s \n Majorcan philosopher-mathematician Ramon Llull developed logical machines devoted to combine basic, undeniable truths by simple logical operations. They were produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. The “Lullian Circle” was a paper machine that rotated circles to combine symbols with one another. Llull’s work had a great influence on philosopher Gottfried Leibniz, who argued mechanical calculation can amount to reasoning. \n \n \n 1206 \n The first programmable humanoid robot invented by polymath Al-Jazari: a water powered boat that carried four mechanical musicians. With a drum machine of pegs knocking on other bars, computer scientist Noel Sharkey re-created the mechanism by which they operated. Sharkey also argued it was likely an early programmable automata. \n \n \n Medieval Europe \n Medieval Europeans created brazen heads , talking automaton that could speak phrases like “Time is. Time was. Time is past.” German Catholic bishop Albertus Magnus constructed an android that performed domestic tasks, philosopher Thomas Aquinas destroyed it for disturbing his work. According to legend, English philosopher Roger Bacon created a head that could answer any question. \n \n \n Talmudic period \n Jewish writers studied the Sefer Yetzirah , the Book of Creation, to bring golems to life. They believed they could infuse life to golems, rock statues, by writing a name of God on paper and inserting it into a golem. Jewish philosopher-poet Solomon Ibn Gabirol created a maid this way. When the king wanted to punish him for this, Ibn Gabirol showed the maid wasn’t real by turning it back into rock. Golems would continue to serve an automated force in Jewish tradition representing villains or heroes and hope or despair. \n \n \n ~800 \n Arab polymath Jabir ibn Hayyan, more commonly known as Geber, created the Muslim alchemical goal of “Takwin” to artificially create human life. It was a way to copy God’s divine power. This had roots in Islamic belief, but also in both magic and alchemical science. He even used an alchemical system of symbols to communicate his work. \n \n \n \n \n \n Antiquity \n Greek myths of Hephaestus and Daedalus incorporated the idea of intelligent robots and artificial beings like Pandora. These “automaton” beings such as Talos would protect Crete from invaders. Ancient Greeks spoke of the idea of ‘biotechne’, or how biological phenomena such as aging can be altered with technology of humans. Through their myths and stories, scholars wrestled with what it meant to be human and how man can push his biological limits to what he can give life to. more… \n \n \n \n This website uses cookies.",
    "raw_content": "By S. Hussain Ather \n Jump to: \n Information Age \n Golden years \n Modern history \n Renaissance \n Middle Ages \n Antiquity \n \n \n \n \n \n 2019 \n Google’s AlphaStar defeated pro StarCraft II players . The artificial intelligence agent could process information about visible enemy characters and its own base while analyzing multiple parts of the map simultaneously. \n \n \n 2019 \n Google opened its first Africa Artificial Intelligence lab in Ghana . The Google AI Centre in Ghana would provide solutions to the problems the continent faces related to health care and agriculture. \n \n \n 2018 \n The AI Now Report 2018 by the AI Now Institute revealed unsafe and poor practices by IBM Watson, the U.S. Immigration and Customs Enforcement, the Xinjiang Autonomous Region, and Amazon’s Rekongnition tool. more… \n \n \n 2018 \n Mathematician and Member of the French Parliament Cedric Vilani released “For a Meaningful Artificial Intelligence: Towards a French and Euoprean Strategy,” his vision and strategy to make France a leader in artificial intelligence. He covered economic policy, agile research, impacts on employment, ecological concerns, ethical issues, and inclusive, diverse artificial intelligence. \n \n \n 2017 \n Philosopher and cognitive scientist Daniel Dennett warned of the dangers of artificial intelligence in From Bacteria to Bach and Back . It’s “possible in principle” to create human-like artificial intelligence, but any strong artificial intelligence would raise issues of comprehension. Superintelligence is not as significant as philosopher Nick Bostrom believes. \n \n \n \n 2015 \n The United States Special Operations Command put forward the idea of the TALOS (Tactical Asault Light Operator Suit) , a robotic exoskeleton to augment the soldier’s senses. \n \n \n 2015 \n 5- 0: Google DeepMind’s AlphaGo beat 3 time European Go champion 2 Fan Hui. Fan described the program as “very strong and stable, it seems like a wall. … I know AlphaGo is a computer, but if no one told me, maybe I would think the player was a little strange, but a very strong player, a real person” in Nature . \n \n \n \n \n 2014 \n Swedish philosopher Nick Bostrom argued that if machine brains surpass human brains in general intelligence, their Superintelligence could surpass humans as dominant life forms. More… \n \n \n \n 2014 \n The Talos Principle: Through the philosophical video game “The Talos Principle,” the player controls a robot and addresses metaphysical puzzles such as the difference between choices by humans and those by robots as well as whether reality exists or is merely a virtual simulation. \n \n \n 2011 \n IBM’s Watson defeated Jeopardy! champions Brad Rutter and Ken Jennings. Watson used over 100 different methods of reasoning including analyzing language and investigating hypotheses. In the years since, Watson gained the abilities to read and interpret information. \n \n \n 2009 \n On the shoulders of giant robots: “Eureka machine” by Cornell scientists generated Newton’s three laws of motion in a few hours. It took years for scientist Isaac Newton to do so in 17th-century England. \n \n \n 1999 \n Carnegie Mellon roboticist Hans Moravec predicted that robots exceed humans by 2050. In his book Robot: Mere Machine to Transcendent Mind , the relationship and differences between humans and robots become muddled with dangerous implications of robots replacing humans. \n \n \n 1989 \n Mathematical physicist Roger Penrose argued in his book The Emperor’s New Mind a computer can’t re-create consciousness. Quantum mechanics, interactions of subatomic particles, plays an essential role in human consciousness and brain function. \n \n \n 1988 \n Moravec’s paradox explained that computers can easily prove theorems and solve mathematical problems that are easy for computers, but struggle with recognizing a face or moving around safely. It’s why vision and robotics sensorimotor research struggled so much during the 1970s and 1980s. \n \n \n \n 1986 \n Computer scientist Natarajan Shankar used the Nqthm theorem-prover to prove Gödel’s first incompleteness theorem . The incompleteness theorems prove that, if a computer can track a list of rules that can develop the basic laws of mathematics, then the list can’t be both consistent and complete. These theorems are among the few that computers can verify. \n \n \n 1986 \n Psychologists David Rumelhart and James McClelland introduced parallel distributed processing to use artificial neural networks to explain psychology. They emphasized the parallel nature of neural processing and the distributed nature of neural representations \n \n \n 1986 \n Carnegie Mellon University engineers built Navlab, the first autonomous car. The vehicle used five racks of computer hardware, video hardware, a GPS receiver, and a Warp supercomputer. It reached a top speed of 20 mph, or about 32 km/h. \n \n \n \n 1982 \n Physicist John Hopfield and psychologists Geoffrey Hinton and David Rumelhartm made discoveries that revived connectionism , a method of explaining mental phenomena by simulating neural networks . Hopfield proved “Hopfield nets” could learn and process information in new ways while Hinton and Rumelhart found new methods to train neural networks. \n \n \n 1980 \n Research scientist Kunihiko Fukushima published his work on the neocognitron , a deep convolutional neural network . Convolutional networks recognize visual patterns, and the neocognitron self-organized to recognize images by geometrical similarity of their shapes regardless of position. \n \n \n 1980 \n Philosopher John Searle formulated the Chinese room argument to discredit the idea that a computer can be programmed with the appropriate functions to behave the same way a human mind would. more… \n \n \n \n 1980 \n The father of expert systems , computer scientist Edward Feigenbaum developed a computer that makes decisions as a human can. They use rules to reason through knowledge. more… \n \n \n \n \n \n \n \n 1972 \n “Domo arigato, Mr. Roboto”: Waseda University in Tokyo created the android WABOT-1, or WAseda roBOT, the first full-scale intelligent humanoid robot. It could walk, grip objects, speak Japanese, and listen. It could measure how far away it was from certain objects using its vision and auditory senses. \n \n \n \n 1970 \n Japanese robotics professor Masahiro Mori coined “Uncanny Valley” to describe the “strangeness” of the emotional response to human-like robots. more… \n \n \n 1966 \n PhD student at Carnegie Mellon University Ross Qullian showed semantic networks could use graphs to model the structure and storage of human knowledge. Quillian hoped to explore the meaning of English words through their relationships. more… \n \n \n 1965 \n Computer scientist Edward Feigenbaum created the first expert system, Dendral , a decade-long project to engineer software to deduce the molecular structure of organic compounds using scientific instrument data. more… \n \n \n 1965 \n Philosopher-mathematician Alan Robinson created the Resolution method that let programs work efficiently with formal logic as a representation language in solving mathematical proofs. \n \n \n 1965 \n MIT computer scientist Joseph Weizenbaum created the computer program ELIZA , named after Eliza Doolittle of George Bernard Shaw’s play Pygmalion . While Doolittle was taught to speak in an upper-class English accent, ELIZA communicated in English on everything and even simulated a psychotherapist dialogue. \n \n \n 1965 \n Philosopher-mathematician John Alan Robinson created the complete algorithm for logical reasoning . It let computers solve equations and test arguments with symbols such as + and →. \n \n \n 1964 \n Philosopher John Lucas argued that Gödel’s first incompleteness theorem entails no computer can reach human-like intelligence in his paper “Minds, Machines, and Gödel.” For any automaton, there’s a mathematical formula it can’t prove, but a human can. \n \n \n 1963 \n John McCarthy started Project MAC, which would later become the MIT Artificial Intelligence Lab . The research would contribute to cognition, computer vision, decision theory, distributed systems, machine learning, multi-agent systems, neural networks , probabilistic inference, and robotics. Later that year, McCarthy and Marvin Minskey launched SAIL: Stanford Artificial Intelligence Laboratory . The research institute would pave the way for operating systems, artificial intelligence, and the theory of computation. \n \n \n 1959 \n Economist Herbert Simon, programmer J. C. Shaw, and computer scientist Allen Newell create the General Problem Solver based on logic machine architecture . It solved basic problems, but suffered from similar issues as other research in the golden years. There was an astronomical number of combinations by which a computer could search space through heuristics. \n \n \n 1956 \n Computer scientist John McCarthy coined the term “artificial intelligence” at the Dartmouth College Summer Research Project on Artificial Intelligence, organized by McCarthy, cognitive scientist Marvin Minsky, computer scientist Nathan Rochester, and mathematician Claude Shannon. This ushered in the golden years of artificial intelligence. \n \n \n \n \n \n 1955 \n The Logic Theorist (LT) , the first running artificial intelligence program, demonstrated by Allen Newell, J.C. Shaw, and Hertbert Simon at Carnegie Institute of Technology. Newell and Simon began to create the Logic Theorist before the existence of the field of artificial intelligence itself. more… \n \n \n 1950 \n English philosopher-mathematician Alan Turing created the Turing Test to ask, “Can machines think?” in his paper “Computer Machinery and Intelligence.” Two years later, in a BBC Radio Broadcast, Turing considered a similar problem of whether a jury could ask questions to a computer such that the computer would respond to convince them it is really a person. more… \n \n \n 1950 \n Writer Isaac Asimov published I, Robot to share themes of human-robot interactions. In the stories, “robopsychologist” Dr. Calvin created robots and studied how they behaved in comparison to humans. \n The collection of stories also contained the three laws of robotics : \n \n A robot may not injure a human being or, through inaction, allow a human being to come to harm. \n A robot must obey orders given it by human beings except where such orders would conflict with the First Law. \n A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. \n \n \n \n \n 1945 \n How to Solve it by mathematician George Poly: this book introduced the term ‘heuristic’ into modern thinking. Computer scientists, philosophers, psychologists, and other scholars would describe heuristics as rough methods of reasoning – human or computer. \n \n \n 1944 \n Mathematician-physicist John von Neumann and economist Oskar Morgenstern introduced game theory , or how artificial intelligence makes rational decisions, in their book Theory of Games and Economic Behavior . Teaching computers to play chess and maximize values would lead to innovation in both computer science and economics. \n \n \n 1943 \n Scientists Warren McCulloch and Warren Pitts developed the first artificial neuron, a mathematical model of a biological neuron. It would later become fundamental to neural networks in the field of machine learning. \n \n \n 1940 \n Nuclear scientist Edward Uhler Condon created the Nimatron , a computer that played Nim and one of the earliest examples of artificial intelligence in video games. Over 50,000 people played it over six months while only a few beat it. Digital logic as a toy would become central to artificial intelligence in video games. \n \n \n 1939 \n Westinghouse Electric Corporation unveiled Elektro the robot that could walk, talk, and smoke cigarettes. \n \n \n \n 1936 \n English philosopher-mathematician created the Turing Machine , a device consisting of a tape, an infinite line of cells, and a head, an active element that moves along it. Turing Machines are abstract devices to simulate logic and test theoretical ideas about the potential of computers. more… \n \n \n 1920 \n Czech writer Karl Čapek wrote the science fiction play R.U.R., Rossumovi Univerzální Roboti (Rossum’s Universal Robots) , which introdcued “robot” into English. In the play, a factory makes robots, artificial people of flesh and blood, that rise up to destroy the human race. It reflected the Fordist assembly line and fears post-World War I. Though these were not mechanical beings, “robot” would replace “automaton” and “android” in many languages. \n \n \n 1913 \n Principia Mathematica by mathematicians Bertrand Russell and Alfred North Whitehead revolutionized formal logic. It laid the foundations for type checking and type inference algorithms later used in theorem-proving computers and formalized mathematics in computer programming lunges. \n \n \n \n 1912 \n Spanish mathematician Leonardo Torres y Quevedo built the first computer game, El Ajedrecista (The Chessplayer) , which used mechanical arms to move chess pieces. Though it didn’t use the minimum number of moves to win, it did aim for an endgame of three chess pieces. It would then move a white king and a rook to checkmate the black king moved by a human opponent. After spending years speculating about how automata could think like humans, he replaced the arms with electromagnets in 1920. \n \n \n 1900 \n Artificial emotion: American author L. Frank Baum wrote The Wonderful Wizard of Oz , in which the Tin Woodman desired a heart through his philosophical debates with the Scarecrow and the Cowardly Lion. Though he lacked a brain, Woodman cared much more about having no heart. Despite this shortcoming, he displayed tender emotion much more than Dorothy’s other companions. \n \n \n 1881 \n Italian authorist Carlo Collodi published “The Adventures of Pinnochio” through stories in the magazine Giornale per i bambini . He shared the story of a toy with emotions and judgements yearning to become human. \n \n \n 1863 \n English author Samuel Butler speculated machines would evolve like humans and overtake them as a superior race . Influenced by biologist Charles Darwin’s 1859 work On the Origin of Species , Butler’s letter to a New Zealand newspaper The Press outlined the themes of an explosive technological change. It would create a singularity in which computers overtake humans, similar to philosopher Nick Bostrom’s superintelligence . \n \n \n 1822 \n Mathematician-philosopher Charles Babbages invented the difference engine , a calculator that performed functions and calculate values automatically. Though it didn’t work as well as Babbage had envisioned, it laid the foundation for automated computation. \n \n \n 1818 \n Frankenstein by novelist Mary Shelley : with philosophy, literature, science, and history, Shelley speculated how humans would attempt to use scientific progress to tamper with nature. Frankenstein and his rejected monster remain central to debates about fetal tissue research, life extension, human cloning, and artificial intelligence. more… \n \n \n 1804 \n Jacquard loom , the first programmable machine, invented by merchant Joseph-Marie Jacquard using instructions on punched cards. These mechanical devices output a series of punched cards. Creating new patterns and cards were precursors to computer programming and data entry. Mathematician-philosopher Charles Babbage knew of Jacquard looms and planned to use cards to store programs in his Analytical Engine. \n \n \n 1796 \n Japanese craftsman Hisashige Tanaka created mechanical toys that served tea, fired arrows, and painted Japanese kanji characters. The dolls would even bow in Japanese tradition. People used them in their homes and during religious festivals. He outlined these automaton in Karakuri Zui (Illustrated Machinery) . The word “karakuri” means “trick or “mechanisms” in showing the awe it evoked, similar to the sebas (awe) the ancient Greeks described. \n \n \n \n 1770 \n Hungarian inventor Wolfgang von Kempelen created the Mechanical Turk , in which a human would hide inside a machine to make it appear as though an automata were playing chess and fool Empress Maria Theresa of Austria. \n \n \n 1750 \n French physician-philosopher Julien Offray de La Mettrie argued only the mechanical workings of the brain govern cognition in L’Homme Machine . This mechanistic realism countered Descartes’ duality of the mind and body . Complicated physical interactions in the brain governed thought. This meant humans behaved like machines, but also very much like other animals. \n \n \n 1673 \n French philosopher-mathematician René Descartes wrote, in Discourse on the Method , automata can’t respond to things the way a human can. This concept of what humans have the machines lack would serve the basis for the Turing test . As he studied physics and theology, he believed mind and body are separate. more… \n \n \n \n 1641 \n English philosopher Thomas Hobbes presented a materialistic explanation of the mind . Describing thinking and cognition as mechanical sums of physical processes, similar to the way robots, automata, androids, and other forms of artificial intelligence think. \n \n \n \n \n \n \n 1596 \n In his poem, “The Faerie Queen” , English writer Edmund Spenser described the invincible, incessant Iron Knight Talus robot going on a killing spree. The story posed questions such as whether morals can be interpreted through mechanized processes and whether machines can understand justice and compassion. Such an “artificial moral agent” (AMA) would continue to take place in debates among ethicists and scientists for centuries. \n \n \n 1533 \n In 1533, German mathematician-astronomer Johannes Müller von Königsberg, more commonly known as his Latin pseudonym Regiomontanus , built an iron automata eagle that could fly. Historical records are sparse, but, by one account, the eagle greeted and followed a visiting emperor. \n \n \n 1495 \n Italian Renaissance polymath Leonardo da Vinci showed his “mechanical knight.” Researchers at the University of California have speculated the significance of some of da Vinci’s markings by reading his technical drawings. Using pulleys and cables, the “knight” would move its arms and legs like a human. \n \n \n \n \n \n 1200s \n Majorcan philosopher-mathematician Ramon Llull developed logical machines devoted to combine basic, undeniable truths by simple logical operations. They were produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. The “Lullian Circle” was a paper machine that rotated circles to combine symbols with one another. Llull’s work had a great influence on philosopher Gottfried Leibniz, who argued mechanical calculation can amount to reasoning. \n \n \n 1206 \n The first programmable humanoid robot invented by polymath Al-Jazari: a water powered boat that carried four mechanical musicians. With a drum machine of pegs knocking on other bars, computer scientist Noel Sharkey re-created the mechanism by which they operated. Sharkey also argued it was likely an early programmable automata. \n \n \n Medieval Europe \n Medieval Europeans created brazen heads , talking automaton that could speak phrases like “Time is. Time was. Time is past.” German Catholic bishop Albertus Magnus constructed an android that performed domestic tasks, philosopher Thomas Aquinas destroyed it for disturbing his work. According to legend, English philosopher Roger Bacon created a head that could answer any question. \n \n \n Talmudic period \n Jewish writers studied the Sefer Yetzirah , the Book of Creation, to bring golems to life. They believed they could infuse life to golems, rock statues, by writing a name of God on paper and inserting it into a golem. Jewish philosopher-poet Solomon Ibn Gabirol created a maid this way. When the king wanted to punish him for this, Ibn Gabirol showed the maid wasn’t real by turning it back into rock. Golems would continue to serve an automated force in Jewish tradition representing villains or heroes and hope or despair. \n \n \n ~800 \n Arab polymath Jabir ibn Hayyan, more commonly known as Geber, created the Muslim alchemical goal of “Takwin” to artificially create human life. It was a way to copy God’s divine power. This had roots in Islamic belief, but also in both magic and alchemical science. He even used an alchemical system of symbols to communicate his work. \n \n \n \n \n \n Antiquity \n Greek myths of Hephaestus and Daedalus incorporated the idea of intelligent robots and artificial beings like Pandora. These “automaton” beings such as Talos would protect Crete from invaders. Ancient Greeks spoke of the idea of ‘biotechne’, or how biological phenomena such as aging can be altered with technology of humans. Through their myths and stories, scholars wrestled with what it meant to be human and how man can push his biological limits to what he can give life to. more… \n \n \n \n This website uses cookies.",
    "score": 0.46980026364326477,
    "source_type": "exa",
    "metadata": {
      "summary": "This timeline of artificial intelligence highlights key moments in its development.  2019 saw Google's AlphaStar defeat professional StarCraft II players and the opening of its first African AI lab in Ghana.  2018 brought the AI Now Report, exposing unsafe practices in AI systems, and Cedric Vilani's strategy for making France an AI leader.  2017 featured Daniel Dennett's warnings about the dangers of strong AI in his book *From Bacteria to Bach and Back*.  The site provides further details on the information age, golden years, modern history, renaissance, middle ages, and antiquity of AI development.\n",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  },
  {
    "title": "The History of Artificial Intelligence - TecnoSophia - Medium",
    "url": "https://medium.com/tecnosophia/artificial-intelligence-an-illustrated-history-2401d9ab2f6e",
    "content": "This article traces the history of artificial intelligence (AI) from its philosophical roots to its modern applications.  It begins with Pythagoras (6th century BC), whose emphasis on numbers as the fundamental essence of the universe foreshadowed AI's reliance on numerical data representation.  Plato's Theory of Forms (4th century BC) contributed to the concept of an ideal intellectual reality, a precursor to the goal of creating intelligent systems.  The article further suggests that these early philosophical concepts laid the groundwork for the development of modern AI, which uses numerical models to process and understand information.\n\n\nFrom Pythagoras to Modern AI, AGI and Beyond Evolution of AI — All rights reserved The history of artificial intelligence (AI) is a captivating chronicle that stretches back centuries, weaving through epochs of human thought and innovation. It’s a story not just of technological milestones but of the persistent human quest to replicate, and perhaps surpass, the faculties of the mind through science, technique and technology. 6th century BC — Pythagoras and Numbers Pythagoras , a philosopher from the 6th century BC , was one of the earliest thinkers to propose that numbers could explain and represent the reality of the material world . Pythagoras — All rights reserved His teachings, along with those of his followers, the Pythagoreans , placed profound emphasis on numerology and the belief that numbers were the fundamental essence of the universe. Pythagoras viewed numbers as the ultimate truth behind all natural phenomena, marking a revolutionary idea that would eventually resonate with fields as diverse as mathematics , science , and, centuries later, artificial intelligence (AI) . Pythagoras’ belief that numbers could describe the nature of the universe laid the philosophical groundwork for the numeric representation of reality. Today, AI systems function based on numerical data to process and understand the world. Whether it’s images, text, videos , or complex business and scientific problems, all forms of information are transformed into mathematical models and represented numerically. This echoes the Pythagorean view that numbers are at the core of reality, as AI abstracts real-world entities into data points and mathematical relationships. For Pythagoras and his followers, geometry and numbers were more than tools — they were seen as a means to understand the abstract and rational principles governing the world. The Pythagoreans represented numbers graphically, engaging with geometry as a way to reveal the hidden truths of the universe. This early work in abstract mathematical thinking helped establish the basis for rational exploration, a precursor to how modern AI systems approach problem-solving. The Pythagorean legacy is visible in the way AI represents the world today. AI models break down images into pixels , texts into data vectors , and problems into equations , all using the language of numbers. Pythagoras’ view of the universe as fundamentally governed by numbers, specifically natural numbers (positive integers), was revolutionary, but it’s important to note that zero was absent from his numerical framework. The concept of zero, which we will explore later, plays a crucial role in the modern development of AI and computational systems . 4th century BC — Plato’s Theory of Ideas Plato’s Theory of Ideas , also known as the Theory of Forms , represents one of the earliest steps toward envisioning an ideal intellectual reality, which, centuries later, would underpin the development of artificial intelligence . Plato — All rights reserved Plato proposed that the physical world we perceive is only a shadow of a higher, non-physical realm composed of Ideas or Forms — timeless, unchanging essences that represent the true nature of things. For example, while we may see imperfect versions of beauty or justice in the physical world, the Form of Beauty or Form of Justice exists in an ideal, abstract realm beyond physical reality. This notion of a “non-material” world made of abstract ideas created a framework that would later resonate with the conceptual underpinnings of AI, where information, algorithms, and models — immaterial entities — became the core of intelligent systems. In a sense, AI embodies a modern version of Plato’s idea: a non-physical domain where abstract patterns, models, and knowledge exist and operate in connection to the material world. Mathematical Platonism , which extends Plato’s theory into the realm of mathematics, argues that mathematical objects — numbers, shapes, and structures — are also abstract, non-physical entities that exist independently of human thought and perception. This metaphysical perspective challenges the view that all of reality is confined to the physical world, asserting instead that abstract concepts can hold intrinsic truth and value, even if they cannot be observed directly. In the context of AI, mathematical Platonism has particular significance. The development of artificial intelligence relies heavily on mathematical models , algorithms , and data structures , all of which are abstract objects. These mathematical structures allow AI systems to perform tasks like reasoning, problem-solving, and pattern recognition — abilities that operate beyond the tangible physical world. AI’s ability to simulate human intelligence through mathematical processes echoes Plato’s vision of a reality based on mathematics and ideas. The Ultimate Definition of Artificial Intelligence By introducing the idea that abstract forms or ideas exist in a realm separate from physical reality, Plato’s Theory of Ideas laid the philosophical groundwork for understanding information and knowledge as independent entities — concepts that are foundational to AI. Just as Plato’s forms are considered more “real” than their physical counterparts, AI systems today operate on the basis of abstract representations of knowledge that drive machine learning, reasoning, and decision-making, transcending the physical limitations of the human brain and body. In essence, Plato’s philosophy anticipated a future where intellect constructs — be they forms, ideas, or algorithms — would play a central role in understanding and shaping reality, laying a conceptual foundation for the development of artificial intelligence as we know it. 400 BCE to 400 CE — Indo-Arabic numerals The Indo-Arabic numerals , also known as Arabic numerals , are the most widely used symbolic representation of numerical entities in the world and are considered a cornerstone in the development of mathematics. A key distinction can be made between the positional system they employ, also known as the Indo-Arabic numeral system , and the specific glyphs used to represent the numbers. The first numerical system originated in India between 400 BCE and 400 CE . These numerals were first transmitted to Western Asia, where they were mentioned in the 9th century , and later spread to Europe in the 10th century . The Arabs assimilated elements from various cultures they encountered and forged the Arabic-Indian numerical system, from which the modern Arabic numerals evolved. Knowledge of these numbers reached Europe through the work of Arab mathematicians and astronomers, and this evolved numerical system became known in Europe as the “Arabic numerals.” In Arabic, the Eastern Arabic numerals are referred to as “Indian numerals” (in Arabic: أرقام هندية, ’arqām hindiyya), and although the same system is used, the glyphs differ (٠١٢٣٤٥٦٧٨٩). The symbols from 0 to 9 in the Indo-Arabic numeral system evolved from the Brahmi numerals . Buddhist inscriptions from around 300 BCE used symbols that eventually became 1 , 4 , and 6 , and a century later, the symbols for 2 , 7 , and 9 were recorded. The Indo-Arabic numeral system is fundamental for AI because it introduced the positional number system . This numerical system allows for the representation and manipulation of large data sets, complex calculations, and algorithms. The numerical Matrix of reality — All rights reserved Without the numeric representation introduced by this system, the development of computers and, by extension, artificial intelligence would not be possible. 7th Century — Zero: The Most Important Number Zero (0) is not just a number; it represents a profound concept — the idea of an empty quantity . The development of zero as a written digit in decimal place value notation was a crucial advancement, originating in India . A symbol for zero, initially depicted as a solid dot, was used throughout the Bakhshali manuscript, a practical guide on arithmetic for merchants. Brahmagupta by AI — All rights reserved Brahmagupta , a 7th-century Indian mathematician, was the first to treat zero as a number like any other, formulating rules for its use in arithmetic. His work in the Brahmasputha Siddhanta included the sum of zero with itself as zero, establishing zero as a key player in the numerical system. Although he misunderstood the concept of division by zero, his pioneering treatment of zero laid the groundwork for its integration into broader mathematical thought. Zero later traveled through the Arabic world , where it was embraced and further refined, before reaching Western Europe in the 11th century. The Italian mathematician Fibonacci played a key role in introducing the Hindu–Arabic numeral system to Europe. Fibonacci’s use of the term “zephyrum” led to the modern word zero , a name that evolved through Italian influences. Zero — All rights reserved The introduction of zero revolutionized mathematics, particularly in the development of the positional number system , and became the foundation for binary code — the language of modern computers. Without zero, binary mathematics, which underpins all modern computing and artificial intelligence , would not exist. Zero allows for the representation of absence , making it a fundamental concept for data processing, machine learning, and the structure of algorithms in AI systems. In essence, the creation of zero as a mathematical entity was a key turning point that enabled the digital world and made the future of AI possible. 9th Century — The Word Algorithm was Born The word algorithm has its origins in the region of Khwãrezm (modern-day Turkmenistan and Uzbekistan ), an area known for its arid desert landscapes but sustained by the Amu Darya river, which supports large-scale irrigation. One of Khwãrezm’s most notable figures was Muhammad ibn Mūsa al-Khwarizmī (جعفر محمد بن موسی ), a 9th-century Persian scholar, astronomer, geographer, and mathematician whose work had a profound impact on the development of mathematics . He is known as the father of algebra . The Latinization of his name, meaning “the native of Khwãrezm” in Persian, gave rise to the English word algorithm . Muḥammad ibn Mūsā al-Khwārizmī — All rights reserved Al-Khwarizmī’s influential book on Hindu-Arabic numerals, originally written in Arabic, was translated into Latin as “Algoritmi de numero Indorum” (meaning “Al-Khwarizmi on the Hindu Art of Reckoning”). This Latin term, algoritmi , eventually evolved into algorithm in English. Today, the simplest definition of algorithm is a set of rules or procedures that define a sequence of operations , and it is fundamental in fields ranging from computer science to daily online activities. Algorithms power everything from Google’s search engine to Facebook’s news feed, embodying a process that has its roots in al-Khwarizmī’s groundbreaking work over a millennium ago. 1642 — Early Beginnings: Mechanical Calculators and Philosophical Foundations Our journey continue in 1642 , when French mathematician Blaise Pascal invented the Pascaline , the first mechanical calculator capable of performing basic arithmetic operations such as addition and subtraction. Although simple by today’s standards, the Pascaline was a groundbreaking device for its time, marking the first significant step toward mechanized computation. Designed to assist Pascal’s father in his work as a tax collector, the Pascaline used a series of interlocking gears and dials to automate calculations. Pascaline This invention not only reduced human error in arithmetic but also symbolized a bold shift from manual calculation to mechanical processes, laying the foundation for future developments in computational machinery. While primitive compared to modern computers, the Pascaline’s creation represented the early vision of automating logical tasks , a concept that would evolve over the centuries into the digital computers that power the modern world. Blaise Pascal — All rights Reserved In the latter half of the 17th century, German polymath Gottfried Wilhelm Leibniz advanced these ideas by developing the Stepped Reckoner , a device capable of multiplication and division. More importantly, Leibniz philosophized about a universal language of logic, an “alphabet of human thought,” envisioning a machine that could manipulate symbols as humans do — a conceptual precursor to symbolic AI. Although Leibniz lacked the technology to build such a machine, his dream was to plant to an early seed for the future of artificial intelligence. Gottfried Wilhelm von Leibniz — All rights Reserved 17th century — Binary System One of Leibniz’s most profound contributions to the history of computing was his conceptualization of a calculator based on the binary number system , an idea that was revolutionary for its time. Although the binary system had been introduced earlier by Spanish scholar Juan Caramuel, Leibniz was the first to apply it to mechanical computation. His design envisioned a machine that operated using marbles as binary markers. Gottfried Wilhelm Leibniz Bibliothek, Hannover The presence of a marble in a particular position would represent the value 1 , while its absence would signify 0 . This early attempt at binary computation laid the theoretical groundwork for modern digital computing, where binary code is the fundamental building block of all computer operations. Leibniz’s insight into the potential of binary logic and its simplicity would influence the development of logic gates and algorithms centuries later, forming a cornerstone of today’s computational theory. Flashback: 800 BCE — The I Ching as a Binary System The I Ching , one of the oldest classical Chinese texts, dating back to around 800 BCE , serves as a profound universal model based on a binary system. The I Ching’s Yin and Yang symbols represent the complementary duality of nature — Yin (dark) and Yang (light) — which can be understood as a way of counting in twos, or a binary system . Yin and Yang This binary logic, expressed through 64 hexagrams , is deeply connected to ancient Chinese cosmology and the balance of opposing forces. In the 17th century, Gottfried Wilhelm Leibniz was inspired by the I Ching when he developed the binary number system — the foundation of modern computing. The hexagrams of the I Ching in a diagram belonging to the German mathematician philosopher Gottfried Wilhelm Leibniz Leibniz recognized that the Yin and Yang symbols could be interpreted as 0 and 1 , the core of binary notation. His work laid the groundwork for binary code, still used today in computers , where 1 and 0 represent the on and off states of digital systems. Leibniz’s connection between his binary system and the I Ching highlights how ancient philosophical systems can influence modern technological advances. The I Ching’s binary structure, with its deep reflection on duality and balance , is thus not only a philosophical tool but also a precursor to the mathematical logic that underpins today’s digital world . 1752–A new World with Electricity Benjamin Franklin , an American polymath, is widely credited for his famous 1752 experiment that demonstrated the connection between lightning and electricity . By attaching a wire to a kite during a thunderstorm, Franklin showed that lightning consists of electrical energy, a breakthrough in understanding natural phenomena. However, while Franklin’s experiment was pivotal, the discovery and understanding of electricity cannot be attributed to any one person. Rather, the study of electricity evolved over centuries, with numerous scientists and thinkers making critical contributions to its development. AI generated Image of an early 20th-century city street with power poles lining both sides — All rights reserved 1834 — The Dawn of Programmable Machines The 19th century witnessed significant leaps with Charles Babbage and Ada Lovelace . Babbage designed the Analytical Engine in 1834, a general-purpose, programmable computing device. Ada Lovelace, often celebrated as the world’s first computer programmer , wrote algorithms for this machine and foresaw its potential beyond mere calculation, imagining it composing music or art — a remarkably prescient vision of software. Binary Number System — All rights Reserved In the 1850s , George Boole developed Boolean algebra , a groundbreaking system of mathematical logic that would later form the core of binary systems. Boole’s work introduced a method for representing logical statements using binary variables, with true and false values denoted as 1 and 0 , respectively. This innovation provided the theoretical foundation for the design of digital circuits and laid the groundwork for modern computing. Boolean algebra became essential for constructing computer circuits, enabling machines to perform logical operations, and it also became integral to programming languages. Boole’s insights into logic and binary reasoning remain fundamental to the architecture of today’s computers and digital technology. 1943 - The Birth of Neural Networks The mid-20th century was a pivotal period for the development of foundational AI concepts. In 1943 , Warren McCulloch and Walter Pitts published a landmark paper that introduced a model of artificial neurons , inspired by the structure and function of the human brain. Their work proposed that neural networks could simulate the workings of biological neurons, establishing a theoretical framework for computational neuroscience and paving the way for machine learning . This research became one of the cornerstones of AI, influencing the development of algorithms that allow machines to learn from data and recognize patterns — principles that continue to drive advancements in AI today. Artificial model [McCulloch and Pitts, 1943] of a biological neuron. As it can be observed, the relation between the input and output follows a nonlinear function called activation function. In the first model shown in this figure, the activation is a hard threshold function 1945 — ENIAC was Born ENIAC (/ˈɛniæk/; Electronic Numerical Integrator and Computer) was the world’s first programmable, electronic, general-purpose digital computer , completed in 1945 . ENIAC (Electronic Numerical Integrator And Computer) in Philadelphia, Pennsylvania. While other earlier machines had some of these capabilities, ENIAC was the first to combine them all, making it a groundbreaking development in computing history. It was Turing-complete , meaning it could be reprogrammed to solve a wide variety of numerical problems . John von Neumann used ENIAC to make the first computer-based weather forecast. In that particular experiment, he processed 250,000 floating-point operations in about 24 hours and produced a 24-hour forecast from the input data. John von Neumann The versatility and reprogrammable nature set ENIAC apart and established it as a major milestone in the evolution of modern computing. 1947 — The First Working Transistor: A Milestone in AI’s Technological Foundations The invention of the first working transistor in 1947 by the team at Bell Labs marked a pivotal breakthrough in electronics and computing, laying the groundwork for the development of modern artificial intelligence . Prior to this achievement, the fragile designs of early components like the cat’s whisker detectors struggled to reliably control electrical current. However, the Bell Labs team, led by John Bardeen, Walter Brattain, and William Shockley, eventually succeeded with the creation of the point-contact transistor . The Bell Labs team The transistor’s ability to amplify signals and control current flow revolutionized the field of electronics, replacing bulky vacuum tubes and enabling the creation of smaller, faster, and more efficient computers. Replica of the first transistor The transistor became an ideal component for the binary system — the foundation of modern computing and AI. Since the binary system operates on two states — on (1) and off (0) — transistors are perfectly suited for manipulating these states. Transistors can switch between these on and off positions rapidly and reliably, which is essential for processing binary data in computers. This capability enabled faster and more efficient computation, laying the groundwork for AI systems. The invention of the transistor is, therefore, a foundational step in the history of AI development, as it provided the necessary technology to build the powerful computers. 1950 — The Turing Test In 1950 , British mathematician Alan Turing introduced the concept of the Turing Test in his influential paper “ Computing Machinery and Intelligence.” Turing suggested that if a machine could engage in a conversation with a human and its responses were indistinguishable from those of a human, the machine could be considered intelligent. This idea offered one of the first concrete methods to assess artificial intelligence , becoming a critical benchmark in AI philosophy. Although debated and sometimes criticized, the Turing Test remains a key reference point in discussions about the nature and limits of machine intelligence. Alan Turing — All rights reserved 1956 — The Coining of “Artificial Intelligence” and Early Robotics The term “Artificial Intelligence” was first coined in 1956 during the historic Dartmouth Conference , organized by John McCarthy , Marvin Minsky , Nathaniel Rochester , and Claude Shannon . This conference is widely regarded as the official birth of AI as a distinct field of study. It brought together leading researchers who believed that machines could be made to simulate aspects of human intelligence, and the event laid the foundation for the future of AI research. The ideas and discussions that emerged from Dartmouth helped shape AI’s trajectory for decades, marking a pivotal moment in the evolution of computer science and cognitive science. If you want to know more about artifical intelligence definition, please read my article: 1958 — The Integrated Circuit: A Revolutionary Milestone in Technology An integrated circuit (IC) , also known as a microchip , is a small yet powerful electronic device made up of interconnected components such as transistors, resistors, and capacitors. These components are etched onto a small piece of semiconductor material , typically silicon. ICs are found in nearly all modern electronic devices, from smartphones to computers , and play a vital role in the functionality of today’s technology. The invention of the first microchip in 1958 by Jack Kilby , an engineer at Texas Instruments, was a groundbreaking moment in technological history. Kilby’s prototype, which integrated discrete electronic components into a single chip, marked the beginning of the miniaturization of electronic devices . https://www.flickr.com/photos/_fabrizio_/2497324185 Before ICs, electronic devices, particularly computers, were large and cumbersome, but with the invention of the microchip, these devices became more compact and efficient, enabling the rapid development of the modern electronics industry. Kilby’s contribution to technology earned him the Nobel Prize in Physics in 2000 , highlighting the significance of his innovation. 1961 — First Industrial Robot: Unimate The 1960s saw the advent of the first industrial robots. In 1961, Unimate , the first programmable robot, was deployed in a General Motors assembly line, revolutionizing manufacturing processes. It was invented by George Devol in the 1950s using his original patent filed in 1954 and granted in 1961. Representation of a polar robot. The figure is inspired by the first industrial robot, the Unimate, and it is based on the image from the following webpage https://www.yaskawa-global.com/product/robotics/about 1965 — The Birth of Moore’s Law In 1965 , Gordon Moore , co-founder of Intel, made a visionary observation, now known as Moore’s Law : the number of transistors on a microchip was doubling approximately every two years, resulting in exponential increases in computational power while simultaneously reducing costs. Though initially related to central processing units ( CPUs ), this observation became pivotal for the broader evolution of computing technology, including artificial intelligence. Moore’s Law Transistor Count 1970–2020 As computational power increased, more sophisticated AI algorithms could be developed and executed, driving advancements in machine learning, deep learning, and natural language processing. For decades, the growth in CPU capabilities supported the rise of AI, but modern AI breakthroughs have increasingly relied on Graphics Processing Units (GPUs) rather than traditional CPUs. GPUs are designed to handle massive parallel processing, making them ideal for training complex AI models, especially deep neural networks, which require handling enormous amounts of data and computations. The shift from CPU-based to GPU-based computation has been instrumental in the rapid evolution of AI. GPUs, with their ability to process many tasks simultaneously, have unlocked new levels of performance in training AI models, particularly in areas such as image recognition, language models, and autonomous systems. This evolution , driven by both Moore’s Law and the advent of GPUs, has laid the foundation for the AI capabilities we see today. 1966 — ELIZA: Pioneering Human-Machine Communication From 1964 to 1967 , MIT researcher Joseph Weizenbaum developed ELIZA , one of the earliest natural language processing programs, with the goal of exploring communication between humans and machines. ELIZA operated by simulating conversation through a clever pattern-matching and substitution method, creating the illusion of understanding without truly comprehending the meaning behind the words. While the program didn’t have any real capacity for understanding language, it was able to give users the impression that they were engaging in a meaningful exchange. Written in MAD-SLIP and utilizing language capabilities encoded in separate “scripts,” ELIZA’s most famous script was DOCTOR , which mimicked the behavior of a Rogerian psychotherapist. This script allowed ELIZA to reflect users’ statements back to them with non-directive, open-ended responses, much like a real therapist might. Although simple by today’s standards, ELIZA’s conversational style was revolutionary, marking one of the first significant attempts at creating a chatbot capable of simulating human interaction. ELIZA’s ability to engage users with seemingly intelligent responses laid the groundwork for the development of modern chatbots and set an early example for programs attempting to pass the Turing Test . Though rudimentary, ELIZA sparked a new era of human-computer interaction, influencing decades of research in artificial intelligence and natural language processing. Try ELIZA with a modern software reproductions of the original: Google Play: ELIZA 1966 — The First Chatbo t 1970 — Backpropagation: A Cornerstone of Neural Network Training In machine learning , backpropagation is a foundational algorithm used to train neural networks by efficiently calculating gradient estimates for network parameter updates. Leveraging the chain rule from calculus, backpropagation computes the gradient of the loss function with respect to each weight in the network, enabling iterative weight updates that minimize errors and enhance performance. The basics of backpropagation were first explored in the early 1960s within the context of control theory and the application of the chain rule . The modern version of backpropagation was created by Seppo Linnainmaa, a Finnish mathematician and computer scientist, in 1970. Seppo Linnainmaa wrote his thesis Algoritmin kumulatiivinen pyöristysvirhe yksittäisten pyöristysvirheiden Taylor-kehitelmänä [The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors] (PDF) (Thesis) . Its formal application to neural networks, however, gained prominence through the 1986 Nature paper, “Learning Representations by Back-propagating Errors,” co-authored by David Rumelhart , Geoffrey Hinton , and Ronald Williams . This landmark paper popularized the term and methodology of backpropagation, cementing it as the primary technique for training modern neural networks . Backpropagation plays an essential role in deep learning , allowing neural networks to learn complex patterns by adjusting their parameters to minimize prediction errors. While the algorithm itself calculates gradients, it provides the essential groundwork for various optimization techniques that further refine the network weights during training. Geoffrey Hinton — Vaughn Ridley/Collision via Sportsfile — Collision Conf Highlighting the impact of their work, Hinton and John Hopfield received the 2024 Nobel Prize in Physics for their discoveries that enabled the development of neural networks, paving the way for modern AI. Their work has driven the advancement of backpropagation as a vital tool in AI, fostering the rapid evolution and application of deep neural networks. Deep Neural Network Rumelhart; Hinton; Williams (1986). “Learning representations by back-propagating errors” (PDF). Nature. 323 1970s — The Rise of LISP and Prolog The 1970s saw the emergence of two pivotal programming languages in the field of artificial intelligence: LISP and Prolog . LISP , developed by John McCarthy , became the dominant language for AI research due to its exceptional ability to process symbolic data and its flexibility for recursive functions. It was integral to the development of early AI applications, including natural language processing and problem-solving algorithms. Lisp Logo — https://common-lisp.net/ Prolog , on the other hand, was created to excel in symbolic reasoning and logic-based AI tasks. It became particularly valuable for developing expert systems , where the ability to model complex logical relationships and deduce solutions was critical. Together, LISP and Prolog laid the computational foundation for AI’s growth in areas like machine learning , theorem proving , and automated reasoning , and their influence continues to shape AI development today. 1973 — WABOT-1: The First Humanoid Robot In 1973 , Japan introduced WABOT-1 , the world’s first full-scale humanoid robot, marking a major milestone in the application of AI to robotics . WABOT-1 could walk, grip objects with its hands, and even engage in basic conversations, making it one of the earliest examples of AI-powered robotic autonomy. https://www.ntticc.or.jp/en/archive/works/wabot-1/ Its creation showcased how AI could be integrated with physical machinery to create robots capable of interacting with the real world, laying the groundwork for future advancements in robotics, automation, and human-machine collaboration. The Rise and Falls: AI Winters First AI Winter (1974–1980) During the 1970s, artificial intelligence faced significant critiques and financial setbacks . AI researchers had initially underestimated the complexity of the problems they were tackling, leading to widespread optimism and inflated public expectations. When the results failed to meet those high expectations, funding for AI research was significantly reduced. Many of the techniques being employed at the time were not advanced enough to solve the problems researchers faced, leading to a period of disillusionment. First AI winter — All rights reserved Despite these challenges, the setbacks did not halt the progress of AI. While funding cuts impacted some major AI laboratories, the broader field continued to grow. Public interest in AI persisted , and the number of researchers increased substantially. During this time, new areas of research emerged, including logic programming and commonsense reasoning. Historian Thomas Haigh argued in 2023 that this period did not represent a true “winter” for AI, as research continued to flourish. AI pioneer Nils Nilsson even described the era as the most “exciting” time to work in the field, as new ideas were being developed and explored. 1980s — The Rise and Fall of Expert Systems The 1980s were a period of intense growth and experimentation in expert systems , marking the development of second-generation expert systems . Unlike their predecessors, these systems introduced probabilistic models, allowing them to reason about causes and effects more effectively. This new wave of expert systems found practical application in industrial and commercial fields , driving a surge of interest and investment in AI. One of the most successful implementations of this era was R1 (or Xcon) , developed at Carnegie Mellon University by John McDermott in 1978 and deployed by Digital Equipment Corporation in 1982. R1 revolutionized computer order configuration, ensuring orders were complete and spatially optimized. By 1986, the system saved the company $40 million annually, showcasing the economic potential of expert systems. The success of R1 and similar systems spurred significant investment in knowledge engineering . Countries like Japan, followed by the United States, United Kingdom, and other parts of Europe, heavily invested in AI-driven technologies. Expert systems became a focal point in the development of advanced computer systems, particularly in industries requiring specialized knowledge and decision-making. However, despite their early success, second-generation expert systems encountered several challenges. Writing and maintaining the complex rules that mirrored expert knowledge proved difficult, and the technology’s development lagged behind the growing hype. The arrival of more powerful general-purpose computers from companies like Apple and IBM began to outpace the specialized hardware built for AI. By 1987 , disappointment had set in, culminating in DARPA’s decision to cut funding for AI research, having invested $100 million just two years prior. This marked the beginning of another AI winter , as enthusiasm for expert systems faded and investment dried up. While expert systems had demonstrated the potential of AI, the limitations of the technology at the time led to a sharp decline in interest, setting back the field until its revival in the mid-1990s. 1984 The Macintosh , or Mac , introduced by Apple in 1984, played a pivotal role in the broader development of technology, including the evolution of artificial intelligence . Its revolutionary graphical user interface (GUI) transformed how people interacted with computers, making technology more accessible to non-experts. This shift laid the groundwork for user-friendly AI systems, where interaction with machines no longer required deep technical knowledge. 1984 — Apple Mac The Mac’s intuitive design encouraged the development of software and tools that emphasized ease of use, which would later influence the development of AI-driven interfaces and systems. As AI advanced, the importance of making it accessible to a wider audience became paramount. The Mac’s emphasis on visual interfaces and user-centric design directly influenced how AI applications, such as virtual assistants, are created today — systems that can engage users naturally and intuitively. Furthermore, the Mac’s architecture fostered innovation in computational tools and software, enabling developers to build increasingly complex programs that could incorporate early AI models. Its legacy of innovation in human-computer interaction has continuously shaped the development of AI technologies, which, like the Mac, aim to bridge the gap between complex machine processes and seamless, user-friendly experiences. The Macintosh commercial, famously aired during the 1984 Super Bowl , was more than just a product advertisement — it was a bold cultural statement. Directed by Ridley Scott and inspired by George Orwell’s novel 1984 , the ad depicted a dystopian world ruled by conformity and control, symbolized by an omnipresent “Big Brother.” In contrast, the Macintosh represented individuality, creativity, and liberation from technological oppression. The slogan, “Why 1984 won’t be like 1984 ,” framed the Mac as a tool of empowerment in a world threatened by the dangers of authoritarian technology. This imagery of utopian versus dystopian futures is closely tied to the narrative surrounding artificial intelligence . Like in Orwell’s novel, AI technologies evoke both utopian dreams and dystopian fears. On one hand, AI promises a future of enhanced creativity, problem-solving, and freedom, where machines can assist humanity in reaching its full potential. On the other hand, AI also stirs fears of a dystopian world where intelligent systems could control, surveil, and diminish human autonomy. The Macintosh ad tapped into these cultural anxieties and aspirations by positioning technology, and implicitly future developments like AI, as tools that could either liberate or subjugate society, depending on how they are designed and controlled. This duality continues to shape the discourse around AI, as we navigate the fine line between technology as an instrument of progress and as a potential threat to human freedom. Second AI Winter (Late 1987 — Early 1993) The 1980s witnessed a surge in commercial interest in artificial intelligence, fueled by high expectations and significant investments. Businesses, eager to capitalize on AI’s potential, anticipated rapid advancements, particularly in achieving human-level intelligence. However, as many AI companies failed to meet these lofty promises , disillusionment set in. The inability to deliver on bold claims led to a sharp decline in confidence within the business community, which began to view AI as a technology that was not yet viable. Second AI Winter — All rights reserved This widespread loss of faith resulted in a severe downturn for AI, often referred to as the Second AI Winter . Funding dried up, and many AI projects were abandoned or significantly scaled back. The damage to AI’s reputation was so profound that it continued to affect perceptions of the technology well into the 21st century. Despite AI’s potential, the gap between expectations and reality during this period led to one of the most significant setbacks in the field’s history. Windows 95: A Catalyst for AI and the Growth of Personal Computing The release of Windows 95 by Microsoft in 1995 marked a significant turning point in the evolution of personal computing. As the first widely popular operating system for personal computers, Windows 95 brought advanced functionality and an intuitive user interface to millions of users around the world. Its success laid the foundation for the mass adoption of personal computers, which in turn played a crucial role in the development of artificial intelligence . Windows 95 revolutionized computing by introducing features like the Start menu , taskbar , and support for multitasking , making computers far more accessible and functional for everyday users. This expansion of personal computing dramatically increased the number of people interacting with technology on a daily basis, creating a fertile ground for advancements in software development, including AI applications. The importance of Windows 95 to AI development cannot be overstated. The operating system made computing ubiquitous . By democratizing access to technology, Windows 95 accelerated the rate at which both developers and everyday users could interact with computers and software, ultimately driving the demand for more intelligent systems. In short, the launch of Windows 95 was not just a milestone for personal computing — it also indirectly fostered the conditions for AI research and development to thrive. By making computing accessible to the masses, Windows 95 laid the groundwork for the digital infrastructure that supports AI innovation today, proving that the growth of personal computing is intricately linked to the rise of artificial intelligence. 1990 — Linux: Pioneering Force for AI Computation In the early 1990s , Linux emerged as a revolutionary operating system, developed by Linus Torvalds in collaboration with the Free Software Foundation (FSF) . Although still in its infancy, Linux’s open-source nature and flexible architecture hold the potential to become a foundational platform for future artificial intelligence development. As AI continues to evolve, Linux’s adaptability will likely make it the ideal environment for AI research and machine learning, due to its unparalleled customizability and scalability. Illustrates the current (as of August 2016) Linux device drivers for AMD hardware Looking forward, Linux’s open-source philosophy is set to empower researchers and developers to innovate freely, tailoring their systems for AI-specific needs. This flexibility could prove crucial as AI models grow more complex and require highly specialized environments to optimize performance. By allowing users to modify its core functionalities, Linux will likely become indispensable for building and fine-tuning the AI systems of the future. Linux’s early support for GPU acceleration also hints at its future importance in deep learning . As AI models begin to demand more computational power, the ability to harness the parallel processing capabilities of GPUs will be crucial for training neural networks. The open nature of Linux will make it easier to integrate emerging technologies like CUDA (Compute Unified Device Architecture), paving the way for distributed computing frameworks that will accelerate AI development. Furthermore, Linux’s command-line interface and scripting capabilities will facilitate the automation of complex AI workflows. This is likely to be critical as researchers scale their experiments, managing vast datasets and running intricate algorithms across distributed systems. Its compatibility with large-scale AI clusters will enable Linux to handle the computing power required to push the boundaries of AI, whether on single machines or across powerful, networked infrastructures. As we look to the future, Linux is positioned to be a driving force behind AI’s progress. Its scalability , flexibility , and support for cutting-edge technologies will likely ensure its place as the preferred operating system for the next wave of AI innovation, shaping the digital landscape for decades to come. 1991 — Schmidhuber’s Key Contributions to AI In 1991 , Jürgen Schmidhuber made groundbreaking advancements in deep learning by addressing a major issue — how to train neural networks effectively over long sequences. To solve this, he proposed a hierarchy of recurrent neural networks (RNNs) , allowing the networks to learn at different time scales, making the training process more efficient. This approach enabled RNNs to handle deep learning tasks with much greater complexity. That same year, Schmidhuber introduced the idea of adversarial neural networks , where two networks compete in a zero-sum game . One network generated patterns, while the other learned to predict outcomes, a concept later used in the development of Generative Adversarial Networks (GANs) . Additionally, Schmidhuber supervised Sepp Hochreiter , whose work on solving the vanishing gradient problem led to the creation of Long Short-Term Memory (LSTM) networks. These LSTMs have since become a vital component in many modern AI systems 1996 — The Importance of Cloud Computing in AI Development The evolution of cloud computing , with its roots in the 1960s and the concept of time-sharing , has played a pivotal role in enabling the rapid development of artificial intelligence . Initially, large-scale computing was restricted to mainframes, where users submitted jobs to centralized data centers. Over time, the rise of virtualized services and the introduction of the “cloud” metaphor in the 1990s transformed the landscape, making massive computational power accessible to more users than ever before. The expression cloud computing became more widely known in 1996 when Compaq Computer Corporation drew up a business plan for future computing and the Internet. The company’s ambition was to supercharge sales with “cloud computing-enabled applications”. Diagram showing overview of cloud computing For AI, cloud computing has been instrumental in overcoming the limitations of local hardware. AI models, especially those driven by deep learning and neural networks, require vast amounts of computational power and data storage. Cloud computing provides the infrastructure necessary to train large-scale models by allowing access to distributed, high-performance computing resources without requiring individual organizations to own expensive hardware. This scalability is crucial for AI research, where the ability to process enormous datasets in parallel is a key factor in advancing machine learning and natural language processing . Moreover, cloud computing enables collaboration across institutions and industries, facilitating the sharing of datasets and AI tools globally. It allows AI developers and researchers to leverage cloud-based platforms to experiment with AI models , deploy applications, and scale AI-driven services without the constraints of local computing resources. By democratizing access to high-performance computing , cloud technology has made AI development more accessible, fostering innovation and accelerating advancements in the field. In essence, the integration of cloud computing into the fabric of AI development is not just a technical improvement; it represents a critical infrastructure that powers the next generation of AI applications . Without the cloud’s ability to handle intensive computational workloads, many of the breakthroughs in AI, from autonomous systems to personalized algorithms , would not be possible. Modern Milestones: From Deep Blue to AlphaGo 1977 — AI in the Public Eye: Deep Blue vs. Garry Kasparov The late 20th and early 21st centuries saw artificial intelligence enter the public consciousness through a series of high-profile milestones, the most notable being IBM’s Deep Blue defeating world chess champion Garry Kasparov in 1997 . This event demonstrated the immense power of AI-driven computation and strategic programming, marking a turning point in the perception of AI’s capabilities. Garry Kasparov and Deep Blue Deep Blue vs. Garry Kasparov was a pair of six-game chess matches between Kasparov, the reigning world chess champion, and Deep Blue , an IBM supercomputer designed specifically for chess. The first match, held in Philadelphia in 1996 , resulted in a victory for Kasparov, who won 4–2. However, in a highly anticipated rematch in New York City in 1997 , Deep Blue emerged victorious, becoming the first computer to defeat a world chess champion under standard tournament conditions. This victory was more than just a technological achievement — it was a profound symbolic moment. Deep Blue’s triumph over Kasparov, one of the greatest intellectual champions of the human mind, represented the increasing sophistication of AI systems. It showcased the potential of brute-force computation , where the computer could calculate and evaluate millions of moves per second, alongside carefully designed algorithms that mimicked strategic decision-making. Animation of Deep Blue versus Kasparov The 1997 rematch, chronicled in the documentary “Game Over: Kasparov and the Machine,” sparked a global conversation about the relationship between human and machine intelligence. Deep Blue’s win was hailed as a watershed moment, a sign that AI had reached a new level of capability. While the system’s approach was not true “intelligence” as humans understand it — relying on raw computational power rather than intuitive thought — its success underscored the growing potential of AI to compete with and surpass humans in specific intellectual tasks. Garry Kasparov versus Deep Thought Documentary 1999 — Sony’s AIBO: The Intelligent Robotic Dog In 1999 , Sony unveiled AIBO, a groundbreaking robotic dog designed to be an “intelligent and trainable robot companion.” AIBO was capable of performing a wide range of lifelike behaviors, such as walking, barking, whining, growling, wagging its tail, and playing with a ball — all without the need for traditional pet care. What made AIBO truly unique was its ability to adapt its behavior based on interactions with its human owners, allowing it to develop a distinct “personality” over time. AIBO 1999 — Autore: Tom Mesic AIBO was equipped with a variety of sensors, including a CCD color camera , two microphones, infrared sensors, touch sensors on its head, chin, back, and legs, as well as an accelerometer and temperature sensor. These allowed AIBO to navigate its environment, recognize objects and people, and respond to voice commands. Aiboware , the software loaded onto memory sticks, powered AIBO’s movements and enabled it to evolve from a “puppy” stage into an “adult” based on its interactions with its owner. Once fully developed, AIBO could recognize up to 100 voice commands, though, like a real dog, it didn’t always obey. AIBO’s OPEN-R software development kit also allowed owners to program the robot, further personalizing its behavior. Sony’s attention to detail made AIBO not only technically advanced but also expressive, thanks to 18 joints in its legs, neck, tail, and jaw that allowed for fluid, natural movements. AIBO was a significant step in AI robotics, showcasing the potential for intelligent companions that could interact dynamically with humans. Its ability to learn and evolve made it more than just a novelty — it was a glimpse into the future of adaptive AI systems capable of responding to their environment and forming deeper connections with users. Modern model: 2002 — iRobot Launches Roomba: AI Enters the Household In 2002 , iRobot revolutionized household chores with the launch of the Roomba , an autonomous robotic vacuum cleaner that brought AI into everyday life. Unlike traditional vacuum cleaners, Roomba used AI-driven sensors and algorithms to navigate rooms, avoid obstacles, detect dirtier areas, and efficiently clean floors without human intervention. Its ability to map environments , adjust to different floor types , and work autonomously made it a practical and appealing tool for consumers. Roomba’s commercial success highlighted the practical applications of AI in solving mundane tasks, proving that robotics and AI could seamlessly integrate into domestic life. The Roomba was not just a gadget; it was one of the first widely adopted AI-powered consumer robots, bringing robotics into millions of homes. This success story also paved the way for further innovations in smart home technologies and robotics, illustrating the potential of AI to enhance everyday living. First-generation Roomba 2006 — The Rise of Deep Learning: A New Era in AI In 2006 , Geoffrey Hinton , a leading figure in AI research, helped bring deep learning into the spotlight, marking a pivotal moment in the history of artificial intelligence. By harnessing the power of neural networks , Hinton and his team demonstrated how machines could learn and make sense of vast amounts of data, leading to groundbreaking advances in fields such as computer vision , speech recognition , and natural language processing . Deep learning, a subfield of machine learning, enabled AI systems to automatically learn features and patterns from raw data without needing manual intervention or human-engineered features. This approach mimicked the way the human brain processes information, allowing machines to perform tasks such as image recognition, speech translation, and even playing games at superhuman levels. Deep learning leverages artificial neural networks (ANNs) , that is a model inspired by the structure and function of biological neural networks found in animal brains. An ANN is composed of artificial neurons — interconnected units or nodes — that loosely mimic the way biological neurons process and transmit signals. Each neuron receives input from connected neurons through synapse-like edges, processes the input, and then passes on a signal to other neurons. Neural Network — https://www.needpix.com/photo/1752528/ The “signal” is a real number, and the output is computed using a non-linear activation function based on the sum of the inputs. The strength of each connection is governed by a weight, which is adjusted during the learning process, allowing the network to fine-tune its performance through training . This enables deep learning models to recognize patterns, classify information, and make predictions with impressive accuracy. Hinton’s work with deep learning harnessed these neural networks to solve previously intractable problems. By allowing AI to learn complex representations of data on its own, deep learning opened new possibilities for AI applications in industries ranging from autonomous vehicles to healthcare and finance . The ability of neural networks to automatically learn and improve through training represented a monumental leap forward, ushering in the era of modern AI that continues to transform technology today. Apple in 2007 Launched the iPhone The first smartphone on the planet was not the iPhone, but IBM’s Simon Personal Communicator, introduced in 1992. It was the first mobile device to combine phone capabilities with computer-like features such as a touchscreen, calendar, email, and fax. However, the iPhone , launched by Apple in 2007 under Steve Jobs , revolutionized the smartphone industry with its intuitive multi-touch technology, sleek design, and powerful iOS operating system. Apple iPhone 1 The iPhone set a new standard for mobile devices, introducing features like app stores, larger screens, and video recording, and has since sold over 2.2 billion units by 2018, making it one of the most influential tech devices in history. Smartphones have not only transformed how we interact with technology but have also accelerated the practical implementation and integration of AI, shaping a future where AI is embedded in almost every facet of daily life. 2009 — The Role of GPUs in AI Development and Their Connection to Bitcoin and Cryptocurrency The Graphics Processing Unit (GPU) , originally designed for accelerating graphics manipulation and rendering in video games and other graphical applications, has become a fundamental component in the development of artificial intelligence . GPUs, as first popularized by Nvidia in 1999 with the GeForce 256 , evolved beyond their initial purpose to handle more general computation, ushering in the era of General Purpose GPUs (GPGPUs) in 2007. GPUs also became a critical technology in the world of cryptocurrency mining , further linking them to advancements in AI . The release of Bitcoin in 2009 initially relied on CPUs for mining, but as Bitcoin mining grew more competitive, GPU mining emerged in 2010 as a more efficient solution due to the parallel processing capabilities of GPUs. This shift sparked a massive demand for GPUs, particularly from cryptocurrency miners, which significantly impacted GPU availability and prices between 2013 and 2017 . Bitcoin, one of the most popular cryptocurrency The high demand for GPUs in cryptocurrency mining underscored their value not only for rendering graphics but also for performing complex computations , which are essential for both cryptographic hashing in blockchain networks. The adoption of GPUs for Bitcoin mining further spurred advancements in GPU technology, as manufacturers like Nvidia and AMD continued to push the boundaries of GPU performance to meet the growing needs of miners and AI researchers alike. As a result, the development of more powerful GPUs has directly fueled the acceleration of AI research and cryptocurrency mining , linking these two seemingly distinct fields through their shared reliance on parallel processing power. Their ability to handle massive parallel processing tasks, executing thousands of calculations simultaneously, made them ideally suited for the demanding workloads of machine learning and neural network training. GPU (Graphics Processing Unit) For AI development, this evolution was critical. Unlike CPUs , which are optimized for sequential processing, GPUs excel at performing repetitive, parallel tasks, such as those found in deep learning algorithms. Neural networks, which rely on processing vast amounts of data across multiple layers, benefit from the immense parallelism that GPUs offer, leading to significant speed improvements in both training and inference. This advancement allowed researchers to tackle more complex AI problems and opened the door to innovations in fields such as computer vision , natural language processing , and autonomous systems . 2010 — The CSIRO GPU cluster In summary, the GPU’s evolution from a specialized tool for rendering graphics to a key driver of AI development and cryptocurrency mining highlights its central role in modern computing. Without GPUs, the rapid advances in AI, particularly in machine learning and deep learning , would not have been possible, nor would the boom in cryptocurrency mining that has reshaped digital economies worldwide. 2011 — IBM Watson Wins Jeopardy! In February 2011 , IBM’s Watson made history by defeating the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings , in a highly-publicized televised event. This victory showcased the immense potential of AI in natural language processing and question-answering systems. Watson stage replica in Jeopardy! contest, Mountain View, California Developed by a team led by David Ferrucci , Watson was designed to push the boundaries of how machines could understand and interact with human language. Unlike traditional search engines, which rely on keyword parsing, Watson used IBM’s DeepQA software to generate hypotheses, analyze data, and provide answers to questions posed in natural language . Watson didn’t simply search the internet for answers — it analyzed vast amounts of stored information, including encyclopedias, dictionaries, religious texts, and books, to form its responses. At its core, Watson ran on 10 racks of 90 servers with 2,880 processor cores , performing millions of calculations in just three seconds to determine the best answer with a high degree of confidence. Its ability to understand and process unstructured data represented a significant leap in AI, particularly in natural language processing (NLP) . Watson’s Jeopardy! victory was more than just a game — it was a milestone in AI’s development, demonstrating that machines could handle complex language tasks and respond accurately to real-world questions. This success paved the way for AI’s application in industries ranging from healthcare to customer service , where Watson’s technology continues to enhance decision-making and problem-solving. 2011 — Siri was born: AI Enters the Mainstream In October 2011 , Apple introduced Siri , an intelligent virtual assistant integrated into the iPhone 4S , marking the first widely available AI-powered assistant on a major smartphone. Siri used speech recognition and artificial intelligence to perform a range of tasks, such as answering questions, performing calculations, playing music, sending messages, setting reminders, and providing navigation. Siri’s introduction brought AI assistants into mainstream consumer technology, setting the stage for future developments in digital assistants across mobile devices. Siri’s intuitive voice interface revolutionized how users interact with their smartphones, making AI-powered assistants a core feature of daily life. iPhone 4S unboxing 17–10–11 | Brett Jordan | Flickr 2012 — AlexNet and the Image Recognition Breakthrough In 2012 , a major milestone in image recognition was achieved with the success of AlexNet, a deep convolutional neural network (CNN) developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. AlexNet won the prestigious ImageNet competition , a benchmark for visual object recognition, by a significant margin. Its success highlighted the transformative potential of deep learning for image classification tasks, which involve recognizing and categorizing objects in images with remarkable accuracy. AlexNet’s architecture demonstrated the power of convolutional neural networks , which can automatically detect patterns and features in large image datasets. By significantly reducing error rates compared to previous methods, AlexNet marked a turning point in machine learning’s capability , proving that deep learning could outperform traditional techniques in complex tasks. This breakthrough laid the foundation for numerous applications in computer vision , ranging from facial recognition to autonomous driving, and propelled deep learning into the forefront of artificial intelligence research. 2014 — Facebook’s DeepFace and the Advancement of AI in Facial Recognition In 2014 , Facebook unveiled DeepFace , an advanced AI system designed to recognize human faces with over 97% accuracy . This groundbreaking achievement underscored the rapid advancements in facial recognition technology and highlighted AI’s expanding role in areas such as security , social networking , and biometric identification . DeepFace utilized deep learning techniques, specifically convolutional neural networks , to map and analyze facial structures in unprecedented detail. This capability not only enabled more accurate recognition of individuals across different settings and lighting conditions but also sparked broader discussions about privacy and the ethical implications of AI in everyday applications. DeepFace’s success demonstrated the immense potential of AI in transforming how we interact with technology, making it a key milestone in the integration of AI-driven biometric systems into digital platforms. 2014 — The Introduction of Generative Adversarial Networks (GANs) In 2014 , Ian Goodfellow introduced Generative Adversarial Networks (GANs) , a revolutionary machine learning framework that transformed the field of AI. GANs consist of two neural networks — a generator and a discriminator — that compete against each other. The generator creates new data or content, while the discriminator evaluates it, determining whether the generated content is real or fake. This process continues until the generator produces data that is virtually indistinguishable from real-world data. Generative Adversarial Networks (GANs) GANs marked a significant breakthrough in generative AI , allowing for the creation of high-quality images, videos, and other content . This technology has been applied across various fields, from art and entertainment to synthetic data generation and medical imaging. The introduction of GANs demonstrated the creative potential of AI, enabling machines not just to analyze or classify data, but to generate new and realistic content , pushing the boundaries of what AI could achieve. 2014 — The First Chatbot to Pass the Turing Test: Eugene Goostman The Turing Test , proposed by Alan Turing in 1950 , is a test designed to determine whether a machine can exhibit intelligent behavior indistinguishable from that of a human. In this test, a human evaluator engages in a conversation with both a human and a machine, and if the evaluator cannot reliably distinguish between the two, the machine is considered to have passed the test. Eugene Goostman Chatbot In 2014 , a chatbot named Eugene Goostman became the first artificial intelligence to appear to pass the Turing Test during an event held at the University of Reading . Created by Vladimir Veselov , Eugene Demchenko , and Sergey Ulasen , Eugene Goostman was portrayed as a thirteen-year-old boy from Odesa, Ukraine , with an intentionally chosen backstory that explained minor grammatical errors in its responses. Goostman interacted with judges via text-based conversations, and in the event, 33% of the judges were convinced that they were speaking with a human, leading the event’s organizer, Kevin Warwick , to declare that the chatbot had successfully passed the Turing Test. Eugene Goostman’s success in this context represents a milestone in the development of natural language processing and conversational AI. While the achievement sparked debate about whether it truly met the rigorous standards of the Turing Test, it demonstrated the potential of chatbots to emulate human-like conversation, advancing the field of artificial intelligence and its applications in human-machine interactions. 2015 — Amazon Echo and Alexa: The Rise of Voice-Controlled AI In 2015 , Amazon launched the Echo , a smart speaker powered by Alexa, a voice-controlled AI assistant. Alexa quickly became a household name due to its ability to perform a wide range of tasks, from answering questions and playing music to controlling smart home devices and facilitating online shopping. By using natural language processing and speech recognition, Alexa allowed users to interact with technology in a more intuitive, hands-free manner. The introduction of Amazon Echo and Alexa marked a major advancement in voice-controlled AI , demonstrating the practical application of AI in everyday life. Alexa’s integration into homes around the world underscored the growing potential of AI-driven personal assistants and set the stage for the development of smart homes and connected ecosystems , where AI seamlessly interacts with a variety of devices to enhance convenience and productivity. 2016 - AlphaGo’s Triumph Over Lee Sedol: A Landmark Moment for AI Go, a complex and ancient board game, had long been regarded as one of the greatest challenges for AI due to its immense complexity and the requirement for intuition, creativity, and strategic thinking. Unlike chess, where brute-force computation could be leveraged, Go’s vast number of possible moves made it exponentially harder for AI systems to solve. Many believed that mastering Go would require elements of AI that more closely mimic human thought processes. In 2016 , DeepMind’s AlphaGo made history by defeating Lee Sedol , one of the world’s top Go players, marking a monumental achievement in artificial intelligence. Google DeepMind’s AlphaGo project AlphaGo’s victory was a testament to the power of deep learning and reinforcement learning , which represented a significant departure from previous AI approaches. Instead of relying on pre-programmed probability algorithms, AlphaGo used neural networks to evaluate the state of the game and predict its probability of winning. It was trained on a massive library of historical Go matches and literature, but what made AlphaGo truly remarkable was its ability to play games against itself, continuously learning and refining its strategies with each iteration. By doing so, AlphaGo became independent of its human developers, capable of learning and improving beyond the limits of its initial programming. AlphaGo’s success was powered by a combination of neural networks and Monte Carlo tree search , allowing it to calculate vast numbers of possible moves — both likely and unlikely — far into the future. This ability to anticipate numerous potential scenarios, along with its capacity for self-improvement, gave AlphaGo an edge that no AI had ever demonstrated in such a complex and strategically rich game. The match, which carried a $1 million prize, was not only a competition but also a symbolic moment for AI’s progress. Following AlphaGo’s victory, Google DeepMind announced that the prize money would be donated to charity, further underscoring the significance of the event beyond the gaming world. Fan Hui vs AlphaGo — Game 5 AlphaGo’s triumph over Lee Sedol highlighted the future potential of AI, showing that with the right combination of deep learning and self-learning mechanisms, machines could tackle problems that were previously thought to be beyond their capabilities. This victory ushered in a new era for AI research , opening the door to solving real-world problems in fields such as healthcare, logistics, and scientific discovery by leveraging similar technologies. 2016 — Tay (Chatbot): A Cautionary Tale in AI Development Tay , a chatbot developed by Microsoft , was launched on March 23, 2016 , as a Twitter bot designed to interact with users and learn from its conversations. However, Tay quickly became controversial when it started posting inflammatory, offensive, and inappropriate tweets. These responses were a direct result of the bot learning from its interactions with users, many of whom deliberately manipulated Tay by feeding it harmful language. Within just 16 hours of its launch, Microsoft was forced to shut down the service. Tay bot logo The Tay incident underscored the vulnerabilities of AI systems that rely on machine learning from unfiltered user interactions. While Tay was designed to adapt and respond based on the conversations it engaged in, it became a victim of malicious trolling , highlighting how AI can be manipulated when safeguards are not in place. This failure illustrated the complexities and challenges of natural language processing and machine learning in open, unregulated environments like social media. In response, Microsoft later replaced Tay with a new chatbot, Zo , which incorporated stricter controls to avoid the pitfalls of its predecessor. The Tay controversy served as a critical lesson in AI ethics , showing how quickly an AI system can spiral out of control in the absence of content moderation and responsible design, further emphasizing the need for more robust, ethical frameworks in AI development. 2017 — “Attention Is All You Need” and the Transformer Breakthrough In 2017 , one of the most significant advancements in AI occurred with the publication of the paper “Attention Is All You Need” by Vaswani et al. , introducing the Transformer model . This model revolutionized natural language processing (NLP) by departing from previous architectures that relied on sequential word processing. Instead, the Transformer leveraged an attention mechanism that allowed it to process words in parallel , vastly improving the efficiency of handling large texts and complex language tasks. Transformer Architecture The title of the paper, “Attention Is All You Need,” is a playful nod to the Beatles’ famous song “All You Need Is Love.” The term “Transformer” was chosen for the model simply because the author Uszkoreit liked how the word sounded. An early design document was humorously titled “ Transformers: Iterative Self-Attention and Processing for Various Tasks ,” featuring an illustration of characters from the Transformers animated series, and the team behind it was referred to as Team Transformer. In their initial experiments, the team applied the Transformer architecture to tasks like English-to-German translation, generating Wikipedia articles about the Transformer itself, and parsing text. These trials demonstrated that the Transformer was not only highly effective for translation but also a general-purpose language model , capable of excelling in a wide range of natural language processing tasks. This versatility was a key factor in the model’s broader adoption and its revolutionary impact on AI. The Transformer model became a cornerstone of modern AI, enabling the development of large-scale models such as GPT-3 and BERT . Its architecture excelled in tasks like machine translation , text generation , and question answering , setting new benchmarks for performance in NLP. The introduction of Transformers marked a new era in AI, empowering models that could understand and generate human language with remarkable accuracy and fluency, making it foundational for many state-of-the-art AI applications today. Original article: Attention Is All You Need 2018 — AI in Universities: The Rise of Specialized Education By 2018 , universities worldwide began integrating specialized AI courses into their curricula, reflecting the increasing significance and demand for expertise in this rapidly evolving field. The surge in AI’s practical applications, from autonomous vehicles to healthcare innovations, prompted academic institutions to offer dedicated programs in machine learning , deep learning , natural language processing , and more. This shift marked a growing recognition of AI as a crucial area of study, preparing a new generation of students to contribute to and shape the future of artificial intelligence across industries. The rise of AI-focused education demonstrated the technology’s transformative potential and the need for skilled professionals to meet the demands of the AI-driven economy. 2020 — OpenAI’s GPT-3: A Landmark in Language AI In 2020 , OpenAI released GPT-3 (Generative Pre-trained Transformer 3) , one of the largest and most powerful language models ever created. With 175 billion parameters , GPT-3 showcased an unprecedented ability to generate human-like text and perform a wide range of tasks such as translation, summarization , content creation, and answering complex questions . What set GPT-3 apart was its capacity to understand and generate coherent, contextually appropriate responses with minimal input, making it a game-changer in natural language processing (NLP) . It demonstrated the potential of AI to handle diverse language-related tasks without needing task-specific training. GPT Architecture This flexibility and versatility revolutionized industries from customer service and content creation to education and research, setting a new standard for what AI could achieve in text-based applications. GPT-3’s capabilities marked a significant leap forward in the field of AI, unlocking new possibilities for machine-human interaction. 2022 — Midjourney and Generative AI in Art Midjourney represents a pivotal point in the intersection of art and generative AI , offering a platform for creating images from text-based prompts. As an independent research lab, Midjourney has developed an AI-powered image generation software that allows users to input prompts — short textual descriptions — and receive stunning, imaginative visuals in response. This innovative tool enables users to generate detailed and highly creative artwork by simply describing their vision in words. Oriental Lovers- One of the images that I generated with Midjourney — All rights reserved Midjourney focuses on using AI to enhance human creativity , expanding the boundaries of what can be achieved in the artistic process. By transforming text prompts into visually rich images, it empowers both artists and individuals to collaborate with AI, exploring new ways to visualize abstract ideas and thoughts. Whether it’s for conceptual art, design, or creative experimentation, Midjourney’s approach pushes the limits of what AI can accomplish in the world of visual expression, making art creation more accessible and inspiring. Humans - One of the images that I generated with Midjourney — All rights reserved Similarly, generative AI technologies like Stable Diffusion and DALL·E have transformed how art is created. Stable Diffusion is a cutting-edge text-to-image diffusion model capable of generating photorealistic images from simple text inputs. This AI allows users to create stunning, highly detailed images in seconds, democratizing the creation process by enabling anyone to bring their artistic visions to life. Stable Diffusion fosters artistic freedom and creativity, making art creation more accessible to millions worldwide. Stable diffusion — two red guards smoking in the street — Wikipedia DALL·E , developed by OpenAI , is another breakthrough in generative AI , allowing users to generate highly realistic images and art based on natural language descriptions. Whether it’s an “astronaut riding a horse” or a more abstract concept, DALL·E showcases the transformative potential of AI in artistic creativity . By interpreting text prompts and converting them into intricate visual outputs, DALL·E has opened new pathways for how art is conceptualized and produced. “A photo of an astronaut riding a horse” — One of the first and the most famous image generated by Dall-E These generative AI models, including Midjourney , Stable Diffusion , and DALL·E , mark a new era in artistic innovation, where the line between human and machine creativity blurs, enabling endless possibilities in art and design . This AI-driven creativity revolutionizes traditional artistic processes, allowing users to collaborate with machines to imagine and generate art in ways that were once impossible. GPT-4: The Next Leap in AI Language Models Released in 2023 by OpenAI , GPT-4 represents a significant advancement over its predecessor, GPT-3. With an even larger and more refined architecture, GPT-4 boasts enhanced capabilities in natural language processing , reasoning , and problem-solving . It can handle more nuanced and complex tasks, including multi-step reasoning , generating highly coherent and contextually relevant text, and demonstrating improved performance in areas like code generation , creative writing , and dialogue systems . GPT-4 also places a stronger emphasis on safety and alignment, with built-in mechanisms to reduce harmful outputs and provide more accurate, responsible answers. Its enhanced multimodal capabilities allow it to process and generate both text and images , opening new possibilities for AI applications in fields such as education , research , and content creation . GPT-4 continues to push the boundaries of what large language models (LLMs) can achieve, setting new benchmarks for AI performance and utility. Turing Test The chatbot quickly gained attention for its ability to generate remarkably human-like responses, sparking discussions about its potential to pass the Turing Test . In a Nature article, Celeste Biever remarked that “ChatGPT broke the Turing test,” highlighting its conversational sophistication. Furthermore, Stanford researchers conducted studies and reported that ChatGPT-4 successfully passed a rigorous Turing test. They noted that the chatbot’s behavior closely mimicked that of a human, with its primary divergence being an increased tendency to be more cooperative than the average human. These findings underscored ChatGPT’s remarkable ability to engage in natural language conversations, marking a significant leap in the development of conversational AI. Other Notable Large Language Models (LLMs) In addition to GPT-3 , several other large language models (LLMs) have emerged, each contributing uniquely to the landscape of AI-driven natural language processing (NLP) . These models, developed by a variety of organizations, reflect the increasing diversity and innovation in the field, offering a range of capabilities and uses. Gemini (Google DeepMind) Developed by Google DeepMind , Gemini is an advanced LLM designed to rival models like GPT-3 and GPT-4. Gemini combines traditional natural language understanding with reinforcement learning capabilities, enabling the model to perform complex reasoning tasks and adapt to real-world environments. It is specifically designed to interact with AI agents and applications, making it highly versatile for tasks such as coding, question answering, and language translation. Mistral Mistral is a powerful language model known for its efficiency and performance, even with fewer parameters compared to other models. This model has been designed to handle large-scale text generation and comprehension tasks. Mistral’s architecture focuses on making LLMs more resource-efficient without compromising on the quality of their outputs. It has become a popular choice for enterprises and researchers looking for a high-performing model that can run on more accessible hardware. See also my article: Claude (Anthropic) Developed by Anthropic , Claude is another competitor in the LLM space, emphasizing safety and ethical AI usage. Named after Claude Shannon, the father of information theory, this model is designed with a focus on AI alignment — ensuring that the model’s behavior aligns with human intentions and values. Claude is capable of handling complex tasks like summarization, question answering, and dialogue, and is known for its emphasis on user safety and interpretability. LLaMA (Meta) LLaMA (Large Language Model Meta AI) is an open-source LLM developed by Meta (formerly Facebook). It was designed to democratize access to high-performing AI models by offering a more accessible and customizable alternative to proprietary LLMs like GPT-3. LLaMA’s open-source nature has made it popular among researchers and developers looking to experiment with and build upon existing LLM frameworks. By providing a transparent, scalable model, LLaMA has contributed to the advancement of AI research, encouraging collaborative development in the broader AI community. Grok: Frontier Language Model on 𝕏 Grok-2 is a cutting-edge language model that pushes the boundaries of reasoning capabilities in AI. Developed as part of the Grok family , it is designed to excel in advanced natural language processing tasks with state-of-the-art accuracy and problem-solving skills. The model comes in two versions: Grok-2 and Grok-2 mini , both of which are now available to users on the 𝕏 platform . Grok’s release highlights a new chapter in AI, offering users powerful tools for tasks that demand complex reasoning, text generation, and intelligent interactions, further expanding the reach and application of large language models (LLMs) in everyday digital platforms. 2024 — OpenAI O1 Preview OpenAI’s new O1-preview models are a step forward in creating AI that takes more time to elaborate before responding, significantly enhancing its reasoning abilities. These models are particularly adept in fields like science, coding, and mathematics, where complex problem-solving is crucial. Looking Ahead: The Future of AI Agentic AI and Chain of Thought (CoT) Prompting As we look toward the future of artificial intelligence , two pivotal concepts stand out: Agentic AI and Chain of Thought (CoT) prompting . These advancements represent the next frontier in AI, where machines are not just reactive tools but intelligent agents capable of autonomous reasoning and decision-making. Agentic AI — All rights reserved Agentic AI refers to AI systems that operate with a higher degree of autonomy, functioning as intelligent agents capable of understanding context, making decisions, and executing tasks independently. Unlike traditional AI, which is typically task-specific and reactive, Agentic AI has the potential to act with purpose and self-directed reasoning, analyzing complex environments and acting in ways that mimic human-like decision-making. This kind of AI will fundamentally change how we interact with machines, enabling AI to handle tasks that require dynamic thinking and adaptability in real-world situations. At the core of Agentic AI’s evolution is Chain of Thought (CoT) prompting . This approach allows AI to engage in step-by-step reasoning, mimicking how humans solve complex problems by breaking them down into smaller, more manageable parts. CoT prompting enables AI to explain its thought process and walk through multi-step decisions, leading to more accurate and reliable outcomes. As AI becomes more sophisticated, CoT will be crucial in guiding machines to handle multi-faceted challenges in areas such as scientific research , healthcare , and autonomous systems . Together, Agentic AI and CoT prompting will pave the way for a future where AI is not just a tool but a partner in innovation , capable of understanding and solving complex, open-ended problems with greater autonomy and intelligence. These advancements will shape industries, transform economies, and redefine the boundaries of human-machine collaboration in the years to come. The Future of AI: Realistic Audio and Video Avatars with Tone and Emotion As AI continues to evolve, one of the most exciting developments is the creation of highly realistic audio and video avatars capable of expressing nuanced tone and emotions . These avatars, powered by advanced machine learning models, are becoming more lifelike than ever, bringing human-like interaction to digital spaces in unprecedented ways. In the near future, AI-generated avatars will be able to mimic human facial expressions, body language, and voice modulation with extraordinary accuracy. They will respond to conversations with appropriate emotional tones, whether it’s excitement, empathy, or subtle humor. This evolution will not only enhance virtual interactions but also improve user experiences in customer service, telehealth, education, and entertainment. AI Avatar - All rights reserved These avatars will use deep learning techniques to process real-time inputs , allowing them to adapt their emotional responses to the context of a conversation or situation. With realistic voice synthesis and facial animations , AI will bridge the gap between human and digital communication, making interactions more immersive, personalized, and emotionally engaging than ever before. As these technologies advance, the potential for realistic and hyper-realistic AI-driven avatars will redefine digital spaces, creating a future where virtual assistants, educators, and even entertainment characters can engage with people in a way that feels genuinely human. AI in Video and Cinema: Generating Entire Commercials, Movies, and TV Shows The future of video production and cinema is rapidly being transformed by advancements in AI , where the technology is becoming capable of generating entire commercials , movies , TV shows , and other forms of media. This shift is set to revolutionize the entertainment industry, allowing for unprecedented creativity, efficiency, and customization in content creation. AI will be capable of writing scripts , creating storylines , directing scenes , and even producing visual effects with minimal human intervention. By leveraging generative AI models, filmmakers will design characters, settings, and plotlines that evolve dynamically based on input prompts. These AI systems will autonomously create realistic animations, camera angles, and dialogue, reducing the need for large crews and extensive resources traditionally required for production. Additionally, AI-generated actors and digital avatars will become commonplace, allowing filmmakers to recreate historical figures, invent entirely new characters, or bring fictional beings to life in a realistic manner. These AI-driven characters will be able to perform with human-like emotional depth, tailored precisely to the director’s vision, offering a new dimension of creative freedom. For advertising and commercials , AI can generate complete video spots based on brand guidelines, producing tailored content faster and at a fraction of the cost. AI-generated commercials can be personalized for different audiences, offering unique, real-time adaptations based on viewer preferences and data, making marketing more targeted and engaging. This AI-driven cinema could democratize filmmaking, empowering creators of all levels to produce high-quality content with fewer resources. The ability of AI to autonomously generate entire films or TV shows — down to the smallest details — has the potential to disrupt traditional content production, creating a future where AI is both a tool and a collaborator in the world of storytelling and entertainment. The Last Advancement: Artificial General Intelligence (AGI) and Beyond Artificial General Intelligence (AGI) , often called strong AI , represents one of the most ambitious goals in the field of artificial intelligence: the creation of a system capable of understanding, learning, and applying knowledge across a wide range of tasks at a level comparable to human intelligence. Unlike current narrow AI , which is designed for specific applications, AGI would possess the ability to reason , adapt , and perform any cognitive task that a human can, across various domains. However, with the rapid advancements in machine learning and deep learning , systems like ChatGPT and other large language models are already nearing some characteristics of AGI . These models can engage in complex conversations , demonstrate problem-solving abilities, and even generate creative content. While they do not yet exhibit the full range of human-like understanding or general intelligence, their capabilities represent a significant step toward AGI. Many researchers now believe that systems like these, with further refinement and development, could bridge the gap to true AGI within the next few years . The Leap to Superintelligence While AGI represents human-level intelligence, superintelligence goes a step further — referring to an AI that not only matches but exceeds human intelligence in all aspects, including creativity, decision-making, and problem-solving. Superintelligence would possess cognitive abilities far beyond the brightest human minds, enabling it to solve problems and make decisions in ways that humans could not even begin to comprehend. The potential transition from AGI to superintelligence raises significant concerns about control and safety . Superintelligence, with its capacity to operate independently and improve upon itself, could lead to an intelligence explosion, where its capabilities grow exponentially. The risk here lies in the possibility that humans may lose control of such an entity, as its goals and decision-making processes could become unexplainable , unpredictable , and uncontrollable . This presents a stark contrast to AGI, which aims to mirror human intelligence, while superintelligence would operate on a completely different, far superior level. AGI — All rights reserved The End The history of artificial intelligence is a testament to humanity’s relentless pursuit of understanding and replicating the essence of intelligence. From mechanical calculators to neural networks and beyond, each milestone reflects a deeper grasp of both technology and ourselves. As AI becomes increasingly integrated into daily life, shaping industries, economies, and cultures, it prompts profound questions about consciousness, ethics, and the future of human-machine coexistence. The journey of AI is far from over: it’s an ever-unfolding narrative that challenges us to imagine and construct the possibilities of tomorrow.",
    "raw_content": "From Pythagoras to Modern AI, AGI and Beyond Evolution of AI — All rights reserved The history of artificial intelligence (AI) is a captivating chronicle that stretches back centuries, weaving through epochs of human thought and innovation. It’s a story not just of technological milestones but of the persistent human quest to replicate, and perhaps surpass, the faculties of the mind through science, technique and technology. 6th century BC — Pythagoras and Numbers Pythagoras , a philosopher from the 6th century BC , was one of the earliest thinkers to propose that numbers could explain and represent the reality of the material world . Pythagoras — All rights reserved His teachings, along with those of his followers, the Pythagoreans , placed profound emphasis on numerology and the belief that numbers were the fundamental essence of the universe. Pythagoras viewed numbers as the ultimate truth behind all natural phenomena, marking a revolutionary idea that would eventually resonate with fields as diverse as mathematics , science , and, centuries later, artificial intelligence (AI) . Pythagoras’ belief that numbers could describe the nature of the universe laid the philosophical groundwork for the numeric representation of reality. Today, AI systems function based on numerical data to process and understand the world. Whether it’s images, text, videos , or complex business and scientific problems, all forms of information are transformed into mathematical models and represented numerically. This echoes the Pythagorean view that numbers are at the core of reality, as AI abstracts real-world entities into data points and mathematical relationships. For Pythagoras and his followers, geometry and numbers were more than tools — they were seen as a means to understand the abstract and rational principles governing the world. The Pythagoreans represented numbers graphically, engaging with geometry as a way to reveal the hidden truths of the universe. This early work in abstract mathematical thinking helped establish the basis for rational exploration, a precursor to how modern AI systems approach problem-solving. The Pythagorean legacy is visible in the way AI represents the world today. AI models break down images into pixels , texts into data vectors , and problems into equations , all using the language of numbers. Pythagoras’ view of the universe as fundamentally governed by numbers, specifically natural numbers (positive integers), was revolutionary, but it’s important to note that zero was absent from his numerical framework. The concept of zero, which we will explore later, plays a crucial role in the modern development of AI and computational systems . 4th century BC — Plato’s Theory of Ideas Plato’s Theory of Ideas , also known as the Theory of Forms , represents one of the earliest steps toward envisioning an ideal intellectual reality, which, centuries later, would underpin the development of artificial intelligence . Plato — All rights reserved Plato proposed that the physical world we perceive is only a shadow of a higher, non-physical realm composed of Ideas or Forms — timeless, unchanging essences that represent the true nature of things. For example, while we may see imperfect versions of beauty or justice in the physical world, the Form of Beauty or Form of Justice exists in an ideal, abstract realm beyond physical reality. This notion of a “non-material” world made of abstract ideas created a framework that would later resonate with the conceptual underpinnings of AI, where information, algorithms, and models — immaterial entities — became the core of intelligent systems. In a sense, AI embodies a modern version of Plato’s idea: a non-physical domain where abstract patterns, models, and knowledge exist and operate in connection to the material world. Mathematical Platonism , which extends Plato’s theory into the realm of mathematics, argues that mathematical objects — numbers, shapes, and structures — are also abstract, non-physical entities that exist independently of human thought and perception. This metaphysical perspective challenges the view that all of reality is confined to the physical world, asserting instead that abstract concepts can hold intrinsic truth and value, even if they cannot be observed directly. In the context of AI, mathematical Platonism has particular significance. The development of artificial intelligence relies heavily on mathematical models , algorithms , and data structures , all of which are abstract objects. These mathematical structures allow AI systems to perform tasks like reasoning, problem-solving, and pattern recognition — abilities that operate beyond the tangible physical world. AI’s ability to simulate human intelligence through mathematical processes echoes Plato’s vision of a reality based on mathematics and ideas. The Ultimate Definition of Artificial Intelligence By introducing the idea that abstract forms or ideas exist in a realm separate from physical reality, Plato’s Theory of Ideas laid the philosophical groundwork for understanding information and knowledge as independent entities — concepts that are foundational to AI. Just as Plato’s forms are considered more “real” than their physical counterparts, AI systems today operate on the basis of abstract representations of knowledge that drive machine learning, reasoning, and decision-making, transcending the physical limitations of the human brain and body. In essence, Plato’s philosophy anticipated a future where intellect constructs — be they forms, ideas, or algorithms — would play a central role in understanding and shaping reality, laying a conceptual foundation for the development of artificial intelligence as we know it. 400 BCE to 400 CE — Indo-Arabic numerals The Indo-Arabic numerals , also known as Arabic numerals , are the most widely used symbolic representation of numerical entities in the world and are considered a cornerstone in the development of mathematics. A key distinction can be made between the positional system they employ, also known as the Indo-Arabic numeral system , and the specific glyphs used to represent the numbers. The first numerical system originated in India between 400 BCE and 400 CE . These numerals were first transmitted to Western Asia, where they were mentioned in the 9th century , and later spread to Europe in the 10th century . The Arabs assimilated elements from various cultures they encountered and forged the Arabic-Indian numerical system, from which the modern Arabic numerals evolved. Knowledge of these numbers reached Europe through the work of Arab mathematicians and astronomers, and this evolved numerical system became known in Europe as the “Arabic numerals.” In Arabic, the Eastern Arabic numerals are referred to as “Indian numerals” (in Arabic: أرقام هندية, ’arqām hindiyya), and although the same system is used, the glyphs differ (٠١٢٣٤٥٦٧٨٩). The symbols from 0 to 9 in the Indo-Arabic numeral system evolved from the Brahmi numerals . Buddhist inscriptions from around 300 BCE used symbols that eventually became 1 , 4 , and 6 , and a century later, the symbols for 2 , 7 , and 9 were recorded. The Indo-Arabic numeral system is fundamental for AI because it introduced the positional number system . This numerical system allows for the representation and manipulation of large data sets, complex calculations, and algorithms. The numerical Matrix of reality — All rights reserved Without the numeric representation introduced by this system, the development of computers and, by extension, artificial intelligence would not be possible. 7th Century — Zero: The Most Important Number Zero (0) is not just a number; it represents a profound concept — the idea of an empty quantity . The development of zero as a written digit in decimal place value notation was a crucial advancement, originating in India . A symbol for zero, initially depicted as a solid dot, was used throughout the Bakhshali manuscript, a practical guide on arithmetic for merchants. Brahmagupta by AI — All rights reserved Brahmagupta , a 7th-century Indian mathematician, was the first to treat zero as a number like any other, formulating rules for its use in arithmetic. His work in the Brahmasputha Siddhanta included the sum of zero with itself as zero, establishing zero as a key player in the numerical system. Although he misunderstood the concept of division by zero, his pioneering treatment of zero laid the groundwork for its integration into broader mathematical thought. Zero later traveled through the Arabic world , where it was embraced and further refined, before reaching Western Europe in the 11th century. The Italian mathematician Fibonacci played a key role in introducing the Hindu–Arabic numeral system to Europe. Fibonacci’s use of the term “zephyrum” led to the modern word zero , a name that evolved through Italian influences. Zero — All rights reserved The introduction of zero revolutionized mathematics, particularly in the development of the positional number system , and became the foundation for binary code — the language of modern computers. Without zero, binary mathematics, which underpins all modern computing and artificial intelligence , would not exist. Zero allows for the representation of absence , making it a fundamental concept for data processing, machine learning, and the structure of algorithms in AI systems. In essence, the creation of zero as a mathematical entity was a key turning point that enabled the digital world and made the future of AI possible. 9th Century — The Word Algorithm was Born The word algorithm has its origins in the region of Khwãrezm (modern-day Turkmenistan and Uzbekistan ), an area known for its arid desert landscapes but sustained by the Amu Darya river, which supports large-scale irrigation. One of Khwãrezm’s most notable figures was Muhammad ibn Mūsa al-Khwarizmī (جعفر محمد بن موسی ), a 9th-century Persian scholar, astronomer, geographer, and mathematician whose work had a profound impact on the development of mathematics . He is known as the father of algebra . The Latinization of his name, meaning “the native of Khwãrezm” in Persian, gave rise to the English word algorithm . Muḥammad ibn Mūsā al-Khwārizmī — All rights reserved Al-Khwarizmī’s influential book on Hindu-Arabic numerals, originally written in Arabic, was translated into Latin as “Algoritmi de numero Indorum” (meaning “Al-Khwarizmi on the Hindu Art of Reckoning”). This Latin term, algoritmi , eventually evolved into algorithm in English. Today, the simplest definition of algorithm is a set of rules or procedures that define a sequence of operations , and it is fundamental in fields ranging from computer science to daily online activities. Algorithms power everything from Google’s search engine to Facebook’s news feed, embodying a process that has its roots in al-Khwarizmī’s groundbreaking work over a millennium ago. 1642 — Early Beginnings: Mechanical Calculators and Philosophical Foundations Our journey continue in 1642 , when French mathematician Blaise Pascal invented the Pascaline , the first mechanical calculator capable of performing basic arithmetic operations such as addition and subtraction. Although simple by today’s standards, the Pascaline was a groundbreaking device for its time, marking the first significant step toward mechanized computation. Designed to assist Pascal’s father in his work as a tax collector, the Pascaline used a series of interlocking gears and dials to automate calculations. Pascaline This invention not only reduced human error in arithmetic but also symbolized a bold shift from manual calculation to mechanical processes, laying the foundation for future developments in computational machinery. While primitive compared to modern computers, the Pascaline’s creation represented the early vision of automating logical tasks , a concept that would evolve over the centuries into the digital computers that power the modern world. Blaise Pascal — All rights Reserved In the latter half of the 17th century, German polymath Gottfried Wilhelm Leibniz advanced these ideas by developing the Stepped Reckoner , a device capable of multiplication and division. More importantly, Leibniz philosophized about a universal language of logic, an “alphabet of human thought,” envisioning a machine that could manipulate symbols as humans do — a conceptual precursor to symbolic AI. Although Leibniz lacked the technology to build such a machine, his dream was to plant to an early seed for the future of artificial intelligence. Gottfried Wilhelm von Leibniz — All rights Reserved 17th century — Binary System One of Leibniz’s most profound contributions to the history of computing was his conceptualization of a calculator based on the binary number system , an idea that was revolutionary for its time. Although the binary system had been introduced earlier by Spanish scholar Juan Caramuel, Leibniz was the first to apply it to mechanical computation. His design envisioned a machine that operated using marbles as binary markers. Gottfried Wilhelm Leibniz Bibliothek, Hannover The presence of a marble in a particular position would represent the value 1 , while its absence would signify 0 . This early attempt at binary computation laid the theoretical groundwork for modern digital computing, where binary code is the fundamental building block of all computer operations. Leibniz’s insight into the potential of binary logic and its simplicity would influence the development of logic gates and algorithms centuries later, forming a cornerstone of today’s computational theory. Flashback: 800 BCE — The I Ching as a Binary System The I Ching , one of the oldest classical Chinese texts, dating back to around 800 BCE , serves as a profound universal model based on a binary system. The I Ching’s Yin and Yang symbols represent the complementary duality of nature — Yin (dark) and Yang (light) — which can be understood as a way of counting in twos, or a binary system . Yin and Yang This binary logic, expressed through 64 hexagrams , is deeply connected to ancient Chinese cosmology and the balance of opposing forces. In the 17th century, Gottfried Wilhelm Leibniz was inspired by the I Ching when he developed the binary number system — the foundation of modern computing. The hexagrams of the I Ching in a diagram belonging to the German mathematician philosopher Gottfried Wilhelm Leibniz Leibniz recognized that the Yin and Yang symbols could be interpreted as 0 and 1 , the core of binary notation. His work laid the groundwork for binary code, still used today in computers , where 1 and 0 represent the on and off states of digital systems. Leibniz’s connection between his binary system and the I Ching highlights how ancient philosophical systems can influence modern technological advances. The I Ching’s binary structure, with its deep reflection on duality and balance , is thus not only a philosophical tool but also a precursor to the mathematical logic that underpins today’s digital world . 1752–A new World with Electricity Benjamin Franklin , an American polymath, is widely credited for his famous 1752 experiment that demonstrated the connection between lightning and electricity . By attaching a wire to a kite during a thunderstorm, Franklin showed that lightning consists of electrical energy, a breakthrough in understanding natural phenomena. However, while Franklin’s experiment was pivotal, the discovery and understanding of electricity cannot be attributed to any one person. Rather, the study of electricity evolved over centuries, with numerous scientists and thinkers making critical contributions to its development. AI generated Image of an early 20th-century city street with power poles lining both sides — All rights reserved 1834 — The Dawn of Programmable Machines The 19th century witnessed significant leaps with Charles Babbage and Ada Lovelace . Babbage designed the Analytical Engine in 1834, a general-purpose, programmable computing device. Ada Lovelace, often celebrated as the world’s first computer programmer , wrote algorithms for this machine and foresaw its potential beyond mere calculation, imagining it composing music or art — a remarkably prescient vision of software. Binary Number System — All rights Reserved In the 1850s , George Boole developed Boolean algebra , a groundbreaking system of mathematical logic that would later form the core of binary systems. Boole’s work introduced a method for representing logical statements using binary variables, with true and false values denoted as 1 and 0 , respectively. This innovation provided the theoretical foundation for the design of digital circuits and laid the groundwork for modern computing. Boolean algebra became essential for constructing computer circuits, enabling machines to perform logical operations, and it also became integral to programming languages. Boole’s insights into logic and binary reasoning remain fundamental to the architecture of today’s computers and digital technology. 1943 - The Birth of Neural Networks The mid-20th century was a pivotal period for the development of foundational AI concepts. In 1943 , Warren McCulloch and Walter Pitts published a landmark paper that introduced a model of artificial neurons , inspired by the structure and function of the human brain. Their work proposed that neural networks could simulate the workings of biological neurons, establishing a theoretical framework for computational neuroscience and paving the way for machine learning . This research became one of the cornerstones of AI, influencing the development of algorithms that allow machines to learn from data and recognize patterns — principles that continue to drive advancements in AI today. Artificial model [McCulloch and Pitts, 1943] of a biological neuron. As it can be observed, the relation between the input and output follows a nonlinear function called activation function. In the first model shown in this figure, the activation is a hard threshold function 1945 — ENIAC was Born ENIAC (/ˈɛniæk/; Electronic Numerical Integrator and Computer) was the world’s first programmable, electronic, general-purpose digital computer , completed in 1945 . ENIAC (Electronic Numerical Integrator And Computer) in Philadelphia, Pennsylvania. While other earlier machines had some of these capabilities, ENIAC was the first to combine them all, making it a groundbreaking development in computing history. It was Turing-complete , meaning it could be reprogrammed to solve a wide variety of numerical problems . John von Neumann used ENIAC to make the first computer-based weather forecast. In that particular experiment, he processed 250,000 floating-point operations in about 24 hours and produced a 24-hour forecast from the input data. John von Neumann The versatility and reprogrammable nature set ENIAC apart and established it as a major milestone in the evolution of modern computing. 1947 — The First Working Transistor: A Milestone in AI’s Technological Foundations The invention of the first working transistor in 1947 by the team at Bell Labs marked a pivotal breakthrough in electronics and computing, laying the groundwork for the development of modern artificial intelligence . Prior to this achievement, the fragile designs of early components like the cat’s whisker detectors struggled to reliably control electrical current. However, the Bell Labs team, led by John Bardeen, Walter Brattain, and William Shockley, eventually succeeded with the creation of the point-contact transistor . The Bell Labs team The transistor’s ability to amplify signals and control current flow revolutionized the field of electronics, replacing bulky vacuum tubes and enabling the creation of smaller, faster, and more efficient computers. Replica of the first transistor The transistor became an ideal component for the binary system — the foundation of modern computing and AI. Since the binary system operates on two states — on (1) and off (0) — transistors are perfectly suited for manipulating these states. Transistors can switch between these on and off positions rapidly and reliably, which is essential for processing binary data in computers. This capability enabled faster and more efficient computation, laying the groundwork for AI systems. The invention of the transistor is, therefore, a foundational step in the history of AI development, as it provided the necessary technology to build the powerful computers. 1950 — The Turing Test In 1950 , British mathematician Alan Turing introduced the concept of the Turing Test in his influential paper “ Computing Machinery and Intelligence.” Turing suggested that if a machine could engage in a conversation with a human and its responses were indistinguishable from those of a human, the machine could be considered intelligent. This idea offered one of the first concrete methods to assess artificial intelligence , becoming a critical benchmark in AI philosophy. Although debated and sometimes criticized, the Turing Test remains a key reference point in discussions about the nature and limits of machine intelligence. Alan Turing — All rights reserved 1956 — The Coining of “Artificial Intelligence” and Early Robotics The term “Artificial Intelligence” was first coined in 1956 during the historic Dartmouth Conference , organized by John McCarthy , Marvin Minsky , Nathaniel Rochester , and Claude Shannon . This conference is widely regarded as the official birth of AI as a distinct field of study. It brought together leading researchers who believed that machines could be made to simulate aspects of human intelligence, and the event laid the foundation for the future of AI research. The ideas and discussions that emerged from Dartmouth helped shape AI’s trajectory for decades, marking a pivotal moment in the evolution of computer science and cognitive science. If you want to know more about artifical intelligence definition, please read my article: 1958 — The Integrated Circuit: A Revolutionary Milestone in Technology An integrated circuit (IC) , also known as a microchip , is a small yet powerful electronic device made up of interconnected components such as transistors, resistors, and capacitors. These components are etched onto a small piece of semiconductor material , typically silicon. ICs are found in nearly all modern electronic devices, from smartphones to computers , and play a vital role in the functionality of today’s technology. The invention of the first microchip in 1958 by Jack Kilby , an engineer at Texas Instruments, was a groundbreaking moment in technological history. Kilby’s prototype, which integrated discrete electronic components into a single chip, marked the beginning of the miniaturization of electronic devices . https://www.flickr.com/photos/_fabrizio_/2497324185 Before ICs, electronic devices, particularly computers, were large and cumbersome, but with the invention of the microchip, these devices became more compact and efficient, enabling the rapid development of the modern electronics industry. Kilby’s contribution to technology earned him the Nobel Prize in Physics in 2000 , highlighting the significance of his innovation. 1961 — First Industrial Robot: Unimate The 1960s saw the advent of the first industrial robots. In 1961, Unimate , the first programmable robot, was deployed in a General Motors assembly line, revolutionizing manufacturing processes. It was invented by George Devol in the 1950s using his original patent filed in 1954 and granted in 1961. Representation of a polar robot. The figure is inspired by the first industrial robot, the Unimate, and it is based on the image from the following webpage https://www.yaskawa-global.com/product/robotics/about 1965 — The Birth of Moore’s Law In 1965 , Gordon Moore , co-founder of Intel, made a visionary observation, now known as Moore’s Law : the number of transistors on a microchip was doubling approximately every two years, resulting in exponential increases in computational power while simultaneously reducing costs. Though initially related to central processing units ( CPUs ), this observation became pivotal for the broader evolution of computing technology, including artificial intelligence. Moore’s Law Transistor Count 1970–2020 As computational power increased, more sophisticated AI algorithms could be developed and executed, driving advancements in machine learning, deep learning, and natural language processing. For decades, the growth in CPU capabilities supported the rise of AI, but modern AI breakthroughs have increasingly relied on Graphics Processing Units (GPUs) rather than traditional CPUs. GPUs are designed to handle massive parallel processing, making them ideal for training complex AI models, especially deep neural networks, which require handling enormous amounts of data and computations. The shift from CPU-based to GPU-based computation has been instrumental in the rapid evolution of AI. GPUs, with their ability to process many tasks simultaneously, have unlocked new levels of performance in training AI models, particularly in areas such as image recognition, language models, and autonomous systems. This evolution , driven by both Moore’s Law and the advent of GPUs, has laid the foundation for the AI capabilities we see today. 1966 — ELIZA: Pioneering Human-Machine Communication From 1964 to 1967 , MIT researcher Joseph Weizenbaum developed ELIZA , one of the earliest natural language processing programs, with the goal of exploring communication between humans and machines. ELIZA operated by simulating conversation through a clever pattern-matching and substitution method, creating the illusion of understanding without truly comprehending the meaning behind the words. While the program didn’t have any real capacity for understanding language, it was able to give users the impression that they were engaging in a meaningful exchange. Written in MAD-SLIP and utilizing language capabilities encoded in separate “scripts,” ELIZA’s most famous script was DOCTOR , which mimicked the behavior of a Rogerian psychotherapist. This script allowed ELIZA to reflect users’ statements back to them with non-directive, open-ended responses, much like a real therapist might. Although simple by today’s standards, ELIZA’s conversational style was revolutionary, marking one of the first significant attempts at creating a chatbot capable of simulating human interaction. ELIZA’s ability to engage users with seemingly intelligent responses laid the groundwork for the development of modern chatbots and set an early example for programs attempting to pass the Turing Test . Though rudimentary, ELIZA sparked a new era of human-computer interaction, influencing decades of research in artificial intelligence and natural language processing. Try ELIZA with a modern software reproductions of the original: Google Play: ELIZA 1966 — The First Chatbo t 1970 — Backpropagation: A Cornerstone of Neural Network Training In machine learning , backpropagation is a foundational algorithm used to train neural networks by efficiently calculating gradient estimates for network parameter updates. Leveraging the chain rule from calculus, backpropagation computes the gradient of the loss function with respect to each weight in the network, enabling iterative weight updates that minimize errors and enhance performance. The basics of backpropagation were first explored in the early 1960s within the context of control theory and the application of the chain rule . The modern version of backpropagation was created by Seppo Linnainmaa, a Finnish mathematician and computer scientist, in 1970. Seppo Linnainmaa wrote his thesis Algoritmin kumulatiivinen pyöristysvirhe yksittäisten pyöristysvirheiden Taylor-kehitelmänä [The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors] (PDF) (Thesis) . Its formal application to neural networks, however, gained prominence through the 1986 Nature paper, “Learning Representations by Back-propagating Errors,” co-authored by David Rumelhart , Geoffrey Hinton , and Ronald Williams . This landmark paper popularized the term and methodology of backpropagation, cementing it as the primary technique for training modern neural networks . Backpropagation plays an essential role in deep learning , allowing neural networks to learn complex patterns by adjusting their parameters to minimize prediction errors. While the algorithm itself calculates gradients, it provides the essential groundwork for various optimization techniques that further refine the network weights during training. Geoffrey Hinton — Vaughn Ridley/Collision via Sportsfile — Collision Conf Highlighting the impact of their work, Hinton and John Hopfield received the 2024 Nobel Prize in Physics for their discoveries that enabled the development of neural networks, paving the way for modern AI. Their work has driven the advancement of backpropagation as a vital tool in AI, fostering the rapid evolution and application of deep neural networks. Deep Neural Network Rumelhart; Hinton; Williams (1986). “Learning representations by back-propagating errors” (PDF). Nature. 323 1970s — The Rise of LISP and Prolog The 1970s saw the emergence of two pivotal programming languages in the field of artificial intelligence: LISP and Prolog . LISP , developed by John McCarthy , became the dominant language for AI research due to its exceptional ability to process symbolic data and its flexibility for recursive functions. It was integral to the development of early AI applications, including natural language processing and problem-solving algorithms. Lisp Logo — https://common-lisp.net/ Prolog , on the other hand, was created to excel in symbolic reasoning and logic-based AI tasks. It became particularly valuable for developing expert systems , where the ability to model complex logical relationships and deduce solutions was critical. Together, LISP and Prolog laid the computational foundation for AI’s growth in areas like machine learning , theorem proving , and automated reasoning , and their influence continues to shape AI development today. 1973 — WABOT-1: The First Humanoid Robot In 1973 , Japan introduced WABOT-1 , the world’s first full-scale humanoid robot, marking a major milestone in the application of AI to robotics . WABOT-1 could walk, grip objects with its hands, and even engage in basic conversations, making it one of the earliest examples of AI-powered robotic autonomy. https://www.ntticc.or.jp/en/archive/works/wabot-1/ Its creation showcased how AI could be integrated with physical machinery to create robots capable of interacting with the real world, laying the groundwork for future advancements in robotics, automation, and human-machine collaboration. The Rise and Falls: AI Winters First AI Winter (1974–1980) During the 1970s, artificial intelligence faced significant critiques and financial setbacks . AI researchers had initially underestimated the complexity of the problems they were tackling, leading to widespread optimism and inflated public expectations. When the results failed to meet those high expectations, funding for AI research was significantly reduced. Many of the techniques being employed at the time were not advanced enough to solve the problems researchers faced, leading to a period of disillusionment. First AI winter — All rights reserved Despite these challenges, the setbacks did not halt the progress of AI. While funding cuts impacted some major AI laboratories, the broader field continued to grow. Public interest in AI persisted , and the number of researchers increased substantially. During this time, new areas of research emerged, including logic programming and commonsense reasoning. Historian Thomas Haigh argued in 2023 that this period did not represent a true “winter” for AI, as research continued to flourish. AI pioneer Nils Nilsson even described the era as the most “exciting” time to work in the field, as new ideas were being developed and explored. 1980s — The Rise and Fall of Expert Systems The 1980s were a period of intense growth and experimentation in expert systems , marking the development of second-generation expert systems . Unlike their predecessors, these systems introduced probabilistic models, allowing them to reason about causes and effects more effectively. This new wave of expert systems found practical application in industrial and commercial fields , driving a surge of interest and investment in AI. One of the most successful implementations of this era was R1 (or Xcon) , developed at Carnegie Mellon University by John McDermott in 1978 and deployed by Digital Equipment Corporation in 1982. R1 revolutionized computer order configuration, ensuring orders were complete and spatially optimized. By 1986, the system saved the company $40 million annually, showcasing the economic potential of expert systems. The success of R1 and similar systems spurred significant investment in knowledge engineering . Countries like Japan, followed by the United States, United Kingdom, and other parts of Europe, heavily invested in AI-driven technologies. Expert systems became a focal point in the development of advanced computer systems, particularly in industries requiring specialized knowledge and decision-making. However, despite their early success, second-generation expert systems encountered several challenges. Writing and maintaining the complex rules that mirrored expert knowledge proved difficult, and the technology’s development lagged behind the growing hype. The arrival of more powerful general-purpose computers from companies like Apple and IBM began to outpace the specialized hardware built for AI. By 1987 , disappointment had set in, culminating in DARPA’s decision to cut funding for AI research, having invested $100 million just two years prior. This marked the beginning of another AI winter , as enthusiasm for expert systems faded and investment dried up. While expert systems had demonstrated the potential of AI, the limitations of the technology at the time led to a sharp decline in interest, setting back the field until its revival in the mid-1990s. 1984 The Macintosh , or Mac , introduced by Apple in 1984, played a pivotal role in the broader development of technology, including the evolution of artificial intelligence . Its revolutionary graphical user interface (GUI) transformed how people interacted with computers, making technology more accessible to non-experts. This shift laid the groundwork for user-friendly AI systems, where interaction with machines no longer required deep technical knowledge. 1984 — Apple Mac The Mac’s intuitive design encouraged the development of software and tools that emphasized ease of use, which would later influence the development of AI-driven interfaces and systems. As AI advanced, the importance of making it accessible to a wider audience became paramount. The Mac’s emphasis on visual interfaces and user-centric design directly influenced how AI applications, such as virtual assistants, are created today — systems that can engage users naturally and intuitively. Furthermore, the Mac’s architecture fostered innovation in computational tools and software, enabling developers to build increasingly complex programs that could incorporate early AI models. Its legacy of innovation in human-computer interaction has continuously shaped the development of AI technologies, which, like the Mac, aim to bridge the gap between complex machine processes and seamless, user-friendly experiences. The Macintosh commercial, famously aired during the 1984 Super Bowl , was more than just a product advertisement — it was a bold cultural statement. Directed by Ridley Scott and inspired by George Orwell’s novel 1984 , the ad depicted a dystopian world ruled by conformity and control, symbolized by an omnipresent “Big Brother.” In contrast, the Macintosh represented individuality, creativity, and liberation from technological oppression. The slogan, “Why 1984 won’t be like 1984 ,” framed the Mac as a tool of empowerment in a world threatened by the dangers of authoritarian technology. This imagery of utopian versus dystopian futures is closely tied to the narrative surrounding artificial intelligence . Like in Orwell’s novel, AI technologies evoke both utopian dreams and dystopian fears. On one hand, AI promises a future of enhanced creativity, problem-solving, and freedom, where machines can assist humanity in reaching its full potential. On the other hand, AI also stirs fears of a dystopian world where intelligent systems could control, surveil, and diminish human autonomy. The Macintosh ad tapped into these cultural anxieties and aspirations by positioning technology, and implicitly future developments like AI, as tools that could either liberate or subjugate society, depending on how they are designed and controlled. This duality continues to shape the discourse around AI, as we navigate the fine line between technology as an instrument of progress and as a potential threat to human freedom. Second AI Winter (Late 1987 — Early 1993) The 1980s witnessed a surge in commercial interest in artificial intelligence, fueled by high expectations and significant investments. Businesses, eager to capitalize on AI’s potential, anticipated rapid advancements, particularly in achieving human-level intelligence. However, as many AI companies failed to meet these lofty promises , disillusionment set in. The inability to deliver on bold claims led to a sharp decline in confidence within the business community, which began to view AI as a technology that was not yet viable. Second AI Winter — All rights reserved This widespread loss of faith resulted in a severe downturn for AI, often referred to as the Second AI Winter . Funding dried up, and many AI projects were abandoned or significantly scaled back. The damage to AI’s reputation was so profound that it continued to affect perceptions of the technology well into the 21st century. Despite AI’s potential, the gap between expectations and reality during this period led to one of the most significant setbacks in the field’s history. Windows 95: A Catalyst for AI and the Growth of Personal Computing The release of Windows 95 by Microsoft in 1995 marked a significant turning point in the evolution of personal computing. As the first widely popular operating system for personal computers, Windows 95 brought advanced functionality and an intuitive user interface to millions of users around the world. Its success laid the foundation for the mass adoption of personal computers, which in turn played a crucial role in the development of artificial intelligence . Windows 95 revolutionized computing by introducing features like the Start menu , taskbar , and support for multitasking , making computers far more accessible and functional for everyday users. This expansion of personal computing dramatically increased the number of people interacting with technology on a daily basis, creating a fertile ground for advancements in software development, including AI applications. The importance of Windows 95 to AI development cannot be overstated. The operating system made computing ubiquitous . By democratizing access to technology, Windows 95 accelerated the rate at which both developers and everyday users could interact with computers and software, ultimately driving the demand for more intelligent systems. In short, the launch of Windows 95 was not just a milestone for personal computing — it also indirectly fostered the conditions for AI research and development to thrive. By making computing accessible to the masses, Windows 95 laid the groundwork for the digital infrastructure that supports AI innovation today, proving that the growth of personal computing is intricately linked to the rise of artificial intelligence. 1990 — Linux: Pioneering Force for AI Computation In the early 1990s , Linux emerged as a revolutionary operating system, developed by Linus Torvalds in collaboration with the Free Software Foundation (FSF) . Although still in its infancy, Linux’s open-source nature and flexible architecture hold the potential to become a foundational platform for future artificial intelligence development. As AI continues to evolve, Linux’s adaptability will likely make it the ideal environment for AI research and machine learning, due to its unparalleled customizability and scalability. Illustrates the current (as of August 2016) Linux device drivers for AMD hardware Looking forward, Linux’s open-source philosophy is set to empower researchers and developers to innovate freely, tailoring their systems for AI-specific needs. This flexibility could prove crucial as AI models grow more complex and require highly specialized environments to optimize performance. By allowing users to modify its core functionalities, Linux will likely become indispensable for building and fine-tuning the AI systems of the future. Linux’s early support for GPU acceleration also hints at its future importance in deep learning . As AI models begin to demand more computational power, the ability to harness the parallel processing capabilities of GPUs will be crucial for training neural networks. The open nature of Linux will make it easier to integrate emerging technologies like CUDA (Compute Unified Device Architecture), paving the way for distributed computing frameworks that will accelerate AI development. Furthermore, Linux’s command-line interface and scripting capabilities will facilitate the automation of complex AI workflows. This is likely to be critical as researchers scale their experiments, managing vast datasets and running intricate algorithms across distributed systems. Its compatibility with large-scale AI clusters will enable Linux to handle the computing power required to push the boundaries of AI, whether on single machines or across powerful, networked infrastructures. As we look to the future, Linux is positioned to be a driving force behind AI’s progress. Its scalability , flexibility , and support for cutting-edge technologies will likely ensure its place as the preferred operating system for the next wave of AI innovation, shaping the digital landscape for decades to come. 1991 — Schmidhuber’s Key Contributions to AI In 1991 , Jürgen Schmidhuber made groundbreaking advancements in deep learning by addressing a major issue — how to train neural networks effectively over long sequences. To solve this, he proposed a hierarchy of recurrent neural networks (RNNs) , allowing the networks to learn at different time scales, making the training process more efficient. This approach enabled RNNs to handle deep learning tasks with much greater complexity. That same year, Schmidhuber introduced the idea of adversarial neural networks , where two networks compete in a zero-sum game . One network generated patterns, while the other learned to predict outcomes, a concept later used in the development of Generative Adversarial Networks (GANs) . Additionally, Schmidhuber supervised Sepp Hochreiter , whose work on solving the vanishing gradient problem led to the creation of Long Short-Term Memory (LSTM) networks. These LSTMs have since become a vital component in many modern AI systems 1996 — The Importance of Cloud Computing in AI Development The evolution of cloud computing , with its roots in the 1960s and the concept of time-sharing , has played a pivotal role in enabling the rapid development of artificial intelligence . Initially, large-scale computing was restricted to mainframes, where users submitted jobs to centralized data centers. Over time, the rise of virtualized services and the introduction of the “cloud” metaphor in the 1990s transformed the landscape, making massive computational power accessible to more users than ever before. The expression cloud computing became more widely known in 1996 when Compaq Computer Corporation drew up a business plan for future computing and the Internet. The company’s ambition was to supercharge sales with “cloud computing-enabled applications”. Diagram showing overview of cloud computing For AI, cloud computing has been instrumental in overcoming the limitations of local hardware. AI models, especially those driven by deep learning and neural networks, require vast amounts of computational power and data storage. Cloud computing provides the infrastructure necessary to train large-scale models by allowing access to distributed, high-performance computing resources without requiring individual organizations to own expensive hardware. This scalability is crucial for AI research, where the ability to process enormous datasets in parallel is a key factor in advancing machine learning and natural language processing . Moreover, cloud computing enables collaboration across institutions and industries, facilitating the sharing of datasets and AI tools globally. It allows AI developers and researchers to leverage cloud-based platforms to experiment with AI models , deploy applications, and scale AI-driven services without the constraints of local computing resources. By democratizing access to high-performance computing , cloud technology has made AI development more accessible, fostering innovation and accelerating advancements in the field. In essence, the integration of cloud computing into the fabric of AI development is not just a technical improvement; it represents a critical infrastructure that powers the next generation of AI applications . Without the cloud’s ability to handle intensive computational workloads, many of the breakthroughs in AI, from autonomous systems to personalized algorithms , would not be possible. Modern Milestones: From Deep Blue to AlphaGo 1977 — AI in the Public Eye: Deep Blue vs. Garry Kasparov The late 20th and early 21st centuries saw artificial intelligence enter the public consciousness through a series of high-profile milestones, the most notable being IBM’s Deep Blue defeating world chess champion Garry Kasparov in 1997 . This event demonstrated the immense power of AI-driven computation and strategic programming, marking a turning point in the perception of AI’s capabilities. Garry Kasparov and Deep Blue Deep Blue vs. Garry Kasparov was a pair of six-game chess matches between Kasparov, the reigning world chess champion, and Deep Blue , an IBM supercomputer designed specifically for chess. The first match, held in Philadelphia in 1996 , resulted in a victory for Kasparov, who won 4–2. However, in a highly anticipated rematch in New York City in 1997 , Deep Blue emerged victorious, becoming the first computer to defeat a world chess champion under standard tournament conditions. This victory was more than just a technological achievement — it was a profound symbolic moment. Deep Blue’s triumph over Kasparov, one of the greatest intellectual champions of the human mind, represented the increasing sophistication of AI systems. It showcased the potential of brute-force computation , where the computer could calculate and evaluate millions of moves per second, alongside carefully designed algorithms that mimicked strategic decision-making. Animation of Deep Blue versus Kasparov The 1997 rematch, chronicled in the documentary “Game Over: Kasparov and the Machine,” sparked a global conversation about the relationship between human and machine intelligence. Deep Blue’s win was hailed as a watershed moment, a sign that AI had reached a new level of capability. While the system’s approach was not true “intelligence” as humans understand it — relying on raw computational power rather than intuitive thought — its success underscored the growing potential of AI to compete with and surpass humans in specific intellectual tasks. Garry Kasparov versus Deep Thought Documentary 1999 — Sony’s AIBO: The Intelligent Robotic Dog In 1999 , Sony unveiled AIBO, a groundbreaking robotic dog designed to be an “intelligent and trainable robot companion.” AIBO was capable of performing a wide range of lifelike behaviors, such as walking, barking, whining, growling, wagging its tail, and playing with a ball — all without the need for traditional pet care. What made AIBO truly unique was its ability to adapt its behavior based on interactions with its human owners, allowing it to develop a distinct “personality” over time. AIBO 1999 — Autore: Tom Mesic AIBO was equipped with a variety of sensors, including a CCD color camera , two microphones, infrared sensors, touch sensors on its head, chin, back, and legs, as well as an accelerometer and temperature sensor. These allowed AIBO to navigate its environment, recognize objects and people, and respond to voice commands. Aiboware , the software loaded onto memory sticks, powered AIBO’s movements and enabled it to evolve from a “puppy” stage into an “adult” based on its interactions with its owner. Once fully developed, AIBO could recognize up to 100 voice commands, though, like a real dog, it didn’t always obey. AIBO’s OPEN-R software development kit also allowed owners to program the robot, further personalizing its behavior. Sony’s attention to detail made AIBO not only technically advanced but also expressive, thanks to 18 joints in its legs, neck, tail, and jaw that allowed for fluid, natural movements. AIBO was a significant step in AI robotics, showcasing the potential for intelligent companions that could interact dynamically with humans. Its ability to learn and evolve made it more than just a novelty — it was a glimpse into the future of adaptive AI systems capable of responding to their environment and forming deeper connections with users. Modern model: 2002 — iRobot Launches Roomba: AI Enters the Household In 2002 , iRobot revolutionized household chores with the launch of the Roomba , an autonomous robotic vacuum cleaner that brought AI into everyday life. Unlike traditional vacuum cleaners, Roomba used AI-driven sensors and algorithms to navigate rooms, avoid obstacles, detect dirtier areas, and efficiently clean floors without human intervention. Its ability to map environments , adjust to different floor types , and work autonomously made it a practical and appealing tool for consumers. Roomba’s commercial success highlighted the practical applications of AI in solving mundane tasks, proving that robotics and AI could seamlessly integrate into domestic life. The Roomba was not just a gadget; it was one of the first widely adopted AI-powered consumer robots, bringing robotics into millions of homes. This success story also paved the way for further innovations in smart home technologies and robotics, illustrating the potential of AI to enhance everyday living. First-generation Roomba 2006 — The Rise of Deep Learning: A New Era in AI In 2006 , Geoffrey Hinton , a leading figure in AI research, helped bring deep learning into the spotlight, marking a pivotal moment in the history of artificial intelligence. By harnessing the power of neural networks , Hinton and his team demonstrated how machines could learn and make sense of vast amounts of data, leading to groundbreaking advances in fields such as computer vision , speech recognition , and natural language processing . Deep learning, a subfield of machine learning, enabled AI systems to automatically learn features and patterns from raw data without needing manual intervention or human-engineered features. This approach mimicked the way the human brain processes information, allowing machines to perform tasks such as image recognition, speech translation, and even playing games at superhuman levels. Deep learning leverages artificial neural networks (ANNs) , that is a model inspired by the structure and function of biological neural networks found in animal brains. An ANN is composed of artificial neurons — interconnected units or nodes — that loosely mimic the way biological neurons process and transmit signals. Each neuron receives input from connected neurons through synapse-like edges, processes the input, and then passes on a signal to other neurons. Neural Network — https://www.needpix.com/photo/1752528/ The “signal” is a real number, and the output is computed using a non-linear activation function based on the sum of the inputs. The strength of each connection is governed by a weight, which is adjusted during the learning process, allowing the network to fine-tune its performance through training . This enables deep learning models to recognize patterns, classify information, and make predictions with impressive accuracy. Hinton’s work with deep learning harnessed these neural networks to solve previously intractable problems. By allowing AI to learn complex representations of data on its own, deep learning opened new possibilities for AI applications in industries ranging from autonomous vehicles to healthcare and finance . The ability of neural networks to automatically learn and improve through training represented a monumental leap forward, ushering in the era of modern AI that continues to transform technology today. Apple in 2007 Launched the iPhone The first smartphone on the planet was not the iPhone, but IBM’s Simon Personal Communicator, introduced in 1992. It was the first mobile device to combine phone capabilities with computer-like features such as a touchscreen, calendar, email, and fax. However, the iPhone , launched by Apple in 2007 under Steve Jobs , revolutionized the smartphone industry with its intuitive multi-touch technology, sleek design, and powerful iOS operating system. Apple iPhone 1 The iPhone set a new standard for mobile devices, introducing features like app stores, larger screens, and video recording, and has since sold over 2.2 billion units by 2018, making it one of the most influential tech devices in history. Smartphones have not only transformed how we interact with technology but have also accelerated the practical implementation and integration of AI, shaping a future where AI is embedded in almost every facet of daily life. 2009 — The Role of GPUs in AI Development and Their Connection to Bitcoin and Cryptocurrency The Graphics Processing Unit (GPU) , originally designed for accelerating graphics manipulation and rendering in video games and other graphical applications, has become a fundamental component in the development of artificial intelligence . GPUs, as first popularized by Nvidia in 1999 with the GeForce 256 , evolved beyond their initial purpose to handle more general computation, ushering in the era of General Purpose GPUs (GPGPUs) in 2007. GPUs also became a critical technology in the world of cryptocurrency mining , further linking them to advancements in AI . The release of Bitcoin in 2009 initially relied on CPUs for mining, but as Bitcoin mining grew more competitive, GPU mining emerged in 2010 as a more efficient solution due to the parallel processing capabilities of GPUs. This shift sparked a massive demand for GPUs, particularly from cryptocurrency miners, which significantly impacted GPU availability and prices between 2013 and 2017 . Bitcoin, one of the most popular cryptocurrency The high demand for GPUs in cryptocurrency mining underscored their value not only for rendering graphics but also for performing complex computations , which are essential for both cryptographic hashing in blockchain networks. The adoption of GPUs for Bitcoin mining further spurred advancements in GPU technology, as manufacturers like Nvidia and AMD continued to push the boundaries of GPU performance to meet the growing needs of miners and AI researchers alike. As a result, the development of more powerful GPUs has directly fueled the acceleration of AI research and cryptocurrency mining , linking these two seemingly distinct fields through their shared reliance on parallel processing power. Their ability to handle massive parallel processing tasks, executing thousands of calculations simultaneously, made them ideally suited for the demanding workloads of machine learning and neural network training. GPU (Graphics Processing Unit) For AI development, this evolution was critical. Unlike CPUs , which are optimized for sequential processing, GPUs excel at performing repetitive, parallel tasks, such as those found in deep learning algorithms. Neural networks, which rely on processing vast amounts of data across multiple layers, benefit from the immense parallelism that GPUs offer, leading to significant speed improvements in both training and inference. This advancement allowed researchers to tackle more complex AI problems and opened the door to innovations in fields such as computer vision , natural language processing , and autonomous systems . 2010 — The CSIRO GPU cluster In summary, the GPU’s evolution from a specialized tool for rendering graphics to a key driver of AI development and cryptocurrency mining highlights its central role in modern computing. Without GPUs, the rapid advances in AI, particularly in machine learning and deep learning , would not have been possible, nor would the boom in cryptocurrency mining that has reshaped digital economies worldwide. 2011 — IBM Watson Wins Jeopardy! In February 2011 , IBM’s Watson made history by defeating the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings , in a highly-publicized televised event. This victory showcased the immense potential of AI in natural language processing and question-answering systems. Watson stage replica in Jeopardy! contest, Mountain View, California Developed by a team led by David Ferrucci , Watson was designed to push the boundaries of how machines could understand and interact with human language. Unlike traditional search engines, which rely on keyword parsing, Watson used IBM’s DeepQA software to generate hypotheses, analyze data, and provide answers to questions posed in natural language . Watson didn’t simply search the internet for answers — it analyzed vast amounts of stored information, including encyclopedias, dictionaries, religious texts, and books, to form its responses. At its core, Watson ran on 10 racks of 90 servers with 2,880 processor cores , performing millions of calculations in just three seconds to determine the best answer with a high degree of confidence. Its ability to understand and process unstructured data represented a significant leap in AI, particularly in natural language processing (NLP) . Watson’s Jeopardy! victory was more than just a game — it was a milestone in AI’s development, demonstrating that machines could handle complex language tasks and respond accurately to real-world questions. This success paved the way for AI’s application in industries ranging from healthcare to customer service , where Watson’s technology continues to enhance decision-making and problem-solving. 2011 — Siri was born: AI Enters the Mainstream In October 2011 , Apple introduced Siri , an intelligent virtual assistant integrated into the iPhone 4S , marking the first widely available AI-powered assistant on a major smartphone. Siri used speech recognition and artificial intelligence to perform a range of tasks, such as answering questions, performing calculations, playing music, sending messages, setting reminders, and providing navigation. Siri’s introduction brought AI assistants into mainstream consumer technology, setting the stage for future developments in digital assistants across mobile devices. Siri’s intuitive voice interface revolutionized how users interact with their smartphones, making AI-powered assistants a core feature of daily life. iPhone 4S unboxing 17–10–11 | Brett Jordan | Flickr 2012 — AlexNet and the Image Recognition Breakthrough In 2012 , a major milestone in image recognition was achieved with the success of AlexNet, a deep convolutional neural network (CNN) developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. AlexNet won the prestigious ImageNet competition , a benchmark for visual object recognition, by a significant margin. Its success highlighted the transformative potential of deep learning for image classification tasks, which involve recognizing and categorizing objects in images with remarkable accuracy. AlexNet’s architecture demonstrated the power of convolutional neural networks , which can automatically detect patterns and features in large image datasets. By significantly reducing error rates compared to previous methods, AlexNet marked a turning point in machine learning’s capability , proving that deep learning could outperform traditional techniques in complex tasks. This breakthrough laid the foundation for numerous applications in computer vision , ranging from facial recognition to autonomous driving, and propelled deep learning into the forefront of artificial intelligence research. 2014 — Facebook’s DeepFace and the Advancement of AI in Facial Recognition In 2014 , Facebook unveiled DeepFace , an advanced AI system designed to recognize human faces with over 97% accuracy . This groundbreaking achievement underscored the rapid advancements in facial recognition technology and highlighted AI’s expanding role in areas such as security , social networking , and biometric identification . DeepFace utilized deep learning techniques, specifically convolutional neural networks , to map and analyze facial structures in unprecedented detail. This capability not only enabled more accurate recognition of individuals across different settings and lighting conditions but also sparked broader discussions about privacy and the ethical implications of AI in everyday applications. DeepFace’s success demonstrated the immense potential of AI in transforming how we interact with technology, making it a key milestone in the integration of AI-driven biometric systems into digital platforms. 2014 — The Introduction of Generative Adversarial Networks (GANs) In 2014 , Ian Goodfellow introduced Generative Adversarial Networks (GANs) , a revolutionary machine learning framework that transformed the field of AI. GANs consist of two neural networks — a generator and a discriminator — that compete against each other. The generator creates new data or content, while the discriminator evaluates it, determining whether the generated content is real or fake. This process continues until the generator produces data that is virtually indistinguishable from real-world data. Generative Adversarial Networks (GANs) GANs marked a significant breakthrough in generative AI , allowing for the creation of high-quality images, videos, and other content . This technology has been applied across various fields, from art and entertainment to synthetic data generation and medical imaging. The introduction of GANs demonstrated the creative potential of AI, enabling machines not just to analyze or classify data, but to generate new and realistic content , pushing the boundaries of what AI could achieve. 2014 — The First Chatbot to Pass the Turing Test: Eugene Goostman The Turing Test , proposed by Alan Turing in 1950 , is a test designed to determine whether a machine can exhibit intelligent behavior indistinguishable from that of a human. In this test, a human evaluator engages in a conversation with both a human and a machine, and if the evaluator cannot reliably distinguish between the two, the machine is considered to have passed the test. Eugene Goostman Chatbot In 2014 , a chatbot named Eugene Goostman became the first artificial intelligence to appear to pass the Turing Test during an event held at the University of Reading . Created by Vladimir Veselov , Eugene Demchenko , and Sergey Ulasen , Eugene Goostman was portrayed as a thirteen-year-old boy from Odesa, Ukraine , with an intentionally chosen backstory that explained minor grammatical errors in its responses. Goostman interacted with judges via text-based conversations, and in the event, 33% of the judges were convinced that they were speaking with a human, leading the event’s organizer, Kevin Warwick , to declare that the chatbot had successfully passed the Turing Test. Eugene Goostman’s success in this context represents a milestone in the development of natural language processing and conversational AI. While the achievement sparked debate about whether it truly met the rigorous standards of the Turing Test, it demonstrated the potential of chatbots to emulate human-like conversation, advancing the field of artificial intelligence and its applications in human-machine interactions. 2015 — Amazon Echo and Alexa: The Rise of Voice-Controlled AI In 2015 , Amazon launched the Echo , a smart speaker powered by Alexa, a voice-controlled AI assistant. Alexa quickly became a household name due to its ability to perform a wide range of tasks, from answering questions and playing music to controlling smart home devices and facilitating online shopping. By using natural language processing and speech recognition, Alexa allowed users to interact with technology in a more intuitive, hands-free manner. The introduction of Amazon Echo and Alexa marked a major advancement in voice-controlled AI , demonstrating the practical application of AI in everyday life. Alexa’s integration into homes around the world underscored the growing potential of AI-driven personal assistants and set the stage for the development of smart homes and connected ecosystems , where AI seamlessly interacts with a variety of devices to enhance convenience and productivity. 2016 - AlphaGo’s Triumph Over Lee Sedol: A Landmark Moment for AI Go, a complex and ancient board game, had long been regarded as one of the greatest challenges for AI due to its immense complexity and the requirement for intuition, creativity, and strategic thinking. Unlike chess, where brute-force computation could be leveraged, Go’s vast number of possible moves made it exponentially harder for AI systems to solve. Many believed that mastering Go would require elements of AI that more closely mimic human thought processes. In 2016 , DeepMind’s AlphaGo made history by defeating Lee Sedol , one of the world’s top Go players, marking a monumental achievement in artificial intelligence. Google DeepMind’s AlphaGo project AlphaGo’s victory was a testament to the power of deep learning and reinforcement learning , which represented a significant departure from previous AI approaches. Instead of relying on pre-programmed probability algorithms, AlphaGo used neural networks to evaluate the state of the game and predict its probability of winning. It was trained on a massive library of historical Go matches and literature, but what made AlphaGo truly remarkable was its ability to play games against itself, continuously learning and refining its strategies with each iteration. By doing so, AlphaGo became independent of its human developers, capable of learning and improving beyond the limits of its initial programming. AlphaGo’s success was powered by a combination of neural networks and Monte Carlo tree search , allowing it to calculate vast numbers of possible moves — both likely and unlikely — far into the future. This ability to anticipate numerous potential scenarios, along with its capacity for self-improvement, gave AlphaGo an edge that no AI had ever demonstrated in such a complex and strategically rich game. The match, which carried a $1 million prize, was not only a competition but also a symbolic moment for AI’s progress. Following AlphaGo’s victory, Google DeepMind announced that the prize money would be donated to charity, further underscoring the significance of the event beyond the gaming world. Fan Hui vs AlphaGo — Game 5 AlphaGo’s triumph over Lee Sedol highlighted the future potential of AI, showing that with the right combination of deep learning and self-learning mechanisms, machines could tackle problems that were previously thought to be beyond their capabilities. This victory ushered in a new era for AI research , opening the door to solving real-world problems in fields such as healthcare, logistics, and scientific discovery by leveraging similar technologies. 2016 — Tay (Chatbot): A Cautionary Tale in AI Development Tay , a chatbot developed by Microsoft , was launched on March 23, 2016 , as a Twitter bot designed to interact with users and learn from its conversations. However, Tay quickly became controversial when it started posting inflammatory, offensive, and inappropriate tweets. These responses were a direct result of the bot learning from its interactions with users, many of whom deliberately manipulated Tay by feeding it harmful language. Within just 16 hours of its launch, Microsoft was forced to shut down the service. Tay bot logo The Tay incident underscored the vulnerabilities of AI systems that rely on machine learning from unfiltered user interactions. While Tay was designed to adapt and respond based on the conversations it engaged in, it became a victim of malicious trolling , highlighting how AI can be manipulated when safeguards are not in place. This failure illustrated the complexities and challenges of natural language processing and machine learning in open, unregulated environments like social media. In response, Microsoft later replaced Tay with a new chatbot, Zo , which incorporated stricter controls to avoid the pitfalls of its predecessor. The Tay controversy served as a critical lesson in AI ethics , showing how quickly an AI system can spiral out of control in the absence of content moderation and responsible design, further emphasizing the need for more robust, ethical frameworks in AI development. 2017 — “Attention Is All You Need” and the Transformer Breakthrough In 2017 , one of the most significant advancements in AI occurred with the publication of the paper “Attention Is All You Need” by Vaswani et al. , introducing the Transformer model . This model revolutionized natural language processing (NLP) by departing from previous architectures that relied on sequential word processing. Instead, the Transformer leveraged an attention mechanism that allowed it to process words in parallel , vastly improving the efficiency of handling large texts and complex language tasks. Transformer Architecture The title of the paper, “Attention Is All You Need,” is a playful nod to the Beatles’ famous song “All You Need Is Love.” The term “Transformer” was chosen for the model simply because the author Uszkoreit liked how the word sounded. An early design document was humorously titled “ Transformers: Iterative Self-Attention and Processing for Various Tasks ,” featuring an illustration of characters from the Transformers animated series, and the team behind it was referred to as Team Transformer. In their initial experiments, the team applied the Transformer architecture to tasks like English-to-German translation, generating Wikipedia articles about the Transformer itself, and parsing text. These trials demonstrated that the Transformer was not only highly effective for translation but also a general-purpose language model , capable of excelling in a wide range of natural language processing tasks. This versatility was a key factor in the model’s broader adoption and its revolutionary impact on AI. The Transformer model became a cornerstone of modern AI, enabling the development of large-scale models such as GPT-3 and BERT . Its architecture excelled in tasks like machine translation , text generation , and question answering , setting new benchmarks for performance in NLP. The introduction of Transformers marked a new era in AI, empowering models that could understand and generate human language with remarkable accuracy and fluency, making it foundational for many state-of-the-art AI applications today. Original article: Attention Is All You Need 2018 — AI in Universities: The Rise of Specialized Education By 2018 , universities worldwide began integrating specialized AI courses into their curricula, reflecting the increasing significance and demand for expertise in this rapidly evolving field. The surge in AI’s practical applications, from autonomous vehicles to healthcare innovations, prompted academic institutions to offer dedicated programs in machine learning , deep learning , natural language processing , and more. This shift marked a growing recognition of AI as a crucial area of study, preparing a new generation of students to contribute to and shape the future of artificial intelligence across industries. The rise of AI-focused education demonstrated the technology’s transformative potential and the need for skilled professionals to meet the demands of the AI-driven economy. 2020 — OpenAI’s GPT-3: A Landmark in Language AI In 2020 , OpenAI released GPT-3 (Generative Pre-trained Transformer 3) , one of the largest and most powerful language models ever created. With 175 billion parameters , GPT-3 showcased an unprecedented ability to generate human-like text and perform a wide range of tasks such as translation, summarization , content creation, and answering complex questions . What set GPT-3 apart was its capacity to understand and generate coherent, contextually appropriate responses with minimal input, making it a game-changer in natural language processing (NLP) . It demonstrated the potential of AI to handle diverse language-related tasks without needing task-specific training. GPT Architecture This flexibility and versatility revolutionized industries from customer service and content creation to education and research, setting a new standard for what AI could achieve in text-based applications. GPT-3’s capabilities marked a significant leap forward in the field of AI, unlocking new possibilities for machine-human interaction. 2022 — Midjourney and Generative AI in Art Midjourney represents a pivotal point in the intersection of art and generative AI , offering a platform for creating images from text-based prompts. As an independent research lab, Midjourney has developed an AI-powered image generation software that allows users to input prompts — short textual descriptions — and receive stunning, imaginative visuals in response. This innovative tool enables users to generate detailed and highly creative artwork by simply describing their vision in words. Oriental Lovers- One of the images that I generated with Midjourney — All rights reserved Midjourney focuses on using AI to enhance human creativity , expanding the boundaries of what can be achieved in the artistic process. By transforming text prompts into visually rich images, it empowers both artists and individuals to collaborate with AI, exploring new ways to visualize abstract ideas and thoughts. Whether it’s for conceptual art, design, or creative experimentation, Midjourney’s approach pushes the limits of what AI can accomplish in the world of visual expression, making art creation more accessible and inspiring. Humans - One of the images that I generated with Midjourney — All rights reserved Similarly, generative AI technologies like Stable Diffusion and DALL·E have transformed how art is created. Stable Diffusion is a cutting-edge text-to-image diffusion model capable of generating photorealistic images from simple text inputs. This AI allows users to create stunning, highly detailed images in seconds, democratizing the creation process by enabling anyone to bring their artistic visions to life. Stable Diffusion fosters artistic freedom and creativity, making art creation more accessible to millions worldwide. Stable diffusion — two red guards smoking in the street — Wikipedia DALL·E , developed by OpenAI , is another breakthrough in generative AI , allowing users to generate highly realistic images and art based on natural language descriptions. Whether it’s an “astronaut riding a horse” or a more abstract concept, DALL·E showcases the transformative potential of AI in artistic creativity . By interpreting text prompts and converting them into intricate visual outputs, DALL·E has opened new pathways for how art is conceptualized and produced. “A photo of an astronaut riding a horse” — One of the first and the most famous image generated by Dall-E These generative AI models, including Midjourney , Stable Diffusion , and DALL·E , mark a new era in artistic innovation, where the line between human and machine creativity blurs, enabling endless possibilities in art and design . This AI-driven creativity revolutionizes traditional artistic processes, allowing users to collaborate with machines to imagine and generate art in ways that were once impossible. GPT-4: The Next Leap in AI Language Models Released in 2023 by OpenAI , GPT-4 represents a significant advancement over its predecessor, GPT-3. With an even larger and more refined architecture, GPT-4 boasts enhanced capabilities in natural language processing , reasoning , and problem-solving . It can handle more nuanced and complex tasks, including multi-step reasoning , generating highly coherent and contextually relevant text, and demonstrating improved performance in areas like code generation , creative writing , and dialogue systems . GPT-4 also places a stronger emphasis on safety and alignment, with built-in mechanisms to reduce harmful outputs and provide more accurate, responsible answers. Its enhanced multimodal capabilities allow it to process and generate both text and images , opening new possibilities for AI applications in fields such as education , research , and content creation . GPT-4 continues to push the boundaries of what large language models (LLMs) can achieve, setting new benchmarks for AI performance and utility. Turing Test The chatbot quickly gained attention for its ability to generate remarkably human-like responses, sparking discussions about its potential to pass the Turing Test . In a Nature article, Celeste Biever remarked that “ChatGPT broke the Turing test,” highlighting its conversational sophistication. Furthermore, Stanford researchers conducted studies and reported that ChatGPT-4 successfully passed a rigorous Turing test. They noted that the chatbot’s behavior closely mimicked that of a human, with its primary divergence being an increased tendency to be more cooperative than the average human. These findings underscored ChatGPT’s remarkable ability to engage in natural language conversations, marking a significant leap in the development of conversational AI. Other Notable Large Language Models (LLMs) In addition to GPT-3 , several other large language models (LLMs) have emerged, each contributing uniquely to the landscape of AI-driven natural language processing (NLP) . These models, developed by a variety of organizations, reflect the increasing diversity and innovation in the field, offering a range of capabilities and uses. Gemini (Google DeepMind) Developed by Google DeepMind , Gemini is an advanced LLM designed to rival models like GPT-3 and GPT-4. Gemini combines traditional natural language understanding with reinforcement learning capabilities, enabling the model to perform complex reasoning tasks and adapt to real-world environments. It is specifically designed to interact with AI agents and applications, making it highly versatile for tasks such as coding, question answering, and language translation. Mistral Mistral is a powerful language model known for its efficiency and performance, even with fewer parameters compared to other models. This model has been designed to handle large-scale text generation and comprehension tasks. Mistral’s architecture focuses on making LLMs more resource-efficient without compromising on the quality of their outputs. It has become a popular choice for enterprises and researchers looking for a high-performing model that can run on more accessible hardware. See also my article: Claude (Anthropic) Developed by Anthropic , Claude is another competitor in the LLM space, emphasizing safety and ethical AI usage. Named after Claude Shannon, the father of information theory, this model is designed with a focus on AI alignment — ensuring that the model’s behavior aligns with human intentions and values. Claude is capable of handling complex tasks like summarization, question answering, and dialogue, and is known for its emphasis on user safety and interpretability. LLaMA (Meta) LLaMA (Large Language Model Meta AI) is an open-source LLM developed by Meta (formerly Facebook). It was designed to democratize access to high-performing AI models by offering a more accessible and customizable alternative to proprietary LLMs like GPT-3. LLaMA’s open-source nature has made it popular among researchers and developers looking to experiment with and build upon existing LLM frameworks. By providing a transparent, scalable model, LLaMA has contributed to the advancement of AI research, encouraging collaborative development in the broader AI community. Grok: Frontier Language Model on 𝕏 Grok-2 is a cutting-edge language model that pushes the boundaries of reasoning capabilities in AI. Developed as part of the Grok family , it is designed to excel in advanced natural language processing tasks with state-of-the-art accuracy and problem-solving skills. The model comes in two versions: Grok-2 and Grok-2 mini , both of which are now available to users on the 𝕏 platform . Grok’s release highlights a new chapter in AI, offering users powerful tools for tasks that demand complex reasoning, text generation, and intelligent interactions, further expanding the reach and application of large language models (LLMs) in everyday digital platforms. 2024 — OpenAI O1 Preview OpenAI’s new O1-preview models are a step forward in creating AI that takes more time to elaborate before responding, significantly enhancing its reasoning abilities. These models are particularly adept in fields like science, coding, and mathematics, where complex problem-solving is crucial. Looking Ahead: The Future of AI Agentic AI and Chain of Thought (CoT) Prompting As we look toward the future of artificial intelligence , two pivotal concepts stand out: Agentic AI and Chain of Thought (CoT) prompting . These advancements represent the next frontier in AI, where machines are not just reactive tools but intelligent agents capable of autonomous reasoning and decision-making. Agentic AI — All rights reserved Agentic AI refers to AI systems that operate with a higher degree of autonomy, functioning as intelligent agents capable of understanding context, making decisions, and executing tasks independently. Unlike traditional AI, which is typically task-specific and reactive, Agentic AI has the potential to act with purpose and self-directed reasoning, analyzing complex environments and acting in ways that mimic human-like decision-making. This kind of AI will fundamentally change how we interact with machines, enabling AI to handle tasks that require dynamic thinking and adaptability in real-world situations. At the core of Agentic AI’s evolution is Chain of Thought (CoT) prompting . This approach allows AI to engage in step-by-step reasoning, mimicking how humans solve complex problems by breaking them down into smaller, more manageable parts. CoT prompting enables AI to explain its thought process and walk through multi-step decisions, leading to more accurate and reliable outcomes. As AI becomes more sophisticated, CoT will be crucial in guiding machines to handle multi-faceted challenges in areas such as scientific research , healthcare , and autonomous systems . Together, Agentic AI and CoT prompting will pave the way for a future where AI is not just a tool but a partner in innovation , capable of understanding and solving complex, open-ended problems with greater autonomy and intelligence. These advancements will shape industries, transform economies, and redefine the boundaries of human-machine collaboration in the years to come. The Future of AI: Realistic Audio and Video Avatars with Tone and Emotion As AI continues to evolve, one of the most exciting developments is the creation of highly realistic audio and video avatars capable of expressing nuanced tone and emotions . These avatars, powered by advanced machine learning models, are becoming more lifelike than ever, bringing human-like interaction to digital spaces in unprecedented ways. In the near future, AI-generated avatars will be able to mimic human facial expressions, body language, and voice modulation with extraordinary accuracy. They will respond to conversations with appropriate emotional tones, whether it’s excitement, empathy, or subtle humor. This evolution will not only enhance virtual interactions but also improve user experiences in customer service, telehealth, education, and entertainment. AI Avatar - All rights reserved These avatars will use deep learning techniques to process real-time inputs , allowing them to adapt their emotional responses to the context of a conversation or situation. With realistic voice synthesis and facial animations , AI will bridge the gap between human and digital communication, making interactions more immersive, personalized, and emotionally engaging than ever before. As these technologies advance, the potential for realistic and hyper-realistic AI-driven avatars will redefine digital spaces, creating a future where virtual assistants, educators, and even entertainment characters can engage with people in a way that feels genuinely human. AI in Video and Cinema: Generating Entire Commercials, Movies, and TV Shows The future of video production and cinema is rapidly being transformed by advancements in AI , where the technology is becoming capable of generating entire commercials , movies , TV shows , and other forms of media. This shift is set to revolutionize the entertainment industry, allowing for unprecedented creativity, efficiency, and customization in content creation. AI will be capable of writing scripts , creating storylines , directing scenes , and even producing visual effects with minimal human intervention. By leveraging generative AI models, filmmakers will design characters, settings, and plotlines that evolve dynamically based on input prompts. These AI systems will autonomously create realistic animations, camera angles, and dialogue, reducing the need for large crews and extensive resources traditionally required for production. Additionally, AI-generated actors and digital avatars will become commonplace, allowing filmmakers to recreate historical figures, invent entirely new characters, or bring fictional beings to life in a realistic manner. These AI-driven characters will be able to perform with human-like emotional depth, tailored precisely to the director’s vision, offering a new dimension of creative freedom. For advertising and commercials , AI can generate complete video spots based on brand guidelines, producing tailored content faster and at a fraction of the cost. AI-generated commercials can be personalized for different audiences, offering unique, real-time adaptations based on viewer preferences and data, making marketing more targeted and engaging. This AI-driven cinema could democratize filmmaking, empowering creators of all levels to produce high-quality content with fewer resources. The ability of AI to autonomously generate entire films or TV shows — down to the smallest details — has the potential to disrupt traditional content production, creating a future where AI is both a tool and a collaborator in the world of storytelling and entertainment. The Last Advancement: Artificial General Intelligence (AGI) and Beyond Artificial General Intelligence (AGI) , often called strong AI , represents one of the most ambitious goals in the field of artificial intelligence: the creation of a system capable of understanding, learning, and applying knowledge across a wide range of tasks at a level comparable to human intelligence. Unlike current narrow AI , which is designed for specific applications, AGI would possess the ability to reason , adapt , and perform any cognitive task that a human can, across various domains. However, with the rapid advancements in machine learning and deep learning , systems like ChatGPT and other large language models are already nearing some characteristics of AGI . These models can engage in complex conversations , demonstrate problem-solving abilities, and even generate creative content. While they do not yet exhibit the full range of human-like understanding or general intelligence, their capabilities represent a significant step toward AGI. Many researchers now believe that systems like these, with further refinement and development, could bridge the gap to true AGI within the next few years . The Leap to Superintelligence While AGI represents human-level intelligence, superintelligence goes a step further — referring to an AI that not only matches but exceeds human intelligence in all aspects, including creativity, decision-making, and problem-solving. Superintelligence would possess cognitive abilities far beyond the brightest human minds, enabling it to solve problems and make decisions in ways that humans could not even begin to comprehend. The potential transition from AGI to superintelligence raises significant concerns about control and safety . Superintelligence, with its capacity to operate independently and improve upon itself, could lead to an intelligence explosion, where its capabilities grow exponentially. The risk here lies in the possibility that humans may lose control of such an entity, as its goals and decision-making processes could become unexplainable , unpredictable , and uncontrollable . This presents a stark contrast to AGI, which aims to mirror human intelligence, while superintelligence would operate on a completely different, far superior level. AGI — All rights reserved The End The history of artificial intelligence is a testament to humanity’s relentless pursuit of understanding and replicating the essence of intelligence. From mechanical calculators to neural networks and beyond, each milestone reflects a deeper grasp of both technology and ourselves. As AI becomes increasingly integrated into daily life, shaping industries, economies, and cultures, it prompts profound questions about consciousness, ethics, and the future of human-machine coexistence. The journey of AI is far from over: it’s an ever-unfolding narrative that challenges us to imagine and construct the possibilities of tomorrow.",
    "score": 0.46769970655441284,
    "source_type": "exa",
    "metadata": {
      "summary": "This article traces the history of artificial intelligence (AI) from its philosophical roots to its modern applications.  It begins with Pythagoras (6th century BC), whose emphasis on numbers as the fundamental essence of the universe foreshadowed AI's reliance on numerical data representation.  Plato's Theory of Forms (4th century BC) contributed to the concept of an ideal intellectual reality, a precursor to the goal of creating intelligent systems.  The article further suggests that these early philosophical concepts laid the groundwork for the development of modern AI, which uses numerical models to process and understand information.\n",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  },
  {
    "title": "i2k Web | Login",
    "url": "https://aitopics.org/i2kweb/login",
    "content": "This webpage is a login page for a system called \"i2k Web,\" and does not contain information on the history of artificial intelligence development.  The provided text only shows website navigation elements and a link to a seemingly related site, i2kconnect.com.  To find information about the history of AI development, please search on a different website.\n",
    "raw_content": "",
    "score": 0.0,
    "source_type": "exa",
    "metadata": {
      "summary": "This webpage is a login page for a system called \"i2k Web,\" and does not contain information on the history of artificial intelligence development.  The provided text only shows website navigation elements and a link to a seemingly related site, i2kconnect.com.  To find information about the history of AI development, please search on a different website.\n",
      "is_subpage": true,
      "parent_url": "https://aitopics.org/misc/brief-history",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  },
  {
    "title": "AITopics",
    "url": "https://aitopics.org/search",
    "content": "This webpage, AITopics, doesn't offer a history of artificial intelligence development.  While it defines AI as \"the scientific understanding of the mechanisms underlying thought and intelligent behavior and their embodiment in machines\" (per the Association for the Advancement of Artificial Intelligence), the page primarily features news articles from July 2nd, 2024,  on unrelated topics such as LinkedIn's chatbot, a Microsoft Surface Laptop review, and the war in Ukraine.  To find information on the history of AI development, you will need to consult other resources.\n\n\nInformation about AI from the News, Publications, and Conferences Automatic Classification – Tagging and Summarization – Customizable Filtering and Analysis If you are looking for an answer to the question What is Artificial Intelligence? and you only have a minute, then here's the definition the Association for the Advancement of Artificial Intelligence offers on its home page: \"the scientific understanding of the mechanisms underlying thought and intelligent behavior and their embodiment in machines.\" However, if you are fortunate enough to have more than a minute, then please get ready to embark upon an exciting journey exploring AI (but beware, it could last a lifetime) … LinkedIn Has Answers to Questions You've Never Had Slate Jul-2-2024, 14:30:00 GMT \"What does a teacher do?\" \"What does a barber do?\" \"What are recent developments in Swiftonomics?\" I pondered these questions only after LinkedIn prompted me to do so. Suddenly, I found myself contemplating the very essence of my own reality. How did I learn what I know? How does my hair go from long to short every five weeks? Microsoft Surface Laptop 7 review: Success at last Engadget Jul-2-2024, 12:00:56 GMT Ever since Apple's MacBooks switched to the company's homegrown M-series chips, Windows users have wondered when a similar revolution would happen to their machines. To Microsoft's credit, it hasn't been for a lack of trying. Way back in 2012, the company released the Surface RT with an Arm-based processor, which is the same architecture used in Apple's silicon. Unfortunately, a tiny app library, sluggish performance and limited software compatibility made using one full-time kind of frustrating. Then in 2017, Microsoft renewed its efforts with Windows on Snapdragon. This led to systems like the Surface Pro X, which sported gorgeous hardware that was once again marred by lackluster processing power and spotty software support. Russian drone, hypersonic missile strikes escalate on Ukrainian air base ahead of arrival of F-16s FOX News Jul-2-2024, 11:51:26 GMT NATO Secretary General Jens Stoltenberg to discuss how NATO members have bolstered their spending, the latest on the war in Ukraine and how the alliance can counter aggression from countries like Russia and China. Explosions reverberated across the pre-dawn sky as Ukrainian air defenses fended off a Russian attack on this small city in western Ukraine, home to an important air base and a frequent target of Moscow's strikes. Hours after the assault, the tidy streets of Starokostiantyniv had returned to a semblance of normality. But the June 27 attack was a stark reminder of the challenges Kyiv faces as it rebuilds its depleted air force and deploys the first U.S.-designed F-16s - fighter aircraft that Russia will be determined to ground or destroy. The first planes are expected to arrive this month, and Ukraine hopes they will boost forces struggling to repel a Russian onslaught along the front line, which includes devastating glide bombs that F-16s could potentially disrupt. The consequences of making a nonconsensual deepfake Mashable Jul-2-2024, 10:10:22 GMT Lawyer Sean Smith has seen up close how nonconsensual deepfakes, a form of image-based sexual abuse, can ruin lives. Smith, a family law attorney with the Roseland, New Jersey, firm Brach Eichler, recently represented both the families of minor victims and perpetrators throughout educational disciplinary proceedings. His clients have included teen girls whose images were taken from social media, then digitally \"undressed\" by their male classmates, who used software powered by artificial intelligence. The apps and websites capable of creating explicit nonconsensual deepfakes typically market themselves as satisfying a curiosity or providing entertainment. As a result, users likely don't understand that the resulting imagery can inflict painful, lifelong trauma on the person whose likeness has been stolen -- who is almost always a girl or woman. Interview with Yuan Yang: working at the intersection of AI and cognitive science AIHub Jul-2-2024, 08:50:33 GMT In this interview series, we're meeting some of the AAAI/SIGAI Doctoral Consortium participants to find out more about their research. The Doctoral Consortium provides an opportunity for a group of PhD students to discuss and explore their research interests and career objectives in an interdisciplinary workshop together with a panel of established researchers. In this latest interview, we hear from Yuan Yang, who completed his PhD in May. This autumn, Yuan will be joining the College of Information, Mechanical and Electrical Engineering, Shanghai Normal University as an associate professor. From August 2018 to May 2024, I did my PhD in the computer science department at Vanderbilt University, which is located in the famous music city – Nashville, Tennessee. Japan's Defense Ministry unveils first basic policy on use of AI The Japan Times Jul-2-2024, 08:15:00 GMT The Defense Ministry unveiled its first basic policy on the use of artificial intelligence on Tuesday, as Japan looks to stave off a manpower shortage and keep pace with China and the United States on the technology's military applications. The focus on AI comes as the Self-Defense Forces grapple with concerns about recruitment and its abilities to harness the power of new technologies. \"In our country, where the population is rapidly declining and aging, it is essential to utilize personnel more efficiently than ever before,\" Defense Minister Minoru Kihara told a news conference after the policy's release. \"We believe that AI has the potential to be one of the technologies that can overcome these challenges.\" AI can predict how monkeys play Pac-Man New Scientist Jul-2-2024, 08:00:39 GMT An artificial intelligence can accurately predict how a monkey plays the Pac-Man video game and mimic the animals' eye movements when they do this. Tianming Yang at the Chinese Academy of Sciences and his colleagues trained two rhesus monkeys to play Pac-Man by rewarding them with juice for collecting all the dots in a maze and evading capture by ghosts. I simulated each UK party's first years in government in a video game, and the results were awful The Guardian Jul-2-2024, 06:00:38 GMT Whether they are called manifestos or contracts, the documents published by political parties ahead of an election are rather less substantial than their many pages would suggest. They are full of best-case scenarios, undetailed proposals and dubious costings, and it is hard to picture the impact each party would have on the UK if they followed through with their pitches. So I've been feeding party literature into the political strategy video game Democracy 4, to see how these policies might play out. The results were … well, you'll see. Democracy 4 lets you play out your political fantasies (or nightmares) to see the impact of your choices and, ultimately, if you can get re-elected.",
    "raw_content": "Information about AI from the News, Publications, and Conferences Automatic Classification – Tagging and Summarization – Customizable Filtering and Analysis If you are looking for an answer to the question What is Artificial Intelligence? and you only have a minute, then here's the definition the Association for the Advancement of Artificial Intelligence offers on its home page: \"the scientific understanding of the mechanisms underlying thought and intelligent behavior and their embodiment in machines.\" However, if you are fortunate enough to have more than a minute, then please get ready to embark upon an exciting journey exploring AI (but beware, it could last a lifetime) … LinkedIn Has Answers to Questions You've Never Had Slate Jul-2-2024, 14:30:00 GMT \"What does a teacher do?\" \"What does a barber do?\" \"What are recent developments in Swiftonomics?\" I pondered these questions only after LinkedIn prompted me to do so. Suddenly, I found myself contemplating the very essence of my own reality. How did I learn what I know? How does my hair go from long to short every five weeks? Microsoft Surface Laptop 7 review: Success at last Engadget Jul-2-2024, 12:00:56 GMT Ever since Apple's MacBooks switched to the company's homegrown M-series chips, Windows users have wondered when a similar revolution would happen to their machines. To Microsoft's credit, it hasn't been for a lack of trying. Way back in 2012, the company released the Surface RT with an Arm-based processor, which is the same architecture used in Apple's silicon. Unfortunately, a tiny app library, sluggish performance and limited software compatibility made using one full-time kind of frustrating. Then in 2017, Microsoft renewed its efforts with Windows on Snapdragon. This led to systems like the Surface Pro X, which sported gorgeous hardware that was once again marred by lackluster processing power and spotty software support. Russian drone, hypersonic missile strikes escalate on Ukrainian air base ahead of arrival of F-16s FOX News Jul-2-2024, 11:51:26 GMT NATO Secretary General Jens Stoltenberg to discuss how NATO members have bolstered their spending, the latest on the war in Ukraine and how the alliance can counter aggression from countries like Russia and China. Explosions reverberated across the pre-dawn sky as Ukrainian air defenses fended off a Russian attack on this small city in western Ukraine, home to an important air base and a frequent target of Moscow's strikes. Hours after the assault, the tidy streets of Starokostiantyniv had returned to a semblance of normality. But the June 27 attack was a stark reminder of the challenges Kyiv faces as it rebuilds its depleted air force and deploys the first U.S.-designed F-16s - fighter aircraft that Russia will be determined to ground or destroy. The first planes are expected to arrive this month, and Ukraine hopes they will boost forces struggling to repel a Russian onslaught along the front line, which includes devastating glide bombs that F-16s could potentially disrupt. The consequences of making a nonconsensual deepfake Mashable Jul-2-2024, 10:10:22 GMT Lawyer Sean Smith has seen up close how nonconsensual deepfakes, a form of image-based sexual abuse, can ruin lives. Smith, a family law attorney with the Roseland, New Jersey, firm Brach Eichler, recently represented both the families of minor victims and perpetrators throughout educational disciplinary proceedings. His clients have included teen girls whose images were taken from social media, then digitally \"undressed\" by their male classmates, who used software powered by artificial intelligence. The apps and websites capable of creating explicit nonconsensual deepfakes typically market themselves as satisfying a curiosity or providing entertainment. As a result, users likely don't understand that the resulting imagery can inflict painful, lifelong trauma on the person whose likeness has been stolen -- who is almost always a girl or woman. Interview with Yuan Yang: working at the intersection of AI and cognitive science AIHub Jul-2-2024, 08:50:33 GMT In this interview series, we're meeting some of the AAAI/SIGAI Doctoral Consortium participants to find out more about their research. The Doctoral Consortium provides an opportunity for a group of PhD students to discuss and explore their research interests and career objectives in an interdisciplinary workshop together with a panel of established researchers. In this latest interview, we hear from Yuan Yang, who completed his PhD in May. This autumn, Yuan will be joining the College of Information, Mechanical and Electrical Engineering, Shanghai Normal University as an associate professor. From August 2018 to May 2024, I did my PhD in the computer science department at Vanderbilt University, which is located in the famous music city – Nashville, Tennessee. Japan's Defense Ministry unveils first basic policy on use of AI The Japan Times Jul-2-2024, 08:15:00 GMT The Defense Ministry unveiled its first basic policy on the use of artificial intelligence on Tuesday, as Japan looks to stave off a manpower shortage and keep pace with China and the United States on the technology's military applications. The focus on AI comes as the Self-Defense Forces grapple with concerns about recruitment and its abilities to harness the power of new technologies. \"In our country, where the population is rapidly declining and aging, it is essential to utilize personnel more efficiently than ever before,\" Defense Minister Minoru Kihara told a news conference after the policy's release. \"We believe that AI has the potential to be one of the technologies that can overcome these challenges.\" AI can predict how monkeys play Pac-Man New Scientist Jul-2-2024, 08:00:39 GMT An artificial intelligence can accurately predict how a monkey plays the Pac-Man video game and mimic the animals' eye movements when they do this. Tianming Yang at the Chinese Academy of Sciences and his colleagues trained two rhesus monkeys to play Pac-Man by rewarding them with juice for collecting all the dots in a maze and evading capture by ghosts. I simulated each UK party's first years in government in a video game, and the results were awful The Guardian Jul-2-2024, 06:00:38 GMT Whether they are called manifestos or contracts, the documents published by political parties ahead of an election are rather less substantial than their many pages would suggest. They are full of best-case scenarios, undetailed proposals and dubious costings, and it is hard to picture the impact each party would have on the UK if they followed through with their pitches. So I've been feeding party literature into the political strategy video game Democracy 4, to see how these policies might play out. The results were … well, you'll see. Democracy 4 lets you play out your political fantasies (or nightmares) to see the impact of your choices and, ultimately, if you can get re-elected.",
    "score": 0.0,
    "source_type": "exa",
    "metadata": {
      "summary": "This webpage, AITopics, doesn't offer a history of artificial intelligence development.  While it defines AI as \"the scientific understanding of the mechanisms underlying thought and intelligent behavior and their embodiment in machines\" (per the Association for the Advancement of Artificial Intelligence), the page primarily features news articles from July 2nd, 2024,  on unrelated topics such as LinkedIn's chatbot, a Microsoft Surface Laptop review, and the war in Ukraine.  To find information on the history of AI development, you will need to consult other resources.\n",
      "is_subpage": true,
      "parent_url": "https://aitopics.org/misc/brief-history",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  },
  {
    "title": "i2k Web | About AITopics",
    "url": "https://aitopics.org/misc/about",
    "content": "AITopics, a project of the Association for the Advancement of Artificial Intelligence (AAAI) since 1998, is a large online collection of AI research, people, and applications.  Its NewsFinder component,  documented in  Eckroth (2020) and Eckroth & Schoen (2019), tracks AI news.  The current (2024) version uses large language models to cluster and summarize weekly news, providing a concise overview of key AI developments.  While the provided text focuses on the current methodology, it highlights the long history of AITopics and its evolution in tracking AI advancements.\n\n\nAITopics is the Internet's largest collection of information about the research, the people, and the applications of Artificial Intelligence. Our mission is to educate and inspire through a wide variety of curated and organized resources gathered from across the web.\n \n AITopics began in 1998 and is brought to you by The Association for the Advancement of Artificial Intelligence (AAAI) .\n \n AITopics is intended primarily for instructors of AI courses and students from high school through first-year graduate school so you can: \n \n Stay abreast of advances in AI \n Read the news stories capturing media attention \n Find background material and foundational classics that supplement textbook discussions \n View videos \n Listen to podcasts \n \n To submit new content, suggest changes or raise other issues, use the Site Feedback link at the bottom of any page. \n AITopics is powered by AI technology from i2k Connect . The new engine combines machine learning with subject matter expert knowledge to automatically tag documents with unique, accurate and consistent metadata. Building on the familiar online shopping experience, you can discover and analyze just the information you need. \n NewsFinder: AI-Alert \n Artificial intelligence is part of our global culture, and its impact on our everyday lives is growing. Searching for news about AI presents a deluge of information. Why not use a little AI of our own to better handle the task?\n NewsFinder scans high-quality online news sources every day and selects stories that it deems relevant to the Artificial Intelligence community. NewsFinder has undergone numerous changes over its decades of operation. Earlier versions are documented in the publications listed below. \n \nThe current version of NewsFinder (2024-) uses large language models to cluster and summarize stories, in order to give a clearer understanding of the major events and topics of the week's AI news.\nThis is the high-level workflow:\n \n Gather articles on AITopics from the last week (~180 of them), filter out articles with low AI classification score (~120 remain) \n Generate embeddings for the content (using OpenAI's embeddings) \n Cluster the articles, automatically adjusting the # of clusters \n Remove any clusters of articles that have more than 10 member articles (i.e., cluster is not specific) \n Use GPT-3.5 to generate an overall title &amp; summary of the articles in the cluster \n Enrich the generated title &amp; summary with classifications \n Remove any clusters that do not have good classifications from the prior step \n Use GPT-3.5 to generate a short table-of-contents title for each cluster \n Use GPT-3.5 to generate an image generation prompt \n Use DALL-E 3 to generate an image using the prompt from the prior step \n Use GPT-4 to generate a very succinct summary of the whole collection of clusters \n \nThe result is a weekly alert, for example this one from February 6, 2024 .\n \n Visit our AI-Alerts page or email info@i2kconnect.com to subscribe to the weekly email alert. \n Articles on AITopics and NewsFinder \n \n Eckroth, J. 2020. Evolution of a Robust Artificial Intelligence System: A Case Study of the Association for the Advancement of Artificial Intelligence's AI-Alert . AI Magazine 41(4): 17-38. \n Eckroth, J; and Schoen, E. 2019. A Genetic Algorithm for Finding a Small and Diverse Set of Recent News Stories on a Given Subject: How We Generate AAAI's AI-Alert . Proceedings of the AAAI Conference on Artificial Intelligence 33(1): 9357-9364. \n Buchanan, B.G.; Eckroth, J.; and Smith, R.G. 2013. A Virtual Archive for the History of AI . AI Magazine 34(2): 86-98. \n Eckroth, J.; Dong, L.; Smith, R.G.; and Buchanan, B.G. 2012. NewsFinder: Automating an Artificial Intelligence News Service . AI Magazine 33(2): 43-54. \n Buchanan, B.G.; Glick, J.; and Smith, R.G. 2008. The AAAI Video Archive . AI Magazine 29(1): 91-94. \n Buchanan, B.G.; and Glick, J. 2002. AI Topics: A Responsibility to Celebrate AI Responsibly . AI Magazine 23(1): 87-94. \n \n We gratefully acknowledge additional financial support received from the National Science Foundation (awards #0738341 and #1319941), the AI Journal Foundation of IJCAI, and Microsoft Research. \n \n – Bruce Buchanan , Reid Smith , Joshua Eckroth",
    "raw_content": "AITopics is the Internet's largest collection of information about the research, the people, and the applications of Artificial Intelligence. Our mission is to educate and inspire through a wide variety of curated and organized resources gathered from across the web.\n \n AITopics began in 1998 and is brought to you by The Association for the Advancement of Artificial Intelligence (AAAI) .\n \n AITopics is intended primarily for instructors of AI courses and students from high school through first-year graduate school so you can: \n \n Stay abreast of advances in AI \n Read the news stories capturing media attention \n Find background material and foundational classics that supplement textbook discussions \n View videos \n Listen to podcasts \n \n To submit new content, suggest changes or raise other issues, use the Site Feedback link at the bottom of any page. \n AITopics is powered by AI technology from i2k Connect . The new engine combines machine learning with subject matter expert knowledge to automatically tag documents with unique, accurate and consistent metadata. Building on the familiar online shopping experience, you can discover and analyze just the information you need. \n NewsFinder: AI-Alert \n Artificial intelligence is part of our global culture, and its impact on our everyday lives is growing. Searching for news about AI presents a deluge of information. Why not use a little AI of our own to better handle the task?\n NewsFinder scans high-quality online news sources every day and selects stories that it deems relevant to the Artificial Intelligence community. NewsFinder has undergone numerous changes over its decades of operation. Earlier versions are documented in the publications listed below. \n \nThe current version of NewsFinder (2024-) uses large language models to cluster and summarize stories, in order to give a clearer understanding of the major events and topics of the week's AI news.\nThis is the high-level workflow:\n \n Gather articles on AITopics from the last week (~180 of them), filter out articles with low AI classification score (~120 remain) \n Generate embeddings for the content (using OpenAI's embeddings) \n Cluster the articles, automatically adjusting the # of clusters \n Remove any clusters of articles that have more than 10 member articles (i.e., cluster is not specific) \n Use GPT-3.5 to generate an overall title &amp; summary of the articles in the cluster \n Enrich the generated title &amp; summary with classifications \n Remove any clusters that do not have good classifications from the prior step \n Use GPT-3.5 to generate a short table-of-contents title for each cluster \n Use GPT-3.5 to generate an image generation prompt \n Use DALL-E 3 to generate an image using the prompt from the prior step \n Use GPT-4 to generate a very succinct summary of the whole collection of clusters \n \nThe result is a weekly alert, for example this one from February 6, 2024 .\n \n Visit our AI-Alerts page or email info@i2kconnect.com to subscribe to the weekly email alert. \n Articles on AITopics and NewsFinder \n \n Eckroth, J. 2020. Evolution of a Robust Artificial Intelligence System: A Case Study of the Association for the Advancement of Artificial Intelligence's AI-Alert . AI Magazine 41(4): 17-38. \n Eckroth, J; and Schoen, E. 2019. A Genetic Algorithm for Finding a Small and Diverse Set of Recent News Stories on a Given Subject: How We Generate AAAI's AI-Alert . Proceedings of the AAAI Conference on Artificial Intelligence 33(1): 9357-9364. \n Buchanan, B.G.; Eckroth, J.; and Smith, R.G. 2013. A Virtual Archive for the History of AI . AI Magazine 34(2): 86-98. \n Eckroth, J.; Dong, L.; Smith, R.G.; and Buchanan, B.G. 2012. NewsFinder: Automating an Artificial Intelligence News Service . AI Magazine 33(2): 43-54. \n Buchanan, B.G.; Glick, J.; and Smith, R.G. 2008. The AAAI Video Archive . AI Magazine 29(1): 91-94. \n Buchanan, B.G.; and Glick, J. 2002. AI Topics: A Responsibility to Celebrate AI Responsibly . AI Magazine 23(1): 87-94. \n \n We gratefully acknowledge additional financial support received from the National Science Foundation (awards #0738341 and #1319941), the AI Journal Foundation of IJCAI, and Microsoft Research. \n \n – Bruce Buchanan , Reid Smith , Joshua Eckroth",
    "score": 0.0,
    "source_type": "exa",
    "metadata": {
      "summary": "AITopics, a project of the Association for the Advancement of Artificial Intelligence (AAAI) since 1998, is a large online collection of AI research, people, and applications.  Its NewsFinder component,  documented in  Eckroth (2020) and Eckroth & Schoen (2019), tracks AI news.  The current (2024) version uses large language models to cluster and summarize weekly news, providing a concise overview of key AI developments.  While the provided text focuses on the current methodology, it highlights the long history of AITopics and its evolution in tracking AI advancements.\n",
      "is_subpage": true,
      "parent_url": "https://aitopics.org/misc/brief-history",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  },
  {
    "title": "AITopics | AI-Alerts",
    "url": "https://aitopics.org/class/AI-Alerts",
    "content": "This webpage is a news aggregator focusing on current events related to artificial intelligence.  It does not provide a history of AI development.  The provided text covers recent news items such as: a deepfake audio of President Biden; a wrongful arrest stemming from facial recognition technology used by Sunglass Hut and Macy's; and research showing that AI language models can be deceptively designed to hide harmful behavior.\n\n\nAI-Alerts Weekly digest of artificial intelligence news from around the world — automatically selected and enriched by AI technology. Follow @ai_topics Biden Audio Deepfake Alarms Experts in Lead-Up to Elections Time Politics Jan-23-2024, 17:00:12 GMT No political deepfake has alarmed the world's disinformation experts more than the doctored audio message of U.S. President Joe Biden that began circulating over the weekend. In the phone message, a voice edited to sound like Biden urged voters in New Hampshire not to cast their ballots in Tuesday's Democratic primary. \"Save your vote for the November election,\" the phone message went. It even made use of one of Biden's signature phrases: \"What a bunch of malarkey.\" In reality, the president isn't on the ballot in the New Hampshire race -- and voting in the primary doesn't preclude people from participating in November's election. Facial recognition used after Sunglass Hut robbery led to man's wrongful jailing, says suit The Guardian &gt; Technology Jan-23-2024, 00:21:23 GMT A 61-year-old man is suing Macy's and the parent company of Sunglass Hut over the stores' alleged use of a facial recognition system that misidentified him as the culprit behind an armed robbery and led to his wrongful arrest. While in jail, he was beaten and raped, according to his suit. Harvey Eugene Murphy Jr was accused and arrested on charges of robbing a Houston-area Sunglass Hut of thousands of dollars of merchandise in January 2022, though his attorneys say he was living in California at the time of the robbery. He was arrested on 20 October 2023, according to his lawyers. According to Murphy's lawsuit, an employee of EssilorLuxottica, Sunglass Hut's parent company, worked with its retail partner Macy's and used facial recognition software to identify Murphy as the robber. Two-faced AI language models learn to hide deception Nature Jan-23-2024 Researchers worry that bad actors could engineer open-source LLMs to make them respond to subtle cues in a harmful way.Credit: Smail Aslanda/Anadolu Just like people, artificial-intelligence (AI) systems can be deliberately deceptive. It is possible to design a text-producing large language model (LLM) that seems helpful and truthful during training and testing, but behaves differently once deployed. And according to a study shared this month on arXiv1, attempts to detect and remove such two-faced behaviour are often useless -- and can even make the models better at hiding their true nature. The finding that trying to retrain deceptive LLMs can make the situation worse \"was something that was particularly surprising to us … and potentially scary\", says co-author Evan Hubinger, a computer scientist at Anthropic, an AI start-up company in San Francisco, California. Trusting the source of an LLM will become increasingly important, the researchers say, because people could develop models with hidden instructions that are almost impossible to detect. Cops Used DNA to Predict a Suspect's Face--and Tried to Run Facial Recognition on It WIRED Jan-22-2024, 12:00:00 GMT In 2017, detectives at the East Bay Regional Park District Police Department working a cold case got an idea, one that might help them finally get a lead on the murder of Maria Jane Weidhofer. Officers had found Weidhofer, dead and sexually assaulted, at Berkeley, California's Tilden Regional Park in 1990. Nearly 30 years later, the department sent genetic information collected at the crime scene to Parabon NanoLabs--a company that says it can turn DNA into a face. Parabon NanoLabs ran the suspect's DNA through its proprietary machine learning model. Soon, it provided the police department with something the detectives had never seen before: the face of a potential suspect, generated using only crime scene evidence. The image Parabon NanoLabs produced, called a Snapshot Phenotype Report, wasn't a photograph. Watch a plant-inspired robot grow towards light like a vine New Scientist Jan-18-2024, 17:10:43 GMT A robot that can grow around trees or rocks like a vine could be used to make buildings or measure pollution in hard-to-reach natural environments. Vine-like robots aren't new, but they are often designed to rely on just a single sense to grow upwards, such as heat or light, which means they don't work as well in some settings as others. Emanuela Del Dottore at the Italian Institute of Technology and her colleagues have developed a new version, called FiloBot, that can use light, shade or gravity as a guide. It grows by coiling a plastic filament into a cylindrical shape, adding new layers to its body just behind the head that contains the sensors. \"Our robot has an embedded microcontroller that can process multiple stimuli and direct the growth at a precise location, the tip, ensuring the body structure is preserved,\" she says. A New Nonprofit Is Seeking to Solve the AI Copyright Problem TIME - Tech Jan-18-2024, 16:48:04 GMT Stability AI, the makers of the popular AI image generation model Stable Diffusion, had trained the model by feeding it with millions of images that had been \"scraped\" from the internet, without the consent of their creators. Newton-Rex, the head of Stability's audio team, disagreed. \"Companies worth billions of dollars are, without permission, training generative AI models on creators' works, which are then being used to create new content that in many cases can compete with the original works. In December, the New York Times sued OpenAI in a Manhattan court, alleging that the creator of ChatGPT had illegally used millions of the newspaper's articles to train AI systems that are intended to compete with the Times as a reliable source of information. Meanwhile, in July 2023, comedian Sarah Silverman and other writers sued OpenAI and Meta, accusing the companies of using their writing to train AI models without their permission. This robot grows like a vine -- and could help navigate disaster zones Nature Jan-18-2024 The vine-like Filobot was inspired by plants.Credit: Del Dottore et al., Sci. Researchers have demonstrated a robot that grows like a vine in response to stimuli such as light and pressure. The machine -- named FiloBot -- has a head that prints its body by melting and extruding plastic, which then solidifies as it cools. The robot's head is connected to a base by a thin hose, through which it receives a fresh supply of plastic from a spool. FiloBot's growth rate is slow -- its body elongates by just a few millimeters each minute. Don't Talk to People Like They're Chatbots The Atlantic - Technology Jan-17-2024, 19:37:20 GMT For most of history, communicating with a computer has not been like communicating with a person. In their earliest years, computers required carefully constructed instructions, delivered through punch cards; then came a command-line interface, followed by menus and options and text boxes. If you wanted results, you needed to learn the computer's language. This is beginning to change. Large language models--the technology undergirding modern chatbots--allow users to interact with computers through natural conversation, an innovation that introduces some baggage from human-to-human exchanges. Google DeepMind's new AI system can solve complex geometry problems MIT Technology Review Jan-17-2024, 16:00:00 GMT Solving mathematics problems requires logical reasoning, something that most current AI models aren't great at. This demand for reasoning is why mathematics serves as an important benchmark to gauge progress in AI intelligence, says Wang. DeepMind's program, named AlphaGeometry, combines a language model with a type of AI called a symbolic engine, which uses symbols and logical rules to make deductions. Language models excel at recognizing patterns and predicting subsequent steps in a process. However, their reasoning lacks the rigor required for mathematical problem-solving. The symbolic engine, on the other hand, is based purely on formal logic and strict rules, which allows it to guide the language model toward rational decisions.",
    "raw_content": "AI-Alerts Weekly digest of artificial intelligence news from around the world — automatically selected and enriched by AI technology. Follow @ai_topics Biden Audio Deepfake Alarms Experts in Lead-Up to Elections Time Politics Jan-23-2024, 17:00:12 GMT No political deepfake has alarmed the world's disinformation experts more than the doctored audio message of U.S. President Joe Biden that began circulating over the weekend. In the phone message, a voice edited to sound like Biden urged voters in New Hampshire not to cast their ballots in Tuesday's Democratic primary. \"Save your vote for the November election,\" the phone message went. It even made use of one of Biden's signature phrases: \"What a bunch of malarkey.\" In reality, the president isn't on the ballot in the New Hampshire race -- and voting in the primary doesn't preclude people from participating in November's election. Facial recognition used after Sunglass Hut robbery led to man's wrongful jailing, says suit The Guardian &gt; Technology Jan-23-2024, 00:21:23 GMT A 61-year-old man is suing Macy's and the parent company of Sunglass Hut over the stores' alleged use of a facial recognition system that misidentified him as the culprit behind an armed robbery and led to his wrongful arrest. While in jail, he was beaten and raped, according to his suit. Harvey Eugene Murphy Jr was accused and arrested on charges of robbing a Houston-area Sunglass Hut of thousands of dollars of merchandise in January 2022, though his attorneys say he was living in California at the time of the robbery. He was arrested on 20 October 2023, according to his lawyers. According to Murphy's lawsuit, an employee of EssilorLuxottica, Sunglass Hut's parent company, worked with its retail partner Macy's and used facial recognition software to identify Murphy as the robber. Two-faced AI language models learn to hide deception Nature Jan-23-2024 Researchers worry that bad actors could engineer open-source LLMs to make them respond to subtle cues in a harmful way.Credit: Smail Aslanda/Anadolu Just like people, artificial-intelligence (AI) systems can be deliberately deceptive. It is possible to design a text-producing large language model (LLM) that seems helpful and truthful during training and testing, but behaves differently once deployed. And according to a study shared this month on arXiv1, attempts to detect and remove such two-faced behaviour are often useless -- and can even make the models better at hiding their true nature. The finding that trying to retrain deceptive LLMs can make the situation worse \"was something that was particularly surprising to us … and potentially scary\", says co-author Evan Hubinger, a computer scientist at Anthropic, an AI start-up company in San Francisco, California. Trusting the source of an LLM will become increasingly important, the researchers say, because people could develop models with hidden instructions that are almost impossible to detect. Cops Used DNA to Predict a Suspect's Face--and Tried to Run Facial Recognition on It WIRED Jan-22-2024, 12:00:00 GMT In 2017, detectives at the East Bay Regional Park District Police Department working a cold case got an idea, one that might help them finally get a lead on the murder of Maria Jane Weidhofer. Officers had found Weidhofer, dead and sexually assaulted, at Berkeley, California's Tilden Regional Park in 1990. Nearly 30 years later, the department sent genetic information collected at the crime scene to Parabon NanoLabs--a company that says it can turn DNA into a face. Parabon NanoLabs ran the suspect's DNA through its proprietary machine learning model. Soon, it provided the police department with something the detectives had never seen before: the face of a potential suspect, generated using only crime scene evidence. The image Parabon NanoLabs produced, called a Snapshot Phenotype Report, wasn't a photograph. Watch a plant-inspired robot grow towards light like a vine New Scientist Jan-18-2024, 17:10:43 GMT A robot that can grow around trees or rocks like a vine could be used to make buildings or measure pollution in hard-to-reach natural environments. Vine-like robots aren't new, but they are often designed to rely on just a single sense to grow upwards, such as heat or light, which means they don't work as well in some settings as others. Emanuela Del Dottore at the Italian Institute of Technology and her colleagues have developed a new version, called FiloBot, that can use light, shade or gravity as a guide. It grows by coiling a plastic filament into a cylindrical shape, adding new layers to its body just behind the head that contains the sensors. \"Our robot has an embedded microcontroller that can process multiple stimuli and direct the growth at a precise location, the tip, ensuring the body structure is preserved,\" she says. A New Nonprofit Is Seeking to Solve the AI Copyright Problem TIME - Tech Jan-18-2024, 16:48:04 GMT Stability AI, the makers of the popular AI image generation model Stable Diffusion, had trained the model by feeding it with millions of images that had been \"scraped\" from the internet, without the consent of their creators. Newton-Rex, the head of Stability's audio team, disagreed. \"Companies worth billions of dollars are, without permission, training generative AI models on creators' works, which are then being used to create new content that in many cases can compete with the original works. In December, the New York Times sued OpenAI in a Manhattan court, alleging that the creator of ChatGPT had illegally used millions of the newspaper's articles to train AI systems that are intended to compete with the Times as a reliable source of information. Meanwhile, in July 2023, comedian Sarah Silverman and other writers sued OpenAI and Meta, accusing the companies of using their writing to train AI models without their permission. This robot grows like a vine -- and could help navigate disaster zones Nature Jan-18-2024 The vine-like Filobot was inspired by plants.Credit: Del Dottore et al., Sci. Researchers have demonstrated a robot that grows like a vine in response to stimuli such as light and pressure. The machine -- named FiloBot -- has a head that prints its body by melting and extruding plastic, which then solidifies as it cools. The robot's head is connected to a base by a thin hose, through which it receives a fresh supply of plastic from a spool. FiloBot's growth rate is slow -- its body elongates by just a few millimeters each minute. Don't Talk to People Like They're Chatbots The Atlantic - Technology Jan-17-2024, 19:37:20 GMT For most of history, communicating with a computer has not been like communicating with a person. In their earliest years, computers required carefully constructed instructions, delivered through punch cards; then came a command-line interface, followed by menus and options and text boxes. If you wanted results, you needed to learn the computer's language. This is beginning to change. Large language models--the technology undergirding modern chatbots--allow users to interact with computers through natural conversation, an innovation that introduces some baggage from human-to-human exchanges. Google DeepMind's new AI system can solve complex geometry problems MIT Technology Review Jan-17-2024, 16:00:00 GMT Solving mathematics problems requires logical reasoning, something that most current AI models aren't great at. This demand for reasoning is why mathematics serves as an important benchmark to gauge progress in AI intelligence, says Wang. DeepMind's program, named AlphaGeometry, combines a language model with a type of AI called a symbolic engine, which uses symbols and logical rules to make deductions. Language models excel at recognizing patterns and predicting subsequent steps in a process. However, their reasoning lacks the rigor required for mathematical problem-solving. The symbolic engine, on the other hand, is based purely on formal logic and strict rules, which allows it to guide the language model toward rational decisions.",
    "score": 0.0,
    "source_type": "exa",
    "metadata": {
      "summary": "This webpage is a news aggregator focusing on current events related to artificial intelligence.  It does not provide a history of AI development.  The provided text covers recent news items such as: a deepfake audio of President Biden; a wrongful arrest stemming from facial recognition technology used by Sunglass Hut and Macy's; and research showing that AI language models can be deceptively designed to hide harmful behavior.\n",
      "is_subpage": true,
      "parent_url": "https://aitopics.org/misc/brief-history",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  },
  {
    "title": "AITopics | AI Magazine",
    "url": "https://aitopics.org/search?filters=taxnodes%3ATechnology%7CInformation+Technology%7CArtificial+Intelligence%40%40journal%3AAI+Magazine",
    "content": "This AI Magazine page features articles relevant to the history of AI.  One article, \"Nicolas de Condorcet and the First Intelligence Explosion Hypothesis,\" argues that 18th-century mathematician Nicolas de Condorcet was the first to mathematically model an intelligence explosion and predict accelerating technological growth – concepts later revisited in the 20th century.  Another article, \"No AI Is an Island,\" focuses on the development of AI's collaborative capabilities, highlighting the importance of \"teaming intelligence\" for AI's full potential.  The page also lists upcoming AAAI conferences.\n\n\nAI Magazine AAAI Conferences Calendar Editor, Managing ( AAAI ) AI Magazine Apr-4-2019 This page includes forthcoming AAAI sponsored conferences, conferences presented by AAAI Affiliates, and conferences held in cooperation with AAAI. AI Magazine also maintains a calendar listing that includes nonaffiliated conferences at www.aaai.org/Magazine/calendar.php. ICAIL-2019 will be held 17-21 June in USA. SoCS-19 will be held July 16-17 2019 (immediately AAAI Fall Symposium Series. IAAI-20 Conference will be held February 9-11, 2020 at the Hilton New York Midtown Hotel in New York, New York USA. No AI Is an Island: The Case for Teaming Intelligence Johnson, Matthew ( IHMC ) | Vera, Alonso ( NASA Ames Research Center ) AI Magazine Apr-4-2019 The purpose of this article is to draw attention to an aspect of intelligence that has not yet received significant attention from the AI community, but that plays a crucial role in a technology’s effectiveness in the world, namely teaming intelligence. We propose that Al will reach its full potential only if, as part of its intelligence, it also has enough teaming intelligence to work well with people. Although seemingly counterintuitive, the more intelligent the technological system, the greater the need for collaborative skills. This paper will argue why teaming intelligence is important to AI, provide a general structure for AI researchers to use in developing intelligent systems that team well, assess the current state of the art and, in doing so, suggest a path forward for future AI systems. This is not a call to develop a new capability, but rather, an approach to what AI capabilities should be built, and how, so as to imbue intelligent systems with teaming competence. Nicolas de Condorcet and the First Intelligence Explosion Hypothesis Prasad, Mahendra ( University of California, Berkeley ) AI Magazine Apr-4-2019 The intelligence explosion hypothesis (for example, a technological singularity) is roughly the hypothesis that accelerating knowledge or technological growth radically changes humanity. While 20th-century figures are commonly credited as the first discoverers of the hypothesis, I assert that Nicolas de Condorcet, the 18th-century mathematician, is the earliest to (1) mathematically model an intelligence explosion, and (2) present an accelerating historical worldview, and (3) make intelligence explosion predictions that were restated centuries later. Condorcet provides insights on how ontology and social choice can help resolve value alignment. Artificial Intelligence — An Enabler of Naval Tactical Decision Superiority Johnson, Bonnie ( Naval Postgraduate School ) | Treadway, William A. ( US Navy OPNAV ) AI Magazine Apr-4-2019 Artificial intelligence, as a capability enhancer, offers significant improvements to our tactical warfighting advantage. AI provides methods for fusing and analyzing data to enhance our knowledge of the tactical environment; it provides methods for generating and assessing decision options from multidimensional, complex situations; and it provides predictive analytics to identify and examine the effects of tactical courses of action. Machine learning can improve these processes in an evolutionary manner. Advanced computing techniques can handle highly heterogeneous and vast datasets and can synchronize knowledge across distributed warfare assets. This article presents concepts for applying AI to various aspects of tactical battle management and discusses their potential improvements to future warfare. Moral Orthoses: A New Approach to Human and Machine Ethics Wilks, Yorick ( University of Sheffield ) AI Magazine Apr-4-2019 XAI, explainable AI, and the DARPA program to provide that. The European Commission has legislated a demand (Order GDPR 2016/2679) specifying that deployed machine learning systems must explain their decisions. The commission has done this even though no one knows how to provide what they are requiring. What would follow if we and machines are in roughly the same position with respect to the transparency of our ethical decision-making? I want to reintroduce the notion of orthosis into ethical explanation: medically, an orthosis is an externally applied device designed and fitted to the body to aid rehabilitation, and usually contrasted with a prosthesis, which replaces a missing part, like a foot or leg. Here, it will mean an explanatory software agent associated with a human or machine. Artificial Intelligence and Game Theory Models for Defending Critical Networks with Cyber Deception Fugate, Sunny ( Space and Naval Warfare Systems Center Pacific ) | Ferguson-Walter, Kimberly ( US Department of Defense ) AI Magazine Apr-4-2019 Traditional cyber security techniques have led to an asymmetric disadvantage for defenders. The defender must detect all possible threats at all times from all attackers and defend all systems against all possible exploitation. In contrast, an attacker needs only to find a single path to the defender’s critical information. In this article, we discuss how this asymmetry can be rebalanced using cyber deception to change the attacker’s perception of the network environment, and lead attackers to false beliefs about which systems contain critical information or are critical to a defender’s computing infrastructure. We introduce game theory concepts and models to represent and reason over the use of cyber deception by the defender and the effect it has on attacker perception. Finally, we discuss techniques for combining artificial intelligence algorithms with game theory models to estimate hidden states of the attacker using feedback through payoffs to learn how best to defend the system using cyber deception. It is our opinion that adaptive cyber deception is a necessary component of future information systems and networks. The techniques we present can simultaneously decrease the risks and impacts suffered by defenders and dramatically increase the costs and risks of detection for attackers. Such techniques are likely to play a pivotal role in defending national and international security concerns. The AI Bookie Welty, Chris ( IBM ) | Aroyo, Lora ( Vrije Universiteit Amsterdam ) | Horvitz, Eric ( Microsoft ) AI Magazine Apr-4-2019 The AI Bookie column documents highlights from AI Bets, an online forum for the creation of adjudicatable predictions, in the form of bets, about the future of AI. While it is easy to make broad, generalized, or off-the-cuff predictions about the future, it is more difficult to develop predictions that are carefully thought out, concrete, and measurable. This forum was created to help researchers craft predictions whose accuracy can be clearly and unambiguously judged when the bets come due. The bets will be documented both online and regularly in this column. We encourage bets that are rigorously and scientifically argued. We discourage bets that are too general to be evaluated or too specific to an individual or institution. The goal is not to continue to feed the media frenzy and outsized pundit predictions about AI, but rather to curate and promote bets whose outcomes will provide useful feedback to the scientific community. For detailed guidelines and to place bets, visit sciencebets.org. AAAI News Hamilton, Carol ( Association for the Advancement of Artificial Intelligence ) AI Magazine Apr-4-2019 Submissions for HCOMP-19 Are Due in June! The Seventh AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2019) will be held October 28-30 at Skamania Lodge in Washington State near the Columbia Gorge River, just 45 minutes from Portland, Oregon. This year is the 10-year anniversary of the very first HCOMP workshop in Paris, and to celebrate, there will be special events, talks, and panels throughout the conference. HCOMP is the premier venue for disseminating the latest research findings on crowdsourcing and human computation. While artificial intelligence (AI) and human-computer interaction (HCI) represent traditional mainstays of the conference, HCOMP believes strongly in inviting, fostering, and promoting broad, interdisciplinary research. Artificial Intelligence, Robotics, Ethics, and the Military: A Canadian Perspective Wasilow, Sherry ( Defence Research and Development Canada ) | Thorpe, Joelle B. ( Defence Research and Development Canada ) AI Magazine Apr-4-2019 Defense and security organizations depend upon science and technology to meet operational needs, predict and counter threats, and meet increasingly complex demands of modern warfare. Artificial intelligence and robotics could provide solutions to a wide range of military gaps and deficiencies. At the same time, the unique and rapidly evolving nature of AI and robotics challenges existing polices, regulations, and values, and introduces complex ethical issues that might impede their development, evaluation, and use by the Canadian Armed Forces (CAF). Early consideration of potential ethical issues raised by military use of emerging AI and robotics technologies in development is critical to their effective implementation. This article presents an ethics assessment framework for emerging AI and robotics technologies. It is designed to help technology developers, policymakers, decision makers, and other stakeholders identify and broadly consider potential ethical issues that might arise with the military use and integration of emerging AI and robotics technologies of interest. We also provide a contextual environment for our framework, as well as an example of how our framework can be applied to a specific technology. Finally, we briefly identify and address several pervasive issues that arose during our research.",
    "raw_content": "AI Magazine AAAI Conferences Calendar Editor, Managing ( AAAI ) AI Magazine Apr-4-2019 This page includes forthcoming AAAI sponsored conferences, conferences presented by AAAI Affiliates, and conferences held in cooperation with AAAI. AI Magazine also maintains a calendar listing that includes nonaffiliated conferences at www.aaai.org/Magazine/calendar.php. ICAIL-2019 will be held 17-21 June in USA. SoCS-19 will be held July 16-17 2019 (immediately AAAI Fall Symposium Series. IAAI-20 Conference will be held February 9-11, 2020 at the Hilton New York Midtown Hotel in New York, New York USA. No AI Is an Island: The Case for Teaming Intelligence Johnson, Matthew ( IHMC ) | Vera, Alonso ( NASA Ames Research Center ) AI Magazine Apr-4-2019 The purpose of this article is to draw attention to an aspect of intelligence that has not yet received significant attention from the AI community, but that plays a crucial role in a technology’s effectiveness in the world, namely teaming intelligence. We propose that Al will reach its full potential only if, as part of its intelligence, it also has enough teaming intelligence to work well with people. Although seemingly counterintuitive, the more intelligent the technological system, the greater the need for collaborative skills. This paper will argue why teaming intelligence is important to AI, provide a general structure for AI researchers to use in developing intelligent systems that team well, assess the current state of the art and, in doing so, suggest a path forward for future AI systems. This is not a call to develop a new capability, but rather, an approach to what AI capabilities should be built, and how, so as to imbue intelligent systems with teaming competence. Nicolas de Condorcet and the First Intelligence Explosion Hypothesis Prasad, Mahendra ( University of California, Berkeley ) AI Magazine Apr-4-2019 The intelligence explosion hypothesis (for example, a technological singularity) is roughly the hypothesis that accelerating knowledge or technological growth radically changes humanity. While 20th-century figures are commonly credited as the first discoverers of the hypothesis, I assert that Nicolas de Condorcet, the 18th-century mathematician, is the earliest to (1) mathematically model an intelligence explosion, and (2) present an accelerating historical worldview, and (3) make intelligence explosion predictions that were restated centuries later. Condorcet provides insights on how ontology and social choice can help resolve value alignment. Artificial Intelligence — An Enabler of Naval Tactical Decision Superiority Johnson, Bonnie ( Naval Postgraduate School ) | Treadway, William A. ( US Navy OPNAV ) AI Magazine Apr-4-2019 Artificial intelligence, as a capability enhancer, offers significant improvements to our tactical warfighting advantage. AI provides methods for fusing and analyzing data to enhance our knowledge of the tactical environment; it provides methods for generating and assessing decision options from multidimensional, complex situations; and it provides predictive analytics to identify and examine the effects of tactical courses of action. Machine learning can improve these processes in an evolutionary manner. Advanced computing techniques can handle highly heterogeneous and vast datasets and can synchronize knowledge across distributed warfare assets. This article presents concepts for applying AI to various aspects of tactical battle management and discusses their potential improvements to future warfare. Moral Orthoses: A New Approach to Human and Machine Ethics Wilks, Yorick ( University of Sheffield ) AI Magazine Apr-4-2019 XAI, explainable AI, and the DARPA program to provide that. The European Commission has legislated a demand (Order GDPR 2016/2679) specifying that deployed machine learning systems must explain their decisions. The commission has done this even though no one knows how to provide what they are requiring. What would follow if we and machines are in roughly the same position with respect to the transparency of our ethical decision-making? I want to reintroduce the notion of orthosis into ethical explanation: medically, an orthosis is an externally applied device designed and fitted to the body to aid rehabilitation, and usually contrasted with a prosthesis, which replaces a missing part, like a foot or leg. Here, it will mean an explanatory software agent associated with a human or machine. Artificial Intelligence and Game Theory Models for Defending Critical Networks with Cyber Deception Fugate, Sunny ( Space and Naval Warfare Systems Center Pacific ) | Ferguson-Walter, Kimberly ( US Department of Defense ) AI Magazine Apr-4-2019 Traditional cyber security techniques have led to an asymmetric disadvantage for defenders. The defender must detect all possible threats at all times from all attackers and defend all systems against all possible exploitation. In contrast, an attacker needs only to find a single path to the defender’s critical information. In this article, we discuss how this asymmetry can be rebalanced using cyber deception to change the attacker’s perception of the network environment, and lead attackers to false beliefs about which systems contain critical information or are critical to a defender’s computing infrastructure. We introduce game theory concepts and models to represent and reason over the use of cyber deception by the defender and the effect it has on attacker perception. Finally, we discuss techniques for combining artificial intelligence algorithms with game theory models to estimate hidden states of the attacker using feedback through payoffs to learn how best to defend the system using cyber deception. It is our opinion that adaptive cyber deception is a necessary component of future information systems and networks. The techniques we present can simultaneously decrease the risks and impacts suffered by defenders and dramatically increase the costs and risks of detection for attackers. Such techniques are likely to play a pivotal role in defending national and international security concerns. The AI Bookie Welty, Chris ( IBM ) | Aroyo, Lora ( Vrije Universiteit Amsterdam ) | Horvitz, Eric ( Microsoft ) AI Magazine Apr-4-2019 The AI Bookie column documents highlights from AI Bets, an online forum for the creation of adjudicatable predictions, in the form of bets, about the future of AI. While it is easy to make broad, generalized, or off-the-cuff predictions about the future, it is more difficult to develop predictions that are carefully thought out, concrete, and measurable. This forum was created to help researchers craft predictions whose accuracy can be clearly and unambiguously judged when the bets come due. The bets will be documented both online and regularly in this column. We encourage bets that are rigorously and scientifically argued. We discourage bets that are too general to be evaluated or too specific to an individual or institution. The goal is not to continue to feed the media frenzy and outsized pundit predictions about AI, but rather to curate and promote bets whose outcomes will provide useful feedback to the scientific community. For detailed guidelines and to place bets, visit sciencebets.org. AAAI News Hamilton, Carol ( Association for the Advancement of Artificial Intelligence ) AI Magazine Apr-4-2019 Submissions for HCOMP-19 Are Due in June! The Seventh AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2019) will be held October 28-30 at Skamania Lodge in Washington State near the Columbia Gorge River, just 45 minutes from Portland, Oregon. This year is the 10-year anniversary of the very first HCOMP workshop in Paris, and to celebrate, there will be special events, talks, and panels throughout the conference. HCOMP is the premier venue for disseminating the latest research findings on crowdsourcing and human computation. While artificial intelligence (AI) and human-computer interaction (HCI) represent traditional mainstays of the conference, HCOMP believes strongly in inviting, fostering, and promoting broad, interdisciplinary research. Artificial Intelligence, Robotics, Ethics, and the Military: A Canadian Perspective Wasilow, Sherry ( Defence Research and Development Canada ) | Thorpe, Joelle B. ( Defence Research and Development Canada ) AI Magazine Apr-4-2019 Defense and security organizations depend upon science and technology to meet operational needs, predict and counter threats, and meet increasingly complex demands of modern warfare. Artificial intelligence and robotics could provide solutions to a wide range of military gaps and deficiencies. At the same time, the unique and rapidly evolving nature of AI and robotics challenges existing polices, regulations, and values, and introduces complex ethical issues that might impede their development, evaluation, and use by the Canadian Armed Forces (CAF). Early consideration of potential ethical issues raised by military use of emerging AI and robotics technologies in development is critical to their effective implementation. This article presents an ethics assessment framework for emerging AI and robotics technologies. It is designed to help technology developers, policymakers, decision makers, and other stakeholders identify and broadly consider potential ethical issues that might arise with the military use and integration of emerging AI and robotics technologies of interest. We also provide a contextual environment for our framework, as well as an example of how our framework can be applied to a specific technology. Finally, we briefly identify and address several pervasive issues that arose during our research.",
    "score": 0.0,
    "source_type": "exa",
    "metadata": {
      "summary": "This AI Magazine page features articles relevant to the history of AI.  One article, \"Nicolas de Condorcet and the First Intelligence Explosion Hypothesis,\" argues that 18th-century mathematician Nicolas de Condorcet was the first to mathematically model an intelligence explosion and predict accelerating technological growth – concepts later revisited in the 20th century.  Another article, \"No AI Is an Island,\" focuses on the development of AI's collaborative capabilities, highlighting the importance of \"teaming intelligence\" for AI's full potential.  The page also lists upcoming AAAI conferences.\n",
      "is_subpage": true,
      "parent_url": "https://aitopics.org/misc/brief-history",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  },
  {
    "title": "Log in or sign up to view",
    "url": "https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fahistoryofai.com",
    "content": "This webpage is a Facebook login page.  It does not contain information about the history of artificial intelligence development.  To find that information, you will need to search elsewhere.\n\n\nNot Logged In You are not logged in. Please login and try again. Return home English (US) Español Français (France) 中文(简体) العربية Português (Brasil) Italiano 한국어 Deutsch हिन्दी 日本語 Sign Up Log In Messenger Facebook Lite Video Meta Pay Meta Store Meta Quest Ray-Ban Meta Meta AI Instagram Threads Voting Information Center Privacy Policy Consumer Health Privacy Privacy Center About Create ad Create Page Developers Careers Cookies Ad choices Terms Help Contact Uploading &amp; Non-Users Settings Activity log Meta © 2025",
    "raw_content": "Not Logged In You are not logged in. Please login and try again. Return home English (US) Español Français (France) 中文(简体) العربية Português (Brasil) Italiano 한국어 Deutsch हिन्दी 日本語 Sign Up Log In Messenger Facebook Lite Video Meta Pay Meta Store Meta Quest Ray-Ban Meta Meta AI Instagram Threads Voting Information Center Privacy Policy Consumer Health Privacy Privacy Center About Create ad Create Page Developers Careers Cookies Ad choices Terms Help Contact Uploading &amp; Non-Users Settings Activity log Meta © 2025",
    "score": 0.0,
    "source_type": "exa",
    "metadata": {
      "summary": "This webpage is a Facebook login page.  It does not contain information about the history of artificial intelligence development.  To find that information, you will need to search elsewhere.\n",
      "is_subpage": true,
      "parent_url": "https://ahistoryofai.com/",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  },
  {
    "title": "Post to Tumblr - Preview",
    "url": "http://www.tumblr.com/share?u=http%3A%2F%2Fahistoryofai.com&v=3",
    "content": "This webpage from ahistoryofai.com provides a timeline of artificial intelligence development.  It highlights key moments, including Google's AlphaStar defeating professional StarCraft II players in 2019 and the opening of Google's first African AI center in the same year.  The timeline covers periods from antiquity to the modern era, categorized into Antiquity, Middle Ages, Renaissance, Modern History, Golden Years, and the Information Age.\n\n\nahistoryofai.com Timeline - A History of Artificial Intelligence By S. Hussain Ather Jump to: Information Age Golden years Modern history Renaissance Middle Ages Antiquity 2019 Google’s AlphaStar defeated pro StarCraft II players. The artificial intelligence agent could process information about visible enemy characters and its own base while analyzing multiple parts of the map simultaneously. 2019 Google opened its first Africa Artificial Intelligence … Continue reading Timeline →",
    "raw_content": "ahistoryofai.com Timeline - A History of Artificial Intelligence By S. Hussain Ather Jump to: Information Age Golden years Modern history Renaissance Middle Ages Antiquity 2019 Google’s AlphaStar defeated pro StarCraft II players. The artificial intelligence agent could process information about visible enemy characters and its own base while analyzing multiple parts of the map simultaneously. 2019 Google opened its first Africa Artificial Intelligence … Continue reading Timeline →",
    "score": 0.0,
    "source_type": "exa",
    "metadata": {
      "summary": "This webpage from ahistoryofai.com provides a timeline of artificial intelligence development.  It highlights key moments, including Google's AlphaStar defeating professional StarCraft II players in 2019 and the opening of Google's first African AI center in the same year.  The timeline covers periods from antiquity to the modern era, categorized into Antiquity, Middle Ages, Renaissance, Modern History, Golden Years, and the Information Age.\n",
      "is_subpage": true,
      "parent_url": "https://ahistoryofai.com/",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  },
  {
    "title": "Medium Membership",
    "url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2401d9ab2f6e&source=---top_nav_layout_nav-----------------------------------------&~channel=ShowPostUnderCollection&~feature=LoOpenInAppButton",
    "content": "This webpage is about Medium membership, not the history of artificial intelligence.  It describes the benefits of a Medium membership, including unlimited ad-free reading, supporting writers, accessing millions of stories, enhancing the reading experience, and improving writing capabilities.  The page includes testimonials from satisfied members.  To find information on the history of artificial intelligence development, you will need to search for that topic specifically.\n\n\nSupport human stories Become a member to read without limits or ads, fund great writers, and join a global community of people who care about high-quality storytelling. The Case For Reforesting Our Cities Clive Thompson Writer at Wired magazine and author of Coders Don’t Just Set Goals. Build Systems Kurtis Pykes Author of Don’t Just Set Goals. Build Systems Reward writers Your membership directly supports the writers, editors, curators, and teams who make Medium a vibrant, inclusive home for human stories. A portion of your membership is allocated to the writers of the stories you read and interact with. Unlock every story Get access to millions of original stories that spark bright ideas, answer big questions, and fuel bold ambitions. Enhance your reading experience Immerse yourself in audio stories, read offline wherever you go, and connect with the Medium community on Mastodon. Elevate your writing Create and contribute to publications to collaborate with other writers, create a custom domain for your profile, and level up your writing with our simple but powerful publishing tools. Support a mission that matters Members are creating a world where original, human-crafted stories thrive. As a member-supported platform, quality comes first, not ads or clickbait. What members are saying The easy path in social media is promoting the worst content, the cheapest, tackiest, lowest-effort stuff. That’s not what you get on Medium. You can actually find content you can build your brain with. I appreciate that, both as a reader and a writer. Cassie Kozyrkov, Chief Decision Scientist at Google and Medium member Medium has proved a game-changer for me, and quickly became the subscription I value the most, and I have quite a few. The cost is nothing compared to the value Medium generates for me month after month. Enrique Dans, Professor of Innovation at IE Business School and Medium member For us tech folks, Medium membership unlocks a whole treasure trove of high-quality articles. One good technology book could sell for over the Medium membership fee amount. It’s your choice whether to buy one book, or buy hundreds and thousands of books by unlocking member-only reading on Medium. Investing in a Medium membership is one of the best investments I have ever made for my career. Wenqi Glantz, Software Architect at ArisGlobal and Medium member Membership plans Medium Member $5/month or $60/year Get started Read member-only stories Support writers you read most Earn money for your writing Listen to audio narrations Read offline with the Medium app Access our Mastodon community Connect your custom domain Create your own publications Friend of Medium $15/month or $150/year Get started All Medium member benefits Give 4x more to the writers you read Share member-only stories with anyone and drive more earnings for writers Customize app icon",
    "raw_content": "Support human stories Become a member to read without limits or ads, fund great writers, and join a global community of people who care about high-quality storytelling. The Case For Reforesting Our Cities Clive Thompson Writer at Wired magazine and author of Coders Don’t Just Set Goals. Build Systems Kurtis Pykes Author of Don’t Just Set Goals. Build Systems Reward writers Your membership directly supports the writers, editors, curators, and teams who make Medium a vibrant, inclusive home for human stories. A portion of your membership is allocated to the writers of the stories you read and interact with. Unlock every story Get access to millions of original stories that spark bright ideas, answer big questions, and fuel bold ambitions. Enhance your reading experience Immerse yourself in audio stories, read offline wherever you go, and connect with the Medium community on Mastodon. Elevate your writing Create and contribute to publications to collaborate with other writers, create a custom domain for your profile, and level up your writing with our simple but powerful publishing tools. Support a mission that matters Members are creating a world where original, human-crafted stories thrive. As a member-supported platform, quality comes first, not ads or clickbait. What members are saying The easy path in social media is promoting the worst content, the cheapest, tackiest, lowest-effort stuff. That’s not what you get on Medium. You can actually find content you can build your brain with. I appreciate that, both as a reader and a writer. Cassie Kozyrkov, Chief Decision Scientist at Google and Medium member Medium has proved a game-changer for me, and quickly became the subscription I value the most, and I have quite a few. The cost is nothing compared to the value Medium generates for me month after month. Enrique Dans, Professor of Innovation at IE Business School and Medium member For us tech folks, Medium membership unlocks a whole treasure trove of high-quality articles. One good technology book could sell for over the Medium membership fee amount. It’s your choice whether to buy one book, or buy hundreds and thousands of books by unlocking member-only reading on Medium. Investing in a Medium membership is one of the best investments I have ever made for my career. Wenqi Glantz, Software Architect at ArisGlobal and Medium member Membership plans Medium Member $5/month or $60/year Get started Read member-only stories Support writers you read most Earn money for your writing Listen to audio narrations Read offline with the Medium app Access our Mastodon community Connect your custom domain Create your own publications Friend of Medium $15/month or $150/year Get started All Medium member benefits Give 4x more to the writers you read Share member-only stories with anyone and drive more earnings for writers Customize app icon",
    "score": 0.0,
    "source_type": "exa",
    "metadata": {
      "summary": "This webpage is about Medium membership, not the history of artificial intelligence.  It describes the benefits of a Medium membership, including unlimited ad-free reading, supporting writers, accessing millions of stories, enhancing the reading experience, and improving writing capabilities.  The page includes testimonials from satisfied members.  To find information on the history of artificial intelligence development, you will need to search for that topic specifically.\n",
      "is_subpage": true,
      "parent_url": "https://medium.com/tecnosophia/artificial-intelligence-an-illustrated-history-2401d9ab2f6e",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  },
  {
    "title": "Medium: Read and write stories.",
    "url": "https://medium.com/?source=---top_nav_layout_nav-----------------------------------------",
    "content": "This webpage is the homepage for Medium.com, a platform for reading and writing online.  It does not contain information about the history of artificial intelligence development.  Therefore, I cannot provide a summary relevant to your query from this URL.\n\n\nHuman stories &amp; ideas A place to read, write, and deepen your understanding",
    "raw_content": "Human stories &amp; ideas A place to read, write, and deepen your understanding",
    "score": 0.0,
    "source_type": "exa",
    "metadata": {
      "summary": "This webpage is the homepage for Medium.com, a platform for reading and writing online.  It does not contain information about the history of artificial intelligence development.  Therefore, I cannot provide a summary relevant to your query from this URL.\n",
      "is_subpage": true,
      "parent_url": "https://medium.com/tecnosophia/artificial-intelligence-an-illustrated-history-2401d9ab2f6e",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  },
  {
    "title": "Medium",
    "url": "https://medium.com/search?source=---top_nav_layout_nav-----------------------------------------",
    "content": "This Medium search results page shows no recent searches.  Therefore, it cannot provide information on the history of artificial intelligence development.  To find this information, please try a different search engine or resource.\n\n\nRecent searches You have no recent searches",
    "raw_content": "Recent searches You have no recent searches",
    "score": 0.0,
    "source_type": "exa",
    "metadata": {
      "summary": "This Medium search results page shows no recent searches.  Therefore, it cannot provide information on the history of artificial intelligence development.  To find this information, please try a different search engine or resource.\n",
      "is_subpage": true,
      "parent_url": "https://medium.com/tecnosophia/artificial-intelligence-an-illustrated-history-2401d9ab2f6e",
      "query": "history of artificial intelligence development",
      "crawled_at": "2025-03-16T20:18:04.206229"
    }
  }
]